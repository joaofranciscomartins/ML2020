{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework4.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2cfbf43c61354a05b325cb4358826a2d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cf0bbf2f96e14be2b4b9a2b12d93fee9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5bd42eca61ad451a8ec3dec255b9701a","IPY_MODEL_1866cddab0f548f8841221359dbb9387"]}},"cf0bbf2f96e14be2b4b9a2b12d93fee9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5bd42eca61ad451a8ec3dec255b9701a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_93a31bd581ac44b6be494faeca0bb5c6","_dom_classes":[],"description":"Dl Completed...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":4,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cb4912391600449db165ac952aeb3d68"}},"1866cddab0f548f8841221359dbb9387":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c802088c09b740a2a971bc8a8ae0c64f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4/4 [00:00&lt;00:00,  4.98 file/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c7d49cd73614c64a5df524348eb46a7"}},"93a31bd581ac44b6be494faeca0bb5c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cb4912391600449db165ac952aeb3d68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c802088c09b740a2a971bc8a8ae0c64f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0c7d49cd73614c64a5df524348eb46a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"5gOQ3xjfcye4","colab_type":"text"},"source":["# Machine Learning\n","\n","## Homework IV\n","\n","Using Tensorflow/Keras as exemplified in practical lecture 9, explore the use of different\n","neural network architectures to achieve the highest performance on the test set of the famous\n","MNIST data set (http://yann.lecun.com/exdb/mnist/)."]},{"cell_type":"markdown","metadata":{"id":"t2R0utxHh7RB","colab_type":"text"},"source":["Let's start by importing the libraries that will be used in this exercise:\n","\n","* [tensorflow](https://www.tensorflow.org/): the neural network library\n","* [tensorflow_datasets](https://www.tensorflow.org/datasets): provides the datasets that we will use\n","* [numpy](https://numpy.org/): we will use it to store the data in array format for visualization\n","* [sklearn](https://scikit-learn.org/): provides a [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) implementation that we will use for visualization\n","* [matplotlib](https://matplotlib.org/): plotting library for visualization"]},{"cell_type":"code","metadata":{"id":"nobKPX3qh9df","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow.keras import layers\n","from tensorflow.keras import regularizers\n","import numpy as np\n","\n","import sklearn.decomposition\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a1ep5eS1fQ_C","colab_type":"text"},"source":["### Dataset\n","We will use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset throught this exercise, which is widely used for introducing image classification problems: \n","\n","* 70k examples of handwritten digits\n","* Image size: 28x28\n","* 1 channel\n","* 10 classes: [0-9]"]},{"cell_type":"code","metadata":{"id":"W0Ziy61Bn1h8","colab_type":"code","outputId":"86e8ded1-8004-46b2-9342-cc5bbae14482","executionInfo":{"status":"ok","timestamp":1590934985029,"user_tz":-60,"elapsed":4210,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":214,"referenced_widgets":["2cfbf43c61354a05b325cb4358826a2d","cf0bbf2f96e14be2b4b9a2b12d93fee9","5bd42eca61ad451a8ec3dec255b9701a","1866cddab0f548f8841221359dbb9387","93a31bd581ac44b6be494faeca0bb5c6","cb4912391600449db165ac952aeb3d68","c802088c09b740a2a971bc8a8ae0c64f","0c7d49cd73614c64a5df524348eb46a7"]}},"source":["mnist_data, mnist_info = tfds.load('mnist', with_info=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n","local data directory. If you'd instead prefer to read directly from our public\n","GCS bucket (recommended if you're running on GCP), you can instead set\n","data_dir=gs://tfds-data/datasets.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[1mDownloading and preparing dataset mnist/3.0.0 (download: 11.06 MiB, generated: Unknown size, total: 11.06 MiB) to /root/tensorflow_datasets/mnist/3.0.0...\u001b[0m\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2cfbf43c61354a05b325cb4358826a2d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptio…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0cwqnl7N7uqE","colab_type":"code","outputId":"c5d4be65-2a9e-4eb9-f165-909a8fd4cf6b","executionInfo":{"status":"ok","timestamp":1590934989533,"user_tz":-60,"elapsed":1053,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["print(mnist_info)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tfds.core.DatasetInfo(\n","    name='mnist',\n","    version=3.0.0,\n","    description='The MNIST database of handwritten digits.',\n","    homepage='http://yann.lecun.com/exdb/mnist/',\n","    features=FeaturesDict({\n","        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n","        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n","    }),\n","    total_num_examples=70000,\n","    splits={\n","        'test': 10000,\n","        'train': 60000,\n","    },\n","    supervised_keys=('image', 'label'),\n","    citation=\"\"\"@article{lecun2010mnist,\n","      title={MNIST handwritten digit database},\n","      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n","      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n","      volume={2},\n","      year={2010}\n","    }\"\"\",\n","    redistribution_info=,\n",")\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wqdICHPJA4jb","colab_type":"text"},"source":["We can see that, in this case, the dataset has a standard partition of 60k examples for training and 10k for testing. Let's convert them to [NumPy](https://numpy.org/) arrays:"]},{"cell_type":"code","metadata":{"id":"eSQ9WCSiBObp","colab_type":"code","colab":{}},"source":["mnist_train_x = np.asarray([instance['image']/255 for instance in tfds.as_numpy(mnist_data['train'])])\n","mnist_train_y = np.asarray([instance['label'] for instance in tfds.as_numpy(mnist_data['train'])])\n","\n","mnist_test_x = np.asarray([instance['image']/255 for instance in tfds.as_numpy(mnist_data['test'])])\n","mnist_test_y = np.asarray([instance['label'] for instance in tfds.as_numpy(mnist_data['test'])])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOaPE_o8el7H","colab_type":"text"},"source":["## Feed-Forward Networks\n","\n","To explain how to create and train a neural network model using the [Keras API](https://www.tensorflow.org/api_docs/python/tf/keras), we will start by exploring feed-forward networks."]},{"cell_type":"markdown","metadata":{"id":"Irnsi3jG-ZYq","colab_type":"text"},"source":["### Models\n","\n","Now, let's create and train some networks to approach the problem posed by the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). [Keras](https://www.tensorflow.org/api_docs/python/tf/keras) provides two ways to create a model: the [Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model, in which the layers form a linear stack, and the [Functional API](https://www.tensorflow.org/guide/keras/functional), which is more flexible. "]},{"cell_type":"markdown","metadata":{"id":"FloIHp4xic5E","colab_type":"text"},"source":["#### Baseline: Single-Layer Network\n","\n","We will start by creating a very simple network without hidden layers. Thus, in addition to the [input](https://www.tensorflow.org/api_docs/python/tf/keras/Input), it will only have a fully connected layer which, in the [Keras API](https://www.tensorflow.org/api_docs/python/tf/keras), is called a [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer. Since the dataset poses a multiclass classification problem, the layer will have a number of neurons equal to the number of classes and we will use the softmax [activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations). The *name* parameter is optional for both the model and the layers, but it is useful in more complex scenarios."]},{"cell_type":"code","metadata":{"id":"2YDXn9cu6SKO","colab_type":"code","outputId":"374a84b7-c3a4-45fa-d80f-0c47a296ac8f","executionInfo":{"status":"ok","timestamp":1590951348877,"user_tz":-60,"elapsed":581,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["mnist_baseline_model = tf.keras.Sequential(name='mnist_baseline')\n","mnist_baseline_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_baseline_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_baseline_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_baseline_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_baseline_model.summary()"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Model: \"mnist_baseline\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten (Flatten)            (None, 784)               0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                7850      \n","=================================================================\n","Total params: 7,850\n","Trainable params: 7,850\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SOvEBs1zDhT3","colab_type":"code","outputId":"30e3323a-689e-44e8-8761-cee92619a071","executionInfo":{"status":"ok","timestamp":1590951474256,"user_tz":-60,"elapsed":122403,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_baseline_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_baseline_model_train = mnist_baseline_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","187/188 [============================>.] - ETA: 0s - loss: 1.6766 - accuracy: 0.5578\n","Epoch 00001: val_accuracy improved from -inf to 0.75492, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 1.6756 - accuracy: 0.5583 - val_loss: 1.2533 - val_accuracy: 0.7549\n","Epoch 2/10000\n","186/188 [============================>.] - ETA: 0s - loss: 1.0580 - accuracy: 0.7937\n","Epoch 00002: val_accuracy improved from 0.75492 to 0.81550, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 1.0569 - accuracy: 0.7940 - val_loss: 0.9230 - val_accuracy: 0.8155\n","Epoch 3/10000\n","175/188 [==========================>...] - ETA: 0s - loss: 0.8401 - accuracy: 0.8258\n","Epoch 00003: val_accuracy improved from 0.81550 to 0.83567, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.8356 - accuracy: 0.8264 - val_loss: 0.7745 - val_accuracy: 0.8357\n","Epoch 4/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.7266 - accuracy: 0.8410\n","Epoch 00004: val_accuracy improved from 0.83567 to 0.84733, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.7239 - accuracy: 0.8409 - val_loss: 0.6895 - val_accuracy: 0.8473\n","Epoch 5/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.6556 - accuracy: 0.8512\n","Epoch 00005: val_accuracy improved from 0.84733 to 0.85375, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.8503 - val_loss: 0.6338 - val_accuracy: 0.8537\n","Epoch 6/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.6101 - accuracy: 0.8570\n","Epoch 00006: val_accuracy improved from 0.85375 to 0.85842, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.6089 - accuracy: 0.8570 - val_loss: 0.5942 - val_accuracy: 0.8584\n","Epoch 7/10000\n","185/188 [============================>.] - ETA: 0s - loss: 0.5742 - accuracy: 0.8626\n","Epoch 00007: val_accuracy improved from 0.85842 to 0.86267, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.5745 - accuracy: 0.8625 - val_loss: 0.5643 - val_accuracy: 0.8627\n","Epoch 8/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.5490 - accuracy: 0.8655\n","Epoch 00008: val_accuracy improved from 0.86267 to 0.86608, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.5481 - accuracy: 0.8658 - val_loss: 0.5406 - val_accuracy: 0.8661\n","Epoch 9/10000\n","185/188 [============================>.] - ETA: 0s - loss: 0.5272 - accuracy: 0.8696\n","Epoch 00009: val_accuracy improved from 0.86608 to 0.86950, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.5269 - accuracy: 0.8697 - val_loss: 0.5214 - val_accuracy: 0.8695\n","Epoch 10/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.5093 - accuracy: 0.8730\n","Epoch 00010: val_accuracy improved from 0.86950 to 0.87208, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.8725 - val_loss: 0.5056 - val_accuracy: 0.8721\n","Epoch 11/10000\n","183/188 [============================>.] - ETA: 0s - loss: 0.4954 - accuracy: 0.8744\n","Epoch 00011: val_accuracy improved from 0.87208 to 0.87433, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.4949 - accuracy: 0.8745 - val_loss: 0.4921 - val_accuracy: 0.8743\n","Epoch 12/10000\n","183/188 [============================>.] - ETA: 0s - loss: 0.4821 - accuracy: 0.8770\n","Epoch 00012: val_accuracy improved from 0.87433 to 0.87642, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.4824 - accuracy: 0.8767 - val_loss: 0.4806 - val_accuracy: 0.8764\n","Epoch 13/10000\n","182/188 [============================>.] - ETA: 0s - loss: 0.4721 - accuracy: 0.8789\n","Epoch 00013: val_accuracy improved from 0.87642 to 0.87742, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.4715 - accuracy: 0.8791 - val_loss: 0.4705 - val_accuracy: 0.8774\n","Epoch 14/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.4614 - accuracy: 0.8806\n","Epoch 00014: val_accuracy improved from 0.87742 to 0.87967, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.4620 - accuracy: 0.8806 - val_loss: 0.4615 - val_accuracy: 0.8797\n","Epoch 15/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.4548 - accuracy: 0.8812\n","Epoch 00015: val_accuracy improved from 0.87967 to 0.88100, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.4536 - accuracy: 0.8819 - val_loss: 0.4536 - val_accuracy: 0.8810\n","Epoch 16/10000\n","174/188 [==========================>...] - ETA: 0s - loss: 0.4463 - accuracy: 0.8831\n","Epoch 00016: val_accuracy improved from 0.88100 to 0.88233, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.4460 - accuracy: 0.8832 - val_loss: 0.4466 - val_accuracy: 0.8823\n","Epoch 17/10000\n","176/188 [===========================>..] - ETA: 0s - loss: 0.4394 - accuracy: 0.8847\n","Epoch 00017: val_accuracy improved from 0.88233 to 0.88392, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.4392 - accuracy: 0.8845 - val_loss: 0.4401 - val_accuracy: 0.8839\n","Epoch 18/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4330 - accuracy: 0.8853\n","Epoch 00018: val_accuracy improved from 0.88392 to 0.88592, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.4330 - accuracy: 0.8853 - val_loss: 0.4343 - val_accuracy: 0.8859\n","Epoch 19/10000\n","177/188 [===========================>..] - ETA: 0s - loss: 0.4277 - accuracy: 0.8862\n","Epoch 00019: val_accuracy improved from 0.88592 to 0.88650, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.4274 - accuracy: 0.8862 - val_loss: 0.4290 - val_accuracy: 0.8865\n","Epoch 20/10000\n","185/188 [============================>.] - ETA: 0s - loss: 0.4225 - accuracy: 0.8868\n","Epoch 00020: val_accuracy improved from 0.88650 to 0.88750, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.4222 - accuracy: 0.8869 - val_loss: 0.4241 - val_accuracy: 0.8875\n","Epoch 21/10000\n","177/188 [===========================>..] - ETA: 0s - loss: 0.4178 - accuracy: 0.8876\n","Epoch 00021: val_accuracy improved from 0.88750 to 0.88850, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.4174 - accuracy: 0.8882 - val_loss: 0.4196 - val_accuracy: 0.8885\n","Epoch 22/10000\n","183/188 [============================>.] - ETA: 0s - loss: 0.4129 - accuracy: 0.8887\n","Epoch 00022: val_accuracy improved from 0.88850 to 0.88892, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.4130 - accuracy: 0.8889 - val_loss: 0.4154 - val_accuracy: 0.8889\n","Epoch 23/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4089 - accuracy: 0.8899\n","Epoch 00023: val_accuracy improved from 0.88892 to 0.89008, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.4089 - accuracy: 0.8899 - val_loss: 0.4116 - val_accuracy: 0.8901\n","Epoch 24/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.4043 - accuracy: 0.8913\n","Epoch 00024: val_accuracy improved from 0.89008 to 0.89083, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.4050 - accuracy: 0.8907 - val_loss: 0.4079 - val_accuracy: 0.8908\n","Epoch 25/10000\n","181/188 [===========================>..] - ETA: 0s - loss: 0.4018 - accuracy: 0.8916\n","Epoch 00025: val_accuracy improved from 0.89083 to 0.89142, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.4014 - accuracy: 0.8916 - val_loss: 0.4045 - val_accuracy: 0.8914\n","Epoch 26/10000\n","183/188 [============================>.] - ETA: 0s - loss: 0.3976 - accuracy: 0.8923\n","Epoch 00026: val_accuracy improved from 0.89142 to 0.89167, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3980 - accuracy: 0.8923 - val_loss: 0.4013 - val_accuracy: 0.8917\n","Epoch 27/10000\n","179/188 [===========================>..] - ETA: 0s - loss: 0.3962 - accuracy: 0.8926\n","Epoch 00027: val_accuracy improved from 0.89167 to 0.89325, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3949 - accuracy: 0.8929 - val_loss: 0.3984 - val_accuracy: 0.8932\n","Epoch 28/10000\n","174/188 [==========================>...] - ETA: 0s - loss: 0.3908 - accuracy: 0.8946\n","Epoch 00028: val_accuracy improved from 0.89325 to 0.89392, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3918 - accuracy: 0.8939 - val_loss: 0.3955 - val_accuracy: 0.8939\n","Epoch 29/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3890 - accuracy: 0.8945\n","Epoch 00029: val_accuracy improved from 0.89392 to 0.89442, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3890 - accuracy: 0.8945 - val_loss: 0.3929 - val_accuracy: 0.8944\n","Epoch 30/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3865 - accuracy: 0.8958\n","Epoch 00030: val_accuracy improved from 0.89442 to 0.89467, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3863 - accuracy: 0.8953 - val_loss: 0.3904 - val_accuracy: 0.8947\n","Epoch 31/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.3844 - accuracy: 0.8956\n","Epoch 00031: val_accuracy improved from 0.89467 to 0.89525, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3838 - accuracy: 0.8959 - val_loss: 0.3880 - val_accuracy: 0.8953\n","Epoch 32/10000\n","183/188 [============================>.] - ETA: 0s - loss: 0.3814 - accuracy: 0.8961\n","Epoch 00032: val_accuracy improved from 0.89525 to 0.89625, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3814 - accuracy: 0.8962 - val_loss: 0.3857 - val_accuracy: 0.8963\n","Epoch 33/10000\n","184/188 [============================>.] - ETA: 0s - loss: 0.3791 - accuracy: 0.8970\n","Epoch 00033: val_accuracy improved from 0.89625 to 0.89700, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3791 - accuracy: 0.8969 - val_loss: 0.3836 - val_accuracy: 0.8970\n","Epoch 34/10000\n","174/188 [==========================>...] - ETA: 0s - loss: 0.3774 - accuracy: 0.8980\n","Epoch 00034: val_accuracy did not improve from 0.89700\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3768 - accuracy: 0.8979 - val_loss: 0.3815 - val_accuracy: 0.8970\n","Epoch 35/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3747 - accuracy: 0.8981\n","Epoch 00035: val_accuracy improved from 0.89700 to 0.89767, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3747 - accuracy: 0.8981 - val_loss: 0.3795 - val_accuracy: 0.8977\n","Epoch 36/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3723 - accuracy: 0.8981\n","Epoch 00036: val_accuracy improved from 0.89767 to 0.89808, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3727 - accuracy: 0.8985 - val_loss: 0.3777 - val_accuracy: 0.8981\n","Epoch 37/10000\n","182/188 [============================>.] - ETA: 0s - loss: 0.3710 - accuracy: 0.8983\n","Epoch 00037: val_accuracy improved from 0.89808 to 0.89900, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3708 - accuracy: 0.8988 - val_loss: 0.3758 - val_accuracy: 0.8990\n","Epoch 38/10000\n","185/188 [============================>.] - ETA: 0s - loss: 0.3691 - accuracy: 0.8997\n","Epoch 00038: val_accuracy improved from 0.89900 to 0.89933, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3689 - accuracy: 0.8996 - val_loss: 0.3741 - val_accuracy: 0.8993\n","Epoch 39/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.3660 - accuracy: 0.9002\n","Epoch 00039: val_accuracy did not improve from 0.89933\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3672 - accuracy: 0.8997 - val_loss: 0.3726 - val_accuracy: 0.8993\n","Epoch 40/10000\n","176/188 [===========================>..] - ETA: 0s - loss: 0.3652 - accuracy: 0.8998\n","Epoch 00040: val_accuracy improved from 0.89933 to 0.89958, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3654 - accuracy: 0.8999 - val_loss: 0.3710 - val_accuracy: 0.8996\n","Epoch 41/10000\n","170/188 [==========================>...] - ETA: 0s - loss: 0.3638 - accuracy: 0.9002\n","Epoch 00041: val_accuracy improved from 0.89958 to 0.89992, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3638 - accuracy: 0.9004 - val_loss: 0.3694 - val_accuracy: 0.8999\n","Epoch 42/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.9007\n","Epoch 00042: val_accuracy improved from 0.89992 to 0.90058, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3622 - accuracy: 0.9007 - val_loss: 0.3680 - val_accuracy: 0.9006\n","Epoch 43/10000\n","178/188 [===========================>..] - ETA: 0s - loss: 0.3605 - accuracy: 0.9009\n","Epoch 00043: val_accuracy improved from 0.90058 to 0.90117, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3607 - accuracy: 0.9011 - val_loss: 0.3665 - val_accuracy: 0.9012\n","Epoch 44/10000\n","183/188 [============================>.] - ETA: 0s - loss: 0.3594 - accuracy: 0.9016\n","Epoch 00044: val_accuracy did not improve from 0.90117\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3592 - accuracy: 0.9017 - val_loss: 0.3652 - val_accuracy: 0.9007\n","Epoch 45/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3578 - accuracy: 0.9015\n","Epoch 00045: val_accuracy did not improve from 0.90117\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3578 - accuracy: 0.9015 - val_loss: 0.3640 - val_accuracy: 0.9012\n","Epoch 46/10000\n","184/188 [============================>.] - ETA: 0s - loss: 0.3562 - accuracy: 0.9017\n","Epoch 00046: val_accuracy improved from 0.90117 to 0.90142, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3564 - accuracy: 0.9018 - val_loss: 0.3626 - val_accuracy: 0.9014\n","Epoch 47/10000\n","180/188 [===========================>..] - ETA: 0s - loss: 0.3549 - accuracy: 0.9022\n","Epoch 00047: val_accuracy did not improve from 0.90142\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3550 - accuracy: 0.9023 - val_loss: 0.3614 - val_accuracy: 0.9014\n","Epoch 48/10000\n","181/188 [===========================>..] - ETA: 0s - loss: 0.3544 - accuracy: 0.9026\n","Epoch 00048: val_accuracy did not improve from 0.90142\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3538 - accuracy: 0.9027 - val_loss: 0.3603 - val_accuracy: 0.9013\n","Epoch 49/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3514 - accuracy: 0.9033\n","Epoch 00049: val_accuracy improved from 0.90142 to 0.90200, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3525 - accuracy: 0.9029 - val_loss: 0.3591 - val_accuracy: 0.9020\n","Epoch 50/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.3502 - accuracy: 0.9036\n","Epoch 00050: val_accuracy improved from 0.90200 to 0.90225, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3513 - accuracy: 0.9031 - val_loss: 0.3580 - val_accuracy: 0.9022\n","Epoch 51/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3496 - accuracy: 0.9033\n","Epoch 00051: val_accuracy did not improve from 0.90225\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3501 - accuracy: 0.9032 - val_loss: 0.3569 - val_accuracy: 0.9021\n","Epoch 52/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.9038\n","Epoch 00052: val_accuracy improved from 0.90225 to 0.90258, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3489 - accuracy: 0.9036 - val_loss: 0.3560 - val_accuracy: 0.9026\n","Epoch 53/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3465 - accuracy: 0.9043\n","Epoch 00053: val_accuracy did not improve from 0.90258\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3478 - accuracy: 0.9039 - val_loss: 0.3550 - val_accuracy: 0.9024\n","Epoch 54/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3462 - accuracy: 0.9044\n","Epoch 00054: val_accuracy improved from 0.90258 to 0.90292, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3467 - accuracy: 0.9043 - val_loss: 0.3539 - val_accuracy: 0.9029\n","Epoch 55/10000\n","184/188 [============================>.] - ETA: 0s - loss: 0.3458 - accuracy: 0.9045\n","Epoch 00055: val_accuracy improved from 0.90292 to 0.90317, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3457 - accuracy: 0.9046 - val_loss: 0.3530 - val_accuracy: 0.9032\n","Epoch 56/10000\n","177/188 [===========================>..] - ETA: 0s - loss: 0.3459 - accuracy: 0.9046\n","Epoch 00056: val_accuracy improved from 0.90317 to 0.90325, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3447 - accuracy: 0.9048 - val_loss: 0.3521 - val_accuracy: 0.9032\n","Epoch 57/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.9052\n","Epoch 00057: val_accuracy improved from 0.90325 to 0.90375, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3437 - accuracy: 0.9052 - val_loss: 0.3511 - val_accuracy: 0.9038\n","Epoch 58/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.3428 - accuracy: 0.9050\n","Epoch 00058: val_accuracy improved from 0.90375 to 0.90392, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3427 - accuracy: 0.9054 - val_loss: 0.3502 - val_accuracy: 0.9039\n","Epoch 59/10000\n","174/188 [==========================>...] - ETA: 0s - loss: 0.3416 - accuracy: 0.9054\n","Epoch 00059: val_accuracy did not improve from 0.90392\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3417 - accuracy: 0.9057 - val_loss: 0.3495 - val_accuracy: 0.9037\n","Epoch 60/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3408 - accuracy: 0.9059\n","Epoch 00060: val_accuracy did not improve from 0.90392\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3408 - accuracy: 0.9059 - val_loss: 0.3486 - val_accuracy: 0.9039\n","Epoch 61/10000\n","180/188 [===========================>..] - ETA: 0s - loss: 0.3401 - accuracy: 0.9060\n","Epoch 00061: val_accuracy did not improve from 0.90392\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3399 - accuracy: 0.9058 - val_loss: 0.3477 - val_accuracy: 0.9039\n","Epoch 62/10000\n","183/188 [============================>.] - ETA: 0s - loss: 0.3392 - accuracy: 0.9062\n","Epoch 00062: val_accuracy improved from 0.90392 to 0.90400, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3390 - accuracy: 0.9063 - val_loss: 0.3471 - val_accuracy: 0.9040\n","Epoch 63/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3396 - accuracy: 0.9057\n","Epoch 00063: val_accuracy improved from 0.90400 to 0.90442, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3382 - accuracy: 0.9065 - val_loss: 0.3463 - val_accuracy: 0.9044\n","Epoch 64/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.3361 - accuracy: 0.9078\n","Epoch 00064: val_accuracy improved from 0.90442 to 0.90450, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3373 - accuracy: 0.9069 - val_loss: 0.3455 - val_accuracy: 0.9045\n","Epoch 65/10000\n","180/188 [===========================>..] - ETA: 0s - loss: 0.3361 - accuracy: 0.9071\n","Epoch 00065: val_accuracy did not improve from 0.90450\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3365 - accuracy: 0.9070 - val_loss: 0.3449 - val_accuracy: 0.9043\n","Epoch 66/10000\n","184/188 [============================>.] - ETA: 0s - loss: 0.3366 - accuracy: 0.9065\n","Epoch 00066: val_accuracy improved from 0.90450 to 0.90500, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3357 - accuracy: 0.9069 - val_loss: 0.3441 - val_accuracy: 0.9050\n","Epoch 67/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.3359 - accuracy: 0.9070\n","Epoch 00067: val_accuracy improved from 0.90500 to 0.90517, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3349 - accuracy: 0.9072 - val_loss: 0.3435 - val_accuracy: 0.9052\n","Epoch 68/10000\n","170/188 [==========================>...] - ETA: 0s - loss: 0.3350 - accuracy: 0.9072\n","Epoch 00068: val_accuracy improved from 0.90517 to 0.90533, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3342 - accuracy: 0.9075 - val_loss: 0.3427 - val_accuracy: 0.9053\n","Epoch 69/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.3348 - accuracy: 0.9073\n","Epoch 00069: val_accuracy did not improve from 0.90533\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3334 - accuracy: 0.9076 - val_loss: 0.3421 - val_accuracy: 0.9049\n","Epoch 70/10000\n","186/188 [============================>.] - ETA: 0s - loss: 0.3328 - accuracy: 0.9076\n","Epoch 00070: val_accuracy did not improve from 0.90533\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3327 - accuracy: 0.9076 - val_loss: 0.3414 - val_accuracy: 0.9053\n","Epoch 71/10000\n","186/188 [============================>.] - ETA: 0s - loss: 0.3324 - accuracy: 0.9079\n","Epoch 00071: val_accuracy improved from 0.90533 to 0.90583, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3320 - accuracy: 0.9080 - val_loss: 0.3409 - val_accuracy: 0.9058\n","Epoch 72/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3327 - accuracy: 0.9077\n","Epoch 00072: val_accuracy improved from 0.90583 to 0.90625, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3313 - accuracy: 0.9080 - val_loss: 0.3401 - val_accuracy: 0.9062\n","Epoch 73/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.9082\n","Epoch 00073: val_accuracy did not improve from 0.90625\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3306 - accuracy: 0.9082 - val_loss: 0.3395 - val_accuracy: 0.9062\n","Epoch 74/10000\n","180/188 [===========================>..] - ETA: 0s - loss: 0.3301 - accuracy: 0.9084\n","Epoch 00074: val_accuracy improved from 0.90625 to 0.90700, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3299 - accuracy: 0.9085 - val_loss: 0.3391 - val_accuracy: 0.9070\n","Epoch 75/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3305 - accuracy: 0.9081\n","Epoch 00075: val_accuracy did not improve from 0.90700\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3292 - accuracy: 0.9084 - val_loss: 0.3385 - val_accuracy: 0.9069\n","Epoch 76/10000\n","182/188 [============================>.] - ETA: 0s - loss: 0.3292 - accuracy: 0.9084\n","Epoch 00076: val_accuracy did not improve from 0.90700\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3286 - accuracy: 0.9085 - val_loss: 0.3379 - val_accuracy: 0.9069\n","Epoch 77/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3278 - accuracy: 0.9090\n","Epoch 00077: val_accuracy improved from 0.90700 to 0.90717, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3279 - accuracy: 0.9089 - val_loss: 0.3373 - val_accuracy: 0.9072\n","Epoch 78/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.3272 - accuracy: 0.9085\n","Epoch 00078: val_accuracy did not improve from 0.90717\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3273 - accuracy: 0.9088 - val_loss: 0.3368 - val_accuracy: 0.9070\n","Epoch 79/10000\n","184/188 [============================>.] - ETA: 0s - loss: 0.3268 - accuracy: 0.9090\n","Epoch 00079: val_accuracy improved from 0.90717 to 0.90725, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3267 - accuracy: 0.9090 - val_loss: 0.3363 - val_accuracy: 0.9072\n","Epoch 80/10000\n","182/188 [============================>.] - ETA: 0s - loss: 0.3262 - accuracy: 0.9093\n","Epoch 00080: val_accuracy improved from 0.90725 to 0.90750, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3261 - accuracy: 0.9093 - val_loss: 0.3357 - val_accuracy: 0.9075\n","Epoch 81/10000\n","184/188 [============================>.] - ETA: 0s - loss: 0.3253 - accuracy: 0.9096\n","Epoch 00081: val_accuracy improved from 0.90750 to 0.90792, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3255 - accuracy: 0.9096 - val_loss: 0.3352 - val_accuracy: 0.9079\n","Epoch 82/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3249 - accuracy: 0.9096\n","Epoch 00082: val_accuracy did not improve from 0.90792\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3249 - accuracy: 0.9095 - val_loss: 0.3347 - val_accuracy: 0.9078\n","Epoch 83/10000\n","184/188 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.9096\n","Epoch 00083: val_accuracy did not improve from 0.90792\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3244 - accuracy: 0.9097 - val_loss: 0.3341 - val_accuracy: 0.9077\n","Epoch 84/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.3236 - accuracy: 0.9095\n","Epoch 00084: val_accuracy did not improve from 0.90792\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3238 - accuracy: 0.9100 - val_loss: 0.3337 - val_accuracy: 0.9077\n","Epoch 85/10000\n","170/188 [==========================>...] - ETA: 0s - loss: 0.3233 - accuracy: 0.9097\n","Epoch 00085: val_accuracy did not improve from 0.90792\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3232 - accuracy: 0.9099 - val_loss: 0.3333 - val_accuracy: 0.9079\n","Epoch 86/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3218 - accuracy: 0.9110\n","Epoch 00086: val_accuracy improved from 0.90792 to 0.90800, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3227 - accuracy: 0.9103 - val_loss: 0.3328 - val_accuracy: 0.9080\n","Epoch 87/10000\n","185/188 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.9101\n","Epoch 00087: val_accuracy improved from 0.90800 to 0.90808, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3222 - accuracy: 0.9103 - val_loss: 0.3324 - val_accuracy: 0.9081\n","Epoch 88/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3216 - accuracy: 0.9103\n","Epoch 00088: val_accuracy improved from 0.90808 to 0.90817, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3216 - accuracy: 0.9103 - val_loss: 0.3319 - val_accuracy: 0.9082\n","Epoch 89/10000\n","175/188 [==========================>...] - ETA: 0s - loss: 0.3234 - accuracy: 0.9098\n","Epoch 00089: val_accuracy improved from 0.90817 to 0.90825, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3211 - accuracy: 0.9105 - val_loss: 0.3314 - val_accuracy: 0.9082\n","Epoch 90/10000\n","185/188 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.9107\n","Epoch 00090: val_accuracy improved from 0.90825 to 0.90833, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3206 - accuracy: 0.9107 - val_loss: 0.3310 - val_accuracy: 0.9083\n","Epoch 91/10000\n","175/188 [==========================>...] - ETA: 0s - loss: 0.3203 - accuracy: 0.9105\n","Epoch 00091: val_accuracy improved from 0.90833 to 0.90883, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3201 - accuracy: 0.9108 - val_loss: 0.3305 - val_accuracy: 0.9088\n","Epoch 92/10000\n","170/188 [==========================>...] - ETA: 0s - loss: 0.3200 - accuracy: 0.9102\n","Epoch 00092: val_accuracy did not improve from 0.90883\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3196 - accuracy: 0.9108 - val_loss: 0.3302 - val_accuracy: 0.9085\n","Epoch 93/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3198 - accuracy: 0.9106\n","Epoch 00093: val_accuracy did not improve from 0.90883\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3192 - accuracy: 0.9109 - val_loss: 0.3298 - val_accuracy: 0.9086\n","Epoch 94/10000\n","180/188 [===========================>..] - ETA: 0s - loss: 0.3195 - accuracy: 0.9107\n","Epoch 00094: val_accuracy did not improve from 0.90883\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3187 - accuracy: 0.9110 - val_loss: 0.3294 - val_accuracy: 0.9088\n","Epoch 95/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.3187 - accuracy: 0.9110\n","Epoch 00095: val_accuracy improved from 0.90883 to 0.90908, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3182 - accuracy: 0.9112 - val_loss: 0.3290 - val_accuracy: 0.9091\n","Epoch 96/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.3179 - accuracy: 0.9112\n","Epoch 00096: val_accuracy did not improve from 0.90908\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3178 - accuracy: 0.9113 - val_loss: 0.3286 - val_accuracy: 0.9090\n","Epoch 97/10000\n","185/188 [============================>.] - ETA: 0s - loss: 0.3177 - accuracy: 0.9114\n","Epoch 00097: val_accuracy did not improve from 0.90908\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3173 - accuracy: 0.9115 - val_loss: 0.3282 - val_accuracy: 0.9091\n","Epoch 98/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3168 - accuracy: 0.9116\n","Epoch 00098: val_accuracy improved from 0.90908 to 0.90933, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3168 - accuracy: 0.9116 - val_loss: 0.3278 - val_accuracy: 0.9093\n","Epoch 99/10000\n","186/188 [============================>.] - ETA: 0s - loss: 0.3163 - accuracy: 0.9119\n","Epoch 00099: val_accuracy improved from 0.90933 to 0.90975, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3164 - accuracy: 0.9120 - val_loss: 0.3274 - val_accuracy: 0.9097\n","Epoch 100/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3160 - accuracy: 0.9120\n","Epoch 00100: val_accuracy did not improve from 0.90975\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3160 - accuracy: 0.9120 - val_loss: 0.3271 - val_accuracy: 0.9091\n","Epoch 101/10000\n","175/188 [==========================>...] - ETA: 0s - loss: 0.3156 - accuracy: 0.9120\n","Epoch 00101: val_accuracy did not improve from 0.90975\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3155 - accuracy: 0.9121 - val_loss: 0.3267 - val_accuracy: 0.9096\n","Epoch 102/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3144 - accuracy: 0.9125\n","Epoch 00102: val_accuracy did not improve from 0.90975\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3151 - accuracy: 0.9122 - val_loss: 0.3264 - val_accuracy: 0.9094\n","Epoch 103/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.3161 - accuracy: 0.9120\n","Epoch 00103: val_accuracy improved from 0.90975 to 0.90983, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3147 - accuracy: 0.9124 - val_loss: 0.3260 - val_accuracy: 0.9098\n","Epoch 104/10000\n","180/188 [===========================>..] - ETA: 0s - loss: 0.3133 - accuracy: 0.9127\n","Epoch 00104: val_accuracy did not improve from 0.90983\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3143 - accuracy: 0.9122 - val_loss: 0.3257 - val_accuracy: 0.9096\n","Epoch 105/10000\n","184/188 [============================>.] - ETA: 0s - loss: 0.3136 - accuracy: 0.9123\n","Epoch 00105: val_accuracy did not improve from 0.90983\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3139 - accuracy: 0.9124 - val_loss: 0.3254 - val_accuracy: 0.9095\n","Epoch 106/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3136 - accuracy: 0.9127\n","Epoch 00106: val_accuracy improved from 0.90983 to 0.91008, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3135 - accuracy: 0.9128 - val_loss: 0.3250 - val_accuracy: 0.9101\n","Epoch 107/10000\n","185/188 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.9128\n","Epoch 00107: val_accuracy did not improve from 0.91008\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3131 - accuracy: 0.9128 - val_loss: 0.3247 - val_accuracy: 0.9097\n","Epoch 108/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.9127\n","Epoch 00108: val_accuracy did not improve from 0.91008\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3127 - accuracy: 0.9127 - val_loss: 0.3244 - val_accuracy: 0.9097\n","Epoch 109/10000\n","175/188 [==========================>...] - ETA: 0s - loss: 0.3131 - accuracy: 0.9127\n","Epoch 00109: val_accuracy did not improve from 0.91008\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3123 - accuracy: 0.9130 - val_loss: 0.3242 - val_accuracy: 0.9099\n","Epoch 110/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.3119 - accuracy: 0.9133\n","Epoch 00110: val_accuracy did not improve from 0.91008\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3120 - accuracy: 0.9131 - val_loss: 0.3238 - val_accuracy: 0.9101\n","Epoch 111/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3116 - accuracy: 0.9134\n","Epoch 00111: val_accuracy did not improve from 0.91008\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3116 - accuracy: 0.9133 - val_loss: 0.3234 - val_accuracy: 0.9101\n","Epoch 112/10000\n","183/188 [============================>.] - ETA: 0s - loss: 0.3109 - accuracy: 0.9134\n","Epoch 00112: val_accuracy improved from 0.91008 to 0.91025, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3112 - accuracy: 0.9133 - val_loss: 0.3231 - val_accuracy: 0.9103\n","Epoch 113/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.3108 - accuracy: 0.9132\n","Epoch 00113: val_accuracy improved from 0.91025 to 0.91050, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3109 - accuracy: 0.9134 - val_loss: 0.3228 - val_accuracy: 0.9105\n","Epoch 114/10000\n","176/188 [===========================>..] - ETA: 0s - loss: 0.3109 - accuracy: 0.9136\n","Epoch 00114: val_accuracy improved from 0.91050 to 0.91058, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3105 - accuracy: 0.9135 - val_loss: 0.3226 - val_accuracy: 0.9106\n","Epoch 115/10000\n","182/188 [============================>.] - ETA: 0s - loss: 0.3109 - accuracy: 0.9131\n","Epoch 00115: val_accuracy did not improve from 0.91058\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3101 - accuracy: 0.9135 - val_loss: 0.3224 - val_accuracy: 0.9102\n","Epoch 116/10000\n","186/188 [============================>.] - ETA: 0s - loss: 0.3096 - accuracy: 0.9136\n","Epoch 00116: val_accuracy improved from 0.91058 to 0.91067, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3098 - accuracy: 0.9136 - val_loss: 0.3220 - val_accuracy: 0.9107\n","Epoch 117/10000\n","170/188 [==========================>...] - ETA: 0s - loss: 0.3107 - accuracy: 0.9135\n","Epoch 00117: val_accuracy did not improve from 0.91067\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3095 - accuracy: 0.9139 - val_loss: 0.3217 - val_accuracy: 0.9106\n","Epoch 118/10000\n","181/188 [===========================>..] - ETA: 0s - loss: 0.3087 - accuracy: 0.9137\n","Epoch 00118: val_accuracy improved from 0.91067 to 0.91075, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3091 - accuracy: 0.9137 - val_loss: 0.3215 - val_accuracy: 0.9107\n","Epoch 119/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.9137\n","Epoch 00119: val_accuracy improved from 0.91075 to 0.91108, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3088 - accuracy: 0.9137 - val_loss: 0.3211 - val_accuracy: 0.9111\n","Epoch 120/10000\n","183/188 [============================>.] - ETA: 0s - loss: 0.3074 - accuracy: 0.9142\n","Epoch 00120: val_accuracy did not improve from 0.91108\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3084 - accuracy: 0.9140 - val_loss: 0.3209 - val_accuracy: 0.9109\n","Epoch 121/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3082 - accuracy: 0.9140\n","Epoch 00121: val_accuracy improved from 0.91108 to 0.91142, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3081 - accuracy: 0.9140 - val_loss: 0.3206 - val_accuracy: 0.9114\n","Epoch 122/10000\n","180/188 [===========================>..] - ETA: 0s - loss: 0.3064 - accuracy: 0.9142\n","Epoch 00122: val_accuracy did not improve from 0.91142\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3078 - accuracy: 0.9142 - val_loss: 0.3203 - val_accuracy: 0.9112\n","Epoch 123/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.3076 - accuracy: 0.9142\n","Epoch 00123: val_accuracy did not improve from 0.91142\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3075 - accuracy: 0.9142 - val_loss: 0.3201 - val_accuracy: 0.9110\n","Epoch 124/10000\n","184/188 [============================>.] - ETA: 0s - loss: 0.3072 - accuracy: 0.9141\n","Epoch 00124: val_accuracy did not improve from 0.91142\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3071 - accuracy: 0.9142 - val_loss: 0.3199 - val_accuracy: 0.9114\n","Epoch 125/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.3074 - accuracy: 0.9140\n","Epoch 00125: val_accuracy improved from 0.91142 to 0.91175, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3068 - accuracy: 0.9142 - val_loss: 0.3196 - val_accuracy: 0.9118\n","Epoch 126/10000\n","185/188 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.9143\n","Epoch 00126: val_accuracy did not improve from 0.91175\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3065 - accuracy: 0.9145 - val_loss: 0.3193 - val_accuracy: 0.9118\n","Epoch 127/10000\n","174/188 [==========================>...] - ETA: 0s - loss: 0.3073 - accuracy: 0.9141\n","Epoch 00127: val_accuracy improved from 0.91175 to 0.91192, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3062 - accuracy: 0.9145 - val_loss: 0.3191 - val_accuracy: 0.9119\n","Epoch 128/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3058 - accuracy: 0.9146\n","Epoch 00128: val_accuracy did not improve from 0.91192\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3059 - accuracy: 0.9146 - val_loss: 0.3189 - val_accuracy: 0.9117\n","Epoch 129/10000\n","181/188 [===========================>..] - ETA: 0s - loss: 0.3059 - accuracy: 0.9148\n","Epoch 00129: val_accuracy did not improve from 0.91192\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3056 - accuracy: 0.9146 - val_loss: 0.3186 - val_accuracy: 0.9118\n","Epoch 130/10000\n","185/188 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.9150\n","Epoch 00130: val_accuracy improved from 0.91192 to 0.91200, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3053 - accuracy: 0.9149 - val_loss: 0.3184 - val_accuracy: 0.9120\n","Epoch 131/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3048 - accuracy: 0.9150\n","Epoch 00131: val_accuracy improved from 0.91200 to 0.91217, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3050 - accuracy: 0.9149 - val_loss: 0.3181 - val_accuracy: 0.9122\n","Epoch 132/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.9147\n","Epoch 00132: val_accuracy improved from 0.91217 to 0.91233, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3047 - accuracy: 0.9147 - val_loss: 0.3179 - val_accuracy: 0.9123\n","Epoch 133/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.3043 - accuracy: 0.9154\n","Epoch 00133: val_accuracy improved from 0.91233 to 0.91250, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3045 - accuracy: 0.9151 - val_loss: 0.3177 - val_accuracy: 0.9125\n","Epoch 134/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3044 - accuracy: 0.9150\n","Epoch 00134: val_accuracy improved from 0.91250 to 0.91258, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3042 - accuracy: 0.9152 - val_loss: 0.3175 - val_accuracy: 0.9126\n","Epoch 135/10000\n","170/188 [==========================>...] - ETA: 0s - loss: 0.3051 - accuracy: 0.9149\n","Epoch 00135: val_accuracy improved from 0.91258 to 0.91275, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3039 - accuracy: 0.9153 - val_loss: 0.3172 - val_accuracy: 0.9128\n","Epoch 136/10000\n","182/188 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.9158\n","Epoch 00136: val_accuracy improved from 0.91275 to 0.91283, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3036 - accuracy: 0.9154 - val_loss: 0.3170 - val_accuracy: 0.9128\n","Epoch 137/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.3031 - accuracy: 0.9152\n","Epoch 00137: val_accuracy improved from 0.91283 to 0.91292, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3033 - accuracy: 0.9151 - val_loss: 0.3168 - val_accuracy: 0.9129\n","Epoch 138/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3036 - accuracy: 0.9151\n","Epoch 00138: val_accuracy did not improve from 0.91292\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3030 - accuracy: 0.9156 - val_loss: 0.3166 - val_accuracy: 0.9129\n","Epoch 139/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.3039 - accuracy: 0.9155\n","Epoch 00139: val_accuracy did not improve from 0.91292\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3028 - accuracy: 0.9156 - val_loss: 0.3164 - val_accuracy: 0.9128\n","Epoch 140/10000\n","186/188 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.9157\n","Epoch 00140: val_accuracy did not improve from 0.91292\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3025 - accuracy: 0.9158 - val_loss: 0.3162 - val_accuracy: 0.9128\n","Epoch 141/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.9159\n","Epoch 00141: val_accuracy improved from 0.91292 to 0.91317, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3023 - accuracy: 0.9159 - val_loss: 0.3160 - val_accuracy: 0.9132\n","Epoch 142/10000\n","183/188 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.9154\n","Epoch 00142: val_accuracy improved from 0.91317 to 0.91325, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3020 - accuracy: 0.9156 - val_loss: 0.3157 - val_accuracy: 0.9133\n","Epoch 143/10000\n","184/188 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.9159\n","Epoch 00143: val_accuracy improved from 0.91325 to 0.91358, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3017 - accuracy: 0.9159 - val_loss: 0.3157 - val_accuracy: 0.9136\n","Epoch 144/10000\n","174/188 [==========================>...] - ETA: 0s - loss: 0.3014 - accuracy: 0.9162\n","Epoch 00144: val_accuracy did not improve from 0.91358\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3015 - accuracy: 0.9161 - val_loss: 0.3153 - val_accuracy: 0.9133\n","Epoch 145/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.9159\n","Epoch 00145: val_accuracy did not improve from 0.91358\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3012 - accuracy: 0.9160 - val_loss: 0.3151 - val_accuracy: 0.9134\n","Epoch 146/10000\n","186/188 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.9160\n","Epoch 00146: val_accuracy improved from 0.91358 to 0.91392, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 4ms/step - loss: 0.3010 - accuracy: 0.9161 - val_loss: 0.3150 - val_accuracy: 0.9139\n","Epoch 147/10000\n","185/188 [============================>.] - ETA: 0s - loss: 0.3009 - accuracy: 0.9160\n","Epoch 00147: val_accuracy did not improve from 0.91392\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3008 - accuracy: 0.9161 - val_loss: 0.3147 - val_accuracy: 0.9135\n","Epoch 148/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3005 - accuracy: 0.9161\n","Epoch 00148: val_accuracy improved from 0.91392 to 0.91408, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3005 - accuracy: 0.9161 - val_loss: 0.3146 - val_accuracy: 0.9141\n","Epoch 149/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.3005 - accuracy: 0.9158\n","Epoch 00149: val_accuracy did not improve from 0.91408\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3002 - accuracy: 0.9164 - val_loss: 0.3144 - val_accuracy: 0.9139\n","Epoch 150/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.9164\n","Epoch 00150: val_accuracy improved from 0.91408 to 0.91417, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.3000 - accuracy: 0.9164 - val_loss: 0.3143 - val_accuracy: 0.9142\n","Epoch 151/10000\n","175/188 [==========================>...] - ETA: 0s - loss: 0.2980 - accuracy: 0.9172\n","Epoch 00151: val_accuracy did not improve from 0.91417\n","188/188 [==============================] - 1s 4ms/step - loss: 0.2998 - accuracy: 0.9165 - val_loss: 0.3139 - val_accuracy: 0.9142\n","Epoch 152/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.3008 - accuracy: 0.9162\n","Epoch 00152: val_accuracy did not improve from 0.91417\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2995 - accuracy: 0.9165 - val_loss: 0.3138 - val_accuracy: 0.9141\n","Epoch 153/10000\n","174/188 [==========================>...] - ETA: 0s - loss: 0.2993 - accuracy: 0.9167\n","Epoch 00153: val_accuracy improved from 0.91417 to 0.91433, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2993 - accuracy: 0.9166 - val_loss: 0.3136 - val_accuracy: 0.9143\n","Epoch 154/10000\n","178/188 [===========================>..] - ETA: 0s - loss: 0.2991 - accuracy: 0.9165\n","Epoch 00154: val_accuracy did not improve from 0.91433\n","188/188 [==============================] - 1s 4ms/step - loss: 0.2991 - accuracy: 0.9166 - val_loss: 0.3135 - val_accuracy: 0.9143\n","Epoch 155/10000\n","185/188 [============================>.] - ETA: 0s - loss: 0.2992 - accuracy: 0.9165\n","Epoch 00155: val_accuracy improved from 0.91433 to 0.91467, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2988 - accuracy: 0.9165 - val_loss: 0.3133 - val_accuracy: 0.9147\n","Epoch 156/10000\n","174/188 [==========================>...] - ETA: 0s - loss: 0.2996 - accuracy: 0.9163\n","Epoch 00156: val_accuracy did not improve from 0.91467\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2986 - accuracy: 0.9167 - val_loss: 0.3131 - val_accuracy: 0.9143\n","Epoch 157/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.2989 - accuracy: 0.9166\n","Epoch 00157: val_accuracy did not improve from 0.91467\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2984 - accuracy: 0.9169 - val_loss: 0.3129 - val_accuracy: 0.9145\n","Epoch 158/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.2974 - accuracy: 0.9169\n","Epoch 00158: val_accuracy did not improve from 0.91467\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2982 - accuracy: 0.9167 - val_loss: 0.3128 - val_accuracy: 0.9147\n","Epoch 159/10000\n","174/188 [==========================>...] - ETA: 0s - loss: 0.2968 - accuracy: 0.9170\n","Epoch 00159: val_accuracy did not improve from 0.91467\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2979 - accuracy: 0.9171 - val_loss: 0.3126 - val_accuracy: 0.9142\n","Epoch 160/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.2969 - accuracy: 0.9166\n","Epoch 00160: val_accuracy did not improve from 0.91467\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2977 - accuracy: 0.9167 - val_loss: 0.3125 - val_accuracy: 0.9144\n","Epoch 161/10000\n","176/188 [===========================>..] - ETA: 0s - loss: 0.2981 - accuracy: 0.9166\n","Epoch 00161: val_accuracy did not improve from 0.91467\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2975 - accuracy: 0.9171 - val_loss: 0.3122 - val_accuracy: 0.9146\n","Epoch 162/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.2969 - accuracy: 0.9168\n","Epoch 00162: val_accuracy did not improve from 0.91467\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2973 - accuracy: 0.9172 - val_loss: 0.3121 - val_accuracy: 0.9146\n","Epoch 163/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.9172\n","Epoch 00163: val_accuracy did not improve from 0.91467\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2971 - accuracy: 0.9172 - val_loss: 0.3119 - val_accuracy: 0.9145\n","Epoch 164/10000\n","186/188 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.9174\n","Epoch 00164: val_accuracy improved from 0.91467 to 0.91483, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2969 - accuracy: 0.9173 - val_loss: 0.3118 - val_accuracy: 0.9148\n","Epoch 165/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.2985 - accuracy: 0.9166\n","Epoch 00165: val_accuracy did not improve from 0.91483\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2967 - accuracy: 0.9172 - val_loss: 0.3116 - val_accuracy: 0.9147\n","Epoch 166/10000\n","184/188 [============================>.] - ETA: 0s - loss: 0.2967 - accuracy: 0.9171\n","Epoch 00166: val_accuracy did not improve from 0.91483\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2965 - accuracy: 0.9172 - val_loss: 0.3114 - val_accuracy: 0.9147\n","Epoch 167/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.2961 - accuracy: 0.9178\n","Epoch 00167: val_accuracy did not improve from 0.91483\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2963 - accuracy: 0.9176 - val_loss: 0.3113 - val_accuracy: 0.9147\n","Epoch 168/10000\n","174/188 [==========================>...] - ETA: 0s - loss: 0.2973 - accuracy: 0.9173\n","Epoch 00168: val_accuracy improved from 0.91483 to 0.91508, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2960 - accuracy: 0.9175 - val_loss: 0.3111 - val_accuracy: 0.9151\n","Epoch 169/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.2945 - accuracy: 0.9178\n","Epoch 00169: val_accuracy did not improve from 0.91508\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2958 - accuracy: 0.9176 - val_loss: 0.3111 - val_accuracy: 0.9151\n","Epoch 170/10000\n","178/188 [===========================>..] - ETA: 0s - loss: 0.2962 - accuracy: 0.9176\n","Epoch 00170: val_accuracy did not improve from 0.91508\n","188/188 [==============================] - 1s 4ms/step - loss: 0.2956 - accuracy: 0.9174 - val_loss: 0.3109 - val_accuracy: 0.9151\n","Epoch 171/10000\n","175/188 [==========================>...] - ETA: 0s - loss: 0.2957 - accuracy: 0.9171\n","Epoch 00171: val_accuracy did not improve from 0.91508\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2954 - accuracy: 0.9178 - val_loss: 0.3107 - val_accuracy: 0.9151\n","Epoch 172/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.2952 - accuracy: 0.9176\n","Epoch 00172: val_accuracy did not improve from 0.91508\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2952 - accuracy: 0.9178 - val_loss: 0.3106 - val_accuracy: 0.9151\n","Epoch 173/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9179\n","Epoch 00173: val_accuracy did not improve from 0.91508\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2950 - accuracy: 0.9180 - val_loss: 0.3105 - val_accuracy: 0.9148\n","Epoch 174/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.9177\n","Epoch 00174: val_accuracy did not improve from 0.91508\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2949 - accuracy: 0.9177 - val_loss: 0.3103 - val_accuracy: 0.9149\n","Epoch 175/10000\n","178/188 [===========================>..] - ETA: 0s - loss: 0.2929 - accuracy: 0.9187\n","Epoch 00175: val_accuracy did not improve from 0.91508\n","188/188 [==============================] - 1s 4ms/step - loss: 0.2947 - accuracy: 0.9179 - val_loss: 0.3101 - val_accuracy: 0.9147\n","Epoch 176/10000\n","170/188 [==========================>...] - ETA: 0s - loss: 0.2949 - accuracy: 0.9178\n","Epoch 00176: val_accuracy did not improve from 0.91508\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2945 - accuracy: 0.9179 - val_loss: 0.3100 - val_accuracy: 0.9148\n","Epoch 177/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.9179\n","Epoch 00177: val_accuracy improved from 0.91508 to 0.91525, saving model to mnist_baseline_best.h5\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2943 - accuracy: 0.9179 - val_loss: 0.3099 - val_accuracy: 0.9153\n","Epoch 178/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9180\n","Epoch 00178: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2941 - accuracy: 0.9180 - val_loss: 0.3097 - val_accuracy: 0.9151\n","Epoch 179/10000\n","186/188 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.9181\n","Epoch 00179: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2939 - accuracy: 0.9181 - val_loss: 0.3095 - val_accuracy: 0.9153\n","Epoch 180/10000\n","172/188 [==========================>...] - ETA: 0s - loss: 0.2925 - accuracy: 0.9188\n","Epoch 00180: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2937 - accuracy: 0.9181 - val_loss: 0.3094 - val_accuracy: 0.9149\n","Epoch 181/10000\n","184/188 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.9181\n","Epoch 00181: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2935 - accuracy: 0.9182 - val_loss: 0.3093 - val_accuracy: 0.9148\n","Epoch 182/10000\n","171/188 [==========================>...] - ETA: 0s - loss: 0.2941 - accuracy: 0.9183\n","Epoch 00182: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2934 - accuracy: 0.9181 - val_loss: 0.3091 - val_accuracy: 0.9148\n","Epoch 183/10000\n","186/188 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.9181\n","Epoch 00183: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2932 - accuracy: 0.9181 - val_loss: 0.3090 - val_accuracy: 0.9150\n","Epoch 184/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.9184\n","Epoch 00184: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2930 - accuracy: 0.9184 - val_loss: 0.3088 - val_accuracy: 0.9152\n","Epoch 185/10000\n","173/188 [==========================>...] - ETA: 0s - loss: 0.2924 - accuracy: 0.9183\n","Epoch 00185: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2928 - accuracy: 0.9184 - val_loss: 0.3088 - val_accuracy: 0.9152\n","Epoch 186/10000\n","170/188 [==========================>...] - ETA: 0s - loss: 0.2938 - accuracy: 0.9182\n","Epoch 00186: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2926 - accuracy: 0.9184 - val_loss: 0.3086 - val_accuracy: 0.9150\n","Epoch 187/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.9186\n","Epoch 00187: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 1s 3ms/step - loss: 0.2925 - accuracy: 0.9186 - val_loss: 0.3085 - val_accuracy: 0.9148\n","Epoch 00187: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UmWBNSEOE7hB","colab_type":"code","outputId":"fdd4d4e7-289f-45f7-d1d6-ff6ba523a118","executionInfo":{"status":"ok","timestamp":1590951477941,"user_tz":-60,"elapsed":1070,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["mnist_baseline_model.load_weights('mnist_baseline_best.h5')\n","loss, acc = mnist_baseline_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 0s 1ms/step - loss: 0.2900 - accuracy: 0.9193\n","Accuracy: 0.9193000197410583\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N6PWZvpcFmaq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":444},"outputId":"2e412aff-b111-47f5-9f73-9a8c2868212b","executionInfo":{"status":"ok","timestamp":1590951481232,"user_tz":-60,"elapsed":989,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_baseline_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_baseline_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_baseline_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_baseline_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":41,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"XVl2dGLwkDuO","colab_type":"text"},"source":["#### Multi-Layer Model\n","\n","To create a network with hidden layers, we simply add additional [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers before the output layer:"]},{"cell_type":"markdown","metadata":{"id":"doKYGKz05Dv1","colab_type":"text"},"source":["##### 1 Hidden Layer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7pl6Ha7g5B2K","outputId":"97ab3263-6f5d-404c-c075-d5ddaf528b8a","executionInfo":{"status":"ok","timestamp":1590951601920,"user_tz":-60,"elapsed":553,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":274}},"source":["mnist_hidden_model = tf.keras.Sequential(name='mnist_hidden')\n","mnist_hidden_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_hidden_model.add(tf.keras.layers.Dense(32, activation='tanh', name='hidden'))\n","mnist_hidden_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_hidden_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_hidden_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_hidden_model.summary()"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Model: \"mnist_hidden\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","hidden (Dense)               (None, 28, 28, 32)        64        \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                250890    \n","=================================================================\n","Total params: 250,954\n","Trainable params: 250,954\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iQT-3Ppw5B2O","outputId":"38561af7-069d-4919-8118-2b4af5935447","executionInfo":{"status":"ok","timestamp":1590952296151,"user_tz":-60,"elapsed":691917,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_hidden_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_hidden_model_train = mnist_hidden_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","187/188 [============================>.] - ETA: 0s - loss: 1.2566 - accuracy: 0.7655\n","Epoch 00001: val_accuracy improved from -inf to 0.84575, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 14s 72ms/step - loss: 1.2554 - accuracy: 0.7657 - val_loss: 0.7209 - val_accuracy: 0.8457\n","Epoch 2/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.5838 - accuracy: 0.8603\n","Epoch 00002: val_accuracy improved from 0.84575 to 0.87092, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 72ms/step - loss: 0.5837 - accuracy: 0.8603 - val_loss: 0.5020 - val_accuracy: 0.8709\n","Epoch 3/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4594 - accuracy: 0.8782\n","Epoch 00003: val_accuracy improved from 0.87092 to 0.88408, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.4591 - accuracy: 0.8783 - val_loss: 0.4322 - val_accuracy: 0.8841\n","Epoch 4/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4093 - accuracy: 0.8873\n","Epoch 00004: val_accuracy improved from 0.88408 to 0.89117, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 14s 72ms/step - loss: 0.4092 - accuracy: 0.8873 - val_loss: 0.3976 - val_accuracy: 0.8912\n","Epoch 5/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3818 - accuracy: 0.8927\n","Epoch 00005: val_accuracy improved from 0.89117 to 0.89367, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 72ms/step - loss: 0.3817 - accuracy: 0.8928 - val_loss: 0.3770 - val_accuracy: 0.8937\n","Epoch 6/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3639 - accuracy: 0.8968\n","Epoch 00006: val_accuracy improved from 0.89367 to 0.89658, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.3638 - accuracy: 0.8969 - val_loss: 0.3626 - val_accuracy: 0.8966\n","Epoch 7/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3507 - accuracy: 0.9001\n","Epoch 00007: val_accuracy improved from 0.89658 to 0.90017, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.3507 - accuracy: 0.9001 - val_loss: 0.3527 - val_accuracy: 0.9002\n","Epoch 8/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3411 - accuracy: 0.9025\n","Epoch 00008: val_accuracy improved from 0.90017 to 0.90183, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.3409 - accuracy: 0.9026 - val_loss: 0.3441 - val_accuracy: 0.9018\n","Epoch 9/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.9042\n","Epoch 00009: val_accuracy improved from 0.90183 to 0.90275, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 72ms/step - loss: 0.3328 - accuracy: 0.9042 - val_loss: 0.3380 - val_accuracy: 0.9028\n","Epoch 10/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3260 - accuracy: 0.9059\n","Epoch 00010: val_accuracy improved from 0.90275 to 0.90492, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.3262 - accuracy: 0.9058 - val_loss: 0.3325 - val_accuracy: 0.9049\n","Epoch 11/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.9079\n","Epoch 00011: val_accuracy improved from 0.90492 to 0.90700, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.3206 - accuracy: 0.9080 - val_loss: 0.3276 - val_accuracy: 0.9070\n","Epoch 12/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3156 - accuracy: 0.9093\n","Epoch 00012: val_accuracy improved from 0.90700 to 0.90817, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.3158 - accuracy: 0.9091 - val_loss: 0.3243 - val_accuracy: 0.9082\n","Epoch 13/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3117 - accuracy: 0.9107\n","Epoch 00013: val_accuracy improved from 0.90817 to 0.90892, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 72ms/step - loss: 0.3117 - accuracy: 0.9107 - val_loss: 0.3207 - val_accuracy: 0.9089\n","Epoch 14/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3079 - accuracy: 0.9122\n","Epoch 00014: val_accuracy improved from 0.90892 to 0.90950, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 14s 72ms/step - loss: 0.3079 - accuracy: 0.9121 - val_loss: 0.3182 - val_accuracy: 0.9095\n","Epoch 15/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.9131\n","Epoch 00015: val_accuracy improved from 0.90950 to 0.91083, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 72ms/step - loss: 0.3047 - accuracy: 0.9131 - val_loss: 0.3156 - val_accuracy: 0.9108\n","Epoch 16/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.9136\n","Epoch 00016: val_accuracy improved from 0.91083 to 0.91125, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.3017 - accuracy: 0.9135 - val_loss: 0.3132 - val_accuracy: 0.9112\n","Epoch 17/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2992 - accuracy: 0.9146\n","Epoch 00017: val_accuracy did not improve from 0.91125\n","188/188 [==============================] - 14s 72ms/step - loss: 0.2990 - accuracy: 0.9147 - val_loss: 0.3116 - val_accuracy: 0.9111\n","Epoch 18/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.9154\n","Epoch 00018: val_accuracy improved from 0.91125 to 0.91192, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 14s 72ms/step - loss: 0.2967 - accuracy: 0.9153 - val_loss: 0.3102 - val_accuracy: 0.9119\n","Epoch 19/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.9161\n","Epoch 00019: val_accuracy improved from 0.91192 to 0.91308, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 14s 72ms/step - loss: 0.2945 - accuracy: 0.9161 - val_loss: 0.3096 - val_accuracy: 0.9131\n","Epoch 20/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9164\n","Epoch 00020: val_accuracy improved from 0.91308 to 0.91367, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 14s 72ms/step - loss: 0.2924 - accuracy: 0.9164 - val_loss: 0.3062 - val_accuracy: 0.9137\n","Epoch 21/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.9173\n","Epoch 00021: val_accuracy improved from 0.91367 to 0.91400, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 14s 72ms/step - loss: 0.2905 - accuracy: 0.9171 - val_loss: 0.3062 - val_accuracy: 0.9140\n","Epoch 22/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9179\n","Epoch 00022: val_accuracy improved from 0.91400 to 0.91567, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 14s 72ms/step - loss: 0.2889 - accuracy: 0.9179 - val_loss: 0.3041 - val_accuracy: 0.9157\n","Epoch 23/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.9183\n","Epoch 00023: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 14s 72ms/step - loss: 0.2872 - accuracy: 0.9183 - val_loss: 0.3027 - val_accuracy: 0.9152\n","Epoch 24/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.9188\n","Epoch 00024: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 16s 85ms/step - loss: 0.2856 - accuracy: 0.9188 - val_loss: 0.3019 - val_accuracy: 0.9154\n","Epoch 25/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.9200\n","Epoch 00025: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 14s 74ms/step - loss: 0.2841 - accuracy: 0.9199 - val_loss: 0.3013 - val_accuracy: 0.9157\n","Epoch 26/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2827 - accuracy: 0.9201\n","Epoch 00026: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2828 - accuracy: 0.9201 - val_loss: 0.3002 - val_accuracy: 0.9155\n","Epoch 27/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2818 - accuracy: 0.9202\n","Epoch 00027: val_accuracy improved from 0.91567 to 0.91717, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2817 - accuracy: 0.9202 - val_loss: 0.2990 - val_accuracy: 0.9172\n","Epoch 28/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2806 - accuracy: 0.9207\n","Epoch 00028: val_accuracy did not improve from 0.91717\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2805 - accuracy: 0.9208 - val_loss: 0.2985 - val_accuracy: 0.9172\n","Epoch 29/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2795 - accuracy: 0.9212\n","Epoch 00029: val_accuracy improved from 0.91717 to 0.91733, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 14s 72ms/step - loss: 0.2793 - accuracy: 0.9212 - val_loss: 0.2976 - val_accuracy: 0.9173\n","Epoch 30/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2782 - accuracy: 0.9214\n","Epoch 00030: val_accuracy improved from 0.91733 to 0.91742, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 14s 72ms/step - loss: 0.2782 - accuracy: 0.9214 - val_loss: 0.2971 - val_accuracy: 0.9174\n","Epoch 31/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2773 - accuracy: 0.9219\n","Epoch 00031: val_accuracy did not improve from 0.91742\n","188/188 [==============================] - 14s 72ms/step - loss: 0.2771 - accuracy: 0.9219 - val_loss: 0.2967 - val_accuracy: 0.9172\n","Epoch 32/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2763 - accuracy: 0.9223\n","Epoch 00032: val_accuracy improved from 0.91742 to 0.91758, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2762 - accuracy: 0.9223 - val_loss: 0.2962 - val_accuracy: 0.9176\n","Epoch 33/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2748 - accuracy: 0.9224\n","Epoch 00033: val_accuracy did not improve from 0.91758\n","188/188 [==============================] - 13s 72ms/step - loss: 0.2752 - accuracy: 0.9223 - val_loss: 0.2956 - val_accuracy: 0.9169\n","Epoch 34/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2746 - accuracy: 0.9232\n","Epoch 00034: val_accuracy did not improve from 0.91758\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2744 - accuracy: 0.9232 - val_loss: 0.2954 - val_accuracy: 0.9170\n","Epoch 35/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2736 - accuracy: 0.9231\n","Epoch 00035: val_accuracy improved from 0.91758 to 0.91767, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2735 - accuracy: 0.9231 - val_loss: 0.2945 - val_accuracy: 0.9177\n","Epoch 36/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2725 - accuracy: 0.9234\n","Epoch 00036: val_accuracy improved from 0.91767 to 0.91800, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 70ms/step - loss: 0.2727 - accuracy: 0.9234 - val_loss: 0.2941 - val_accuracy: 0.9180\n","Epoch 37/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2720 - accuracy: 0.9239\n","Epoch 00037: val_accuracy did not improve from 0.91800\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2719 - accuracy: 0.9239 - val_loss: 0.2936 - val_accuracy: 0.9179\n","Epoch 38/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2712 - accuracy: 0.9239\n","Epoch 00038: val_accuracy improved from 0.91800 to 0.91842, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 72ms/step - loss: 0.2711 - accuracy: 0.9239 - val_loss: 0.2933 - val_accuracy: 0.9184\n","Epoch 39/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2702 - accuracy: 0.9239\n","Epoch 00039: val_accuracy improved from 0.91842 to 0.91850, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 14s 72ms/step - loss: 0.2703 - accuracy: 0.9240 - val_loss: 0.2928 - val_accuracy: 0.9185\n","Epoch 40/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2696 - accuracy: 0.9241\n","Epoch 00040: val_accuracy improved from 0.91850 to 0.91858, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2696 - accuracy: 0.9240 - val_loss: 0.2927 - val_accuracy: 0.9186\n","Epoch 41/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2690 - accuracy: 0.9242\n","Epoch 00041: val_accuracy improved from 0.91858 to 0.91942, saving model to mnist_hidden_best.h5\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2689 - accuracy: 0.9242 - val_loss: 0.2921 - val_accuracy: 0.9194\n","Epoch 42/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2683 - accuracy: 0.9249\n","Epoch 00042: val_accuracy did not improve from 0.91942\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2682 - accuracy: 0.9249 - val_loss: 0.2918 - val_accuracy: 0.9188\n","Epoch 43/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2676 - accuracy: 0.9251\n","Epoch 00043: val_accuracy did not improve from 0.91942\n","188/188 [==============================] - 13s 72ms/step - loss: 0.2676 - accuracy: 0.9251 - val_loss: 0.2916 - val_accuracy: 0.9181\n","Epoch 44/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.9249\n","Epoch 00044: val_accuracy did not improve from 0.91942\n","188/188 [==============================] - 13s 72ms/step - loss: 0.2669 - accuracy: 0.9249 - val_loss: 0.2907 - val_accuracy: 0.9188\n","Epoch 45/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2665 - accuracy: 0.9251\n","Epoch 00045: val_accuracy did not improve from 0.91942\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2663 - accuracy: 0.9252 - val_loss: 0.2904 - val_accuracy: 0.9186\n","Epoch 46/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2660 - accuracy: 0.9256\n","Epoch 00046: val_accuracy did not improve from 0.91942\n","188/188 [==============================] - 13s 70ms/step - loss: 0.2658 - accuracy: 0.9257 - val_loss: 0.2901 - val_accuracy: 0.9183\n","Epoch 47/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2655 - accuracy: 0.9259\n","Epoch 00047: val_accuracy did not improve from 0.91942\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2652 - accuracy: 0.9260 - val_loss: 0.2897 - val_accuracy: 0.9187\n","Epoch 48/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2647 - accuracy: 0.9264\n","Epoch 00048: val_accuracy did not improve from 0.91942\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2646 - accuracy: 0.9264 - val_loss: 0.2895 - val_accuracy: 0.9192\n","Epoch 49/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2642 - accuracy: 0.9262\n","Epoch 00049: val_accuracy did not improve from 0.91942\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2641 - accuracy: 0.9262 - val_loss: 0.2895 - val_accuracy: 0.9187\n","Epoch 50/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2635 - accuracy: 0.9263\n","Epoch 00050: val_accuracy did not improve from 0.91942\n","188/188 [==============================] - 13s 71ms/step - loss: 0.2635 - accuracy: 0.9264 - val_loss: 0.2892 - val_accuracy: 0.9193\n","Epoch 51/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2634 - accuracy: 0.9262\n","Epoch 00051: val_accuracy did not improve from 0.91942\n","188/188 [==============================] - 14s 73ms/step - loss: 0.2631 - accuracy: 0.9262 - val_loss: 0.2893 - val_accuracy: 0.9187\n","Epoch 00051: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x7JW9aCU5B2R","outputId":"7147db66-711f-4740-d748-c25aa28444c4","executionInfo":{"status":"ok","timestamp":1590952356233,"user_tz":-60,"elapsed":1914,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["mnist_hidden_model.load_weights('mnist_hidden_best.h5')\n","loss, acc = mnist_hidden_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 1s 4ms/step - loss: 0.2745 - accuracy: 0.9221\n","Accuracy: 0.9221000075340271\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e6GN749YGYEP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":444},"outputId":"09e2cd0d-c1b7-49b2-c105-165d9ba0c987","executionInfo":{"status":"ok","timestamp":1590952359533,"user_tz":-60,"elapsed":1101,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_hidden_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_hidden_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_hidden_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_hidden_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":48,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"ZprGelIpbqXK","colab_type":"text"},"source":["##### 2 Hidden Layers\n","\n"]},{"cell_type":"code","metadata":{"id":"0Nttf3UccCFZ","colab_type":"code","outputId":"8e5e61cc-9540-4e49-b2b8-4e8440586117","executionInfo":{"status":"ok","timestamp":1590952381809,"user_tz":-60,"elapsed":619,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":311}},"source":["mnist_hidden2_model = tf.keras.Sequential(name='mnist_hidden2')\n","mnist_hidden2_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_hidden2_model.add(tf.keras.layers.Dense(32, activation='tanh', name='hidden1'))\n","mnist_hidden2_model.add(tf.keras.layers.Dense(32, activation='tanh', name='hidden2'))\n","mnist_hidden2_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_hidden2_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_hidden2_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_hidden2_model.summary()"],"execution_count":49,"outputs":[{"output_type":"stream","text":["Model: \"mnist_hidden2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","hidden1 (Dense)              (None, 28, 28, 32)        64        \n","_________________________________________________________________\n","hidden2 (Dense)              (None, 28, 28, 32)        1056      \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                250890    \n","=================================================================\n","Total params: 252,010\n","Trainable params: 252,010\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JjJMRgmddrOR","colab_type":"code","outputId":"40b0a1b6-ccd1-4fb6-fe22-ce1eda89b03a","executionInfo":{"status":"ok","timestamp":1590953451343,"user_tz":-60,"elapsed":1068024,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_hidden2_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_hidden2_model_train = mnist_hidden2_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":50,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","188/188 [==============================] - ETA: 0s - loss: 1.1174 - accuracy: 0.7709\n","Epoch 00001: val_accuracy improved from -inf to 0.86408, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 1.1174 - accuracy: 0.7709 - val_loss: 0.5342 - val_accuracy: 0.8641\n","Epoch 2/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4486 - accuracy: 0.8765\n","Epoch 00002: val_accuracy improved from 0.86408 to 0.88642, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.4486 - accuracy: 0.8765 - val_loss: 0.4025 - val_accuracy: 0.8864\n","Epoch 3/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3764 - accuracy: 0.8917\n","Epoch 00003: val_accuracy improved from 0.88642 to 0.89792, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3764 - accuracy: 0.8917 - val_loss: 0.3631 - val_accuracy: 0.8979\n","Epoch 4/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.8994\n","Epoch 00004: val_accuracy improved from 0.89792 to 0.90275, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3468 - accuracy: 0.8994 - val_loss: 0.3430 - val_accuracy: 0.9028\n","Epoch 5/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3290 - accuracy: 0.9050\n","Epoch 00005: val_accuracy improved from 0.90275 to 0.90492, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3290 - accuracy: 0.9050 - val_loss: 0.3301 - val_accuracy: 0.9049\n","Epoch 6/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3172 - accuracy: 0.9085\n","Epoch 00006: val_accuracy improved from 0.90492 to 0.90783, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 28s 151ms/step - loss: 0.3172 - accuracy: 0.9085 - val_loss: 0.3219 - val_accuracy: 0.9078\n","Epoch 7/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3080 - accuracy: 0.9110\n","Epoch 00007: val_accuracy improved from 0.90783 to 0.90933, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 139ms/step - loss: 0.3080 - accuracy: 0.9110 - val_loss: 0.3163 - val_accuracy: 0.9093\n","Epoch 8/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3017 - accuracy: 0.9132\n","Epoch 00008: val_accuracy improved from 0.90933 to 0.91183, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3017 - accuracy: 0.9132 - val_loss: 0.3115 - val_accuracy: 0.9118\n","Epoch 9/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2962 - accuracy: 0.9150\n","Epoch 00009: val_accuracy improved from 0.91183 to 0.91350, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 25s 135ms/step - loss: 0.2962 - accuracy: 0.9150 - val_loss: 0.3088 - val_accuracy: 0.9135\n","Epoch 10/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2916 - accuracy: 0.9164\n","Epoch 00010: val_accuracy improved from 0.91350 to 0.91367, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.2916 - accuracy: 0.9164 - val_loss: 0.3036 - val_accuracy: 0.9137\n","Epoch 11/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9174\n","Epoch 00011: val_accuracy improved from 0.91367 to 0.91483, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.2878 - accuracy: 0.9174 - val_loss: 0.3037 - val_accuracy: 0.9148\n","Epoch 12/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9186\n","Epoch 00012: val_accuracy improved from 0.91483 to 0.91625, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 25s 135ms/step - loss: 0.2845 - accuracy: 0.9186 - val_loss: 0.2998 - val_accuracy: 0.9162\n","Epoch 13/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2818 - accuracy: 0.9189\n","Epoch 00013: val_accuracy improved from 0.91625 to 0.91650, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.2818 - accuracy: 0.9189 - val_loss: 0.2981 - val_accuracy: 0.9165\n","Epoch 14/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.9206\n","Epoch 00014: val_accuracy improved from 0.91650 to 0.91725, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.2791 - accuracy: 0.9206 - val_loss: 0.2989 - val_accuracy: 0.9172\n","Epoch 15/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.9211\n","Epoch 00015: val_accuracy did not improve from 0.91725\n","188/188 [==============================] - 26s 137ms/step - loss: 0.2769 - accuracy: 0.9211 - val_loss: 0.2963 - val_accuracy: 0.9172\n","Epoch 16/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.9218\n","Epoch 00016: val_accuracy improved from 0.91725 to 0.91817, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.2749 - accuracy: 0.9218 - val_loss: 0.2927 - val_accuracy: 0.9182\n","Epoch 17/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.9229\n","Epoch 00017: val_accuracy improved from 0.91817 to 0.91833, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.2730 - accuracy: 0.9229 - val_loss: 0.2934 - val_accuracy: 0.9183\n","Epoch 18/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.9233\n","Epoch 00018: val_accuracy improved from 0.91833 to 0.91917, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.2712 - accuracy: 0.9233 - val_loss: 0.2910 - val_accuracy: 0.9192\n","Epoch 19/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.9239\n","Epoch 00019: val_accuracy did not improve from 0.91917\n","188/188 [==============================] - 26s 137ms/step - loss: 0.2692 - accuracy: 0.9239 - val_loss: 0.2929 - val_accuracy: 0.9189\n","Epoch 20/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.9242\n","Epoch 00020: val_accuracy improved from 0.91917 to 0.91975, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.2681 - accuracy: 0.9242 - val_loss: 0.2912 - val_accuracy: 0.9197\n","Epoch 21/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.9244\n","Epoch 00021: val_accuracy did not improve from 0.91975\n","188/188 [==============================] - 25s 136ms/step - loss: 0.2665 - accuracy: 0.9244 - val_loss: 0.2904 - val_accuracy: 0.9191\n","Epoch 22/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2652 - accuracy: 0.9251\n","Epoch 00022: val_accuracy did not improve from 0.91975\n","188/188 [==============================] - 26s 137ms/step - loss: 0.2652 - accuracy: 0.9251 - val_loss: 0.2896 - val_accuracy: 0.9190\n","Epoch 23/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.9252\n","Epoch 00023: val_accuracy did not improve from 0.91975\n","188/188 [==============================] - 26s 136ms/step - loss: 0.2638 - accuracy: 0.9252 - val_loss: 0.2892 - val_accuracy: 0.9193\n","Epoch 24/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.9260\n","Epoch 00024: val_accuracy improved from 0.91975 to 0.92017, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 25s 136ms/step - loss: 0.2626 - accuracy: 0.9260 - val_loss: 0.2885 - val_accuracy: 0.9202\n","Epoch 25/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2616 - accuracy: 0.9260\n","Epoch 00025: val_accuracy improved from 0.92017 to 0.92083, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.2616 - accuracy: 0.9260 - val_loss: 0.2874 - val_accuracy: 0.9208\n","Epoch 26/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2605 - accuracy: 0.9271\n","Epoch 00026: val_accuracy did not improve from 0.92083\n","188/188 [==============================] - 26s 138ms/step - loss: 0.2605 - accuracy: 0.9271 - val_loss: 0.2862 - val_accuracy: 0.9204\n","Epoch 27/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.9272\n","Epoch 00027: val_accuracy did not improve from 0.92083\n","188/188 [==============================] - 26s 136ms/step - loss: 0.2592 - accuracy: 0.9272 - val_loss: 0.2871 - val_accuracy: 0.9197\n","Epoch 28/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2583 - accuracy: 0.9264\n","Epoch 00028: val_accuracy did not improve from 0.92083\n","188/188 [==============================] - 26s 136ms/step - loss: 0.2583 - accuracy: 0.9264 - val_loss: 0.2908 - val_accuracy: 0.9194\n","Epoch 29/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9272\n","Epoch 00029: val_accuracy did not improve from 0.92083\n","188/188 [==============================] - 26s 137ms/step - loss: 0.2576 - accuracy: 0.9272 - val_loss: 0.2886 - val_accuracy: 0.9192\n","Epoch 30/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.9272\n","Epoch 00030: val_accuracy did not improve from 0.92083\n","188/188 [==============================] - 29s 153ms/step - loss: 0.2565 - accuracy: 0.9272 - val_loss: 0.2866 - val_accuracy: 0.9201\n","Epoch 31/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.9279\n","Epoch 00031: val_accuracy improved from 0.92083 to 0.92283, saving model to mnist_hidden2_best.h5\n","188/188 [==============================] - 26s 140ms/step - loss: 0.2556 - accuracy: 0.9279 - val_loss: 0.2839 - val_accuracy: 0.9228\n","Epoch 32/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.9281\n","Epoch 00032: val_accuracy did not improve from 0.92283\n","188/188 [==============================] - 26s 141ms/step - loss: 0.2548 - accuracy: 0.9281 - val_loss: 0.2863 - val_accuracy: 0.9196\n","Epoch 33/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.9285\n","Epoch 00033: val_accuracy did not improve from 0.92283\n","188/188 [==============================] - 26s 139ms/step - loss: 0.2540 - accuracy: 0.9285 - val_loss: 0.2855 - val_accuracy: 0.9212\n","Epoch 34/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.9288\n","Epoch 00034: val_accuracy did not improve from 0.92283\n","188/188 [==============================] - 26s 138ms/step - loss: 0.2531 - accuracy: 0.9288 - val_loss: 0.2866 - val_accuracy: 0.9195\n","Epoch 35/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.9291\n","Epoch 00035: val_accuracy did not improve from 0.92283\n","188/188 [==============================] - 26s 138ms/step - loss: 0.2525 - accuracy: 0.9291 - val_loss: 0.2833 - val_accuracy: 0.9216\n","Epoch 36/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9290\n","Epoch 00036: val_accuracy did not improve from 0.92283\n","188/188 [==============================] - 26s 139ms/step - loss: 0.2514 - accuracy: 0.9290 - val_loss: 0.2880 - val_accuracy: 0.9210\n","Epoch 37/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.9292\n","Epoch 00037: val_accuracy did not improve from 0.92283\n","188/188 [==============================] - 26s 138ms/step - loss: 0.2511 - accuracy: 0.9292 - val_loss: 0.2842 - val_accuracy: 0.9202\n","Epoch 38/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.9293\n","Epoch 00038: val_accuracy did not improve from 0.92283\n","188/188 [==============================] - 26s 140ms/step - loss: 0.2501 - accuracy: 0.9293 - val_loss: 0.2870 - val_accuracy: 0.9199\n","Epoch 39/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9292\n","Epoch 00039: val_accuracy did not improve from 0.92283\n","188/188 [==============================] - 26s 138ms/step - loss: 0.2495 - accuracy: 0.9292 - val_loss: 0.2826 - val_accuracy: 0.9227\n","Epoch 40/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.9298\n","Epoch 00040: val_accuracy did not improve from 0.92283\n","188/188 [==============================] - 26s 140ms/step - loss: 0.2489 - accuracy: 0.9298 - val_loss: 0.2845 - val_accuracy: 0.9220\n","Epoch 41/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.9299\n","Epoch 00041: val_accuracy did not improve from 0.92283\n","188/188 [==============================] - 26s 141ms/step - loss: 0.2482 - accuracy: 0.9299 - val_loss: 0.2827 - val_accuracy: 0.9226\n","Epoch 00041: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KriCgmG4lbyL","colab_type":"code","outputId":"247699ca-b631-4ca8-f391-3715e8af7ef6","executionInfo":{"status":"ok","timestamp":1590936755326,"user_tz":-60,"elapsed":2835,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["mnist_hidden2_model.load_weights('mnist_hidden2_best.h5')\n","loss, acc = mnist_hidden2_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 2s 7ms/step - loss: 0.2641 - accuracy: 0.9257\n","Accuracy: 0.9257000088691711\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J_dxttFUGhAW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":444},"outputId":"0e7d9deb-c156-4e27-ab22-79fe05f9b674","executionInfo":{"status":"ok","timestamp":1590953910664,"user_tz":-60,"elapsed":1059,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_hidden2_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_hidden2_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_hidden2_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_hidden2_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":52,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Z-qd7LEaMUBg","colab_type":"text"},"source":["##### 3 Hidden Layers"]},{"cell_type":"code","metadata":{"id":"OzzjW0tmMoqU","colab_type":"code","outputId":"12a88305-5b36-422f-d54e-7ea4893c5a2e","executionInfo":{"status":"ok","timestamp":1590936769570,"user_tz":-60,"elapsed":551,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":348}},"source":["mnist_hidden3_model = tf.keras.Sequential(name='mnist_hidden3')\n","mnist_hidden3_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_hidden3_model.add(tf.keras.layers.Dense(32, activation='tanh', name='hidden1'))\n","mnist_hidden3_model.add(tf.keras.layers.Dense(32, activation='tanh', name='hidden2'))\n","mnist_hidden3_model.add(tf.keras.layers.Dense(32, activation='tanh', name='hidden3'))\n","mnist_hidden3_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_hidden3_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_hidden3_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_hidden3_model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"mnist_hidden3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","hidden1 (Dense)              (None, 28, 28, 32)        64        \n","_________________________________________________________________\n","hidden2 (Dense)              (None, 28, 28, 32)        1056      \n","_________________________________________________________________\n","hidden3 (Dense)              (None, 28, 28, 32)        1056      \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                250890    \n","=================================================================\n","Total params: 253,066\n","Trainable params: 253,066\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W7JyMur8NJTk","colab_type":"code","outputId":"76e8b4ed-5570-45b2-a378-666e0d4b41f4","executionInfo":{"status":"ok","timestamp":1590939524549,"user_tz":-60,"elapsed":1895999,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_hidden3_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_hidden3_model_train = mnist_hidden3_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","188/188 [==============================] - ETA: 0s - loss: 1.0047 - accuracy: 0.7785\n","Epoch 00001: val_accuracy improved from -inf to 0.87358, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 175ms/step - loss: 1.0047 - accuracy: 0.7785 - val_loss: 0.4531 - val_accuracy: 0.8736\n","Epoch 2/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3947 - accuracy: 0.8868\n","Epoch 00002: val_accuracy improved from 0.87358 to 0.89625, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 175ms/step - loss: 0.3947 - accuracy: 0.8868 - val_loss: 0.3645 - val_accuracy: 0.8963\n","Epoch 3/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3429 - accuracy: 0.9005\n","Epoch 00003: val_accuracy improved from 0.89625 to 0.90400, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 174ms/step - loss: 0.3429 - accuracy: 0.9005 - val_loss: 0.3366 - val_accuracy: 0.9040\n","Epoch 4/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3205 - accuracy: 0.9069\n","Epoch 00004: val_accuracy improved from 0.90400 to 0.90717, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 176ms/step - loss: 0.3205 - accuracy: 0.9069 - val_loss: 0.3220 - val_accuracy: 0.9072\n","Epoch 5/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.9113\n","Epoch 00005: val_accuracy improved from 0.90717 to 0.91033, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 175ms/step - loss: 0.3075 - accuracy: 0.9113 - val_loss: 0.3137 - val_accuracy: 0.9103\n","Epoch 6/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2983 - accuracy: 0.9141\n","Epoch 00006: val_accuracy did not improve from 0.91033\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2983 - accuracy: 0.9141 - val_loss: 0.3197 - val_accuracy: 0.9087\n","Epoch 7/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2918 - accuracy: 0.9164\n","Epoch 00007: val_accuracy improved from 0.91033 to 0.91392, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2918 - accuracy: 0.9164 - val_loss: 0.3077 - val_accuracy: 0.9139\n","Epoch 8/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.9185\n","Epoch 00008: val_accuracy improved from 0.91392 to 0.91650, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2862 - accuracy: 0.9185 - val_loss: 0.3020 - val_accuracy: 0.9165\n","Epoch 9/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2818 - accuracy: 0.9202\n","Epoch 00009: val_accuracy did not improve from 0.91650\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2818 - accuracy: 0.9202 - val_loss: 0.2969 - val_accuracy: 0.9161\n","Epoch 10/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2780 - accuracy: 0.9213\n","Epoch 00010: val_accuracy did not improve from 0.91650\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2780 - accuracy: 0.9213 - val_loss: 0.2980 - val_accuracy: 0.9158\n","Epoch 11/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.9220\n","Epoch 00011: val_accuracy improved from 0.91650 to 0.91758, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2750 - accuracy: 0.9220 - val_loss: 0.2960 - val_accuracy: 0.9176\n","Epoch 12/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.9234\n","Epoch 00012: val_accuracy improved from 0.91758 to 0.91917, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2721 - accuracy: 0.9234 - val_loss: 0.2934 - val_accuracy: 0.9192\n","Epoch 13/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2701 - accuracy: 0.9234\n","Epoch 00013: val_accuracy did not improve from 0.91917\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2701 - accuracy: 0.9234 - val_loss: 0.2925 - val_accuracy: 0.9183\n","Epoch 14/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.9243\n","Epoch 00014: val_accuracy did not improve from 0.91917\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2675 - accuracy: 0.9243 - val_loss: 0.2924 - val_accuracy: 0.9178\n","Epoch 15/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2659 - accuracy: 0.9249\n","Epoch 00015: val_accuracy improved from 0.91917 to 0.91958, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2659 - accuracy: 0.9249 - val_loss: 0.2879 - val_accuracy: 0.9196\n","Epoch 16/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.9245\n","Epoch 00016: val_accuracy did not improve from 0.91958\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2641 - accuracy: 0.9245 - val_loss: 0.2958 - val_accuracy: 0.9167\n","Epoch 17/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.9255\n","Epoch 00017: val_accuracy improved from 0.91958 to 0.92133, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2621 - accuracy: 0.9255 - val_loss: 0.2873 - val_accuracy: 0.9213\n","Epoch 18/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.9257\n","Epoch 00018: val_accuracy did not improve from 0.92133\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2607 - accuracy: 0.9257 - val_loss: 0.2905 - val_accuracy: 0.9198\n","Epoch 19/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2593 - accuracy: 0.9259\n","Epoch 00019: val_accuracy did not improve from 0.92133\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2593 - accuracy: 0.9259 - val_loss: 0.2954 - val_accuracy: 0.9164\n","Epoch 20/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.9272\n","Epoch 00020: val_accuracy did not improve from 0.92133\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2574 - accuracy: 0.9272 - val_loss: 0.2843 - val_accuracy: 0.9203\n","Epoch 21/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.9266\n","Epoch 00021: val_accuracy did not improve from 0.92133\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2565 - accuracy: 0.9266 - val_loss: 0.2881 - val_accuracy: 0.9207\n","Epoch 22/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.9277\n","Epoch 00022: val_accuracy did not improve from 0.92133\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2549 - accuracy: 0.9277 - val_loss: 0.2913 - val_accuracy: 0.9201\n","Epoch 23/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.9282\n","Epoch 00023: val_accuracy improved from 0.92133 to 0.92242, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2544 - accuracy: 0.9282 - val_loss: 0.2832 - val_accuracy: 0.9224\n","Epoch 24/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.9285\n","Epoch 00024: val_accuracy did not improve from 0.92242\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2532 - accuracy: 0.9285 - val_loss: 0.2833 - val_accuracy: 0.9212\n","Epoch 25/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9280\n","Epoch 00025: val_accuracy did not improve from 0.92242\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2514 - accuracy: 0.9280 - val_loss: 0.2851 - val_accuracy: 0.9202\n","Epoch 26/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9291\n","Epoch 00026: val_accuracy did not improve from 0.92242\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2509 - accuracy: 0.9291 - val_loss: 0.2856 - val_accuracy: 0.9201\n","Epoch 27/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.9283\n","Epoch 00027: val_accuracy did not improve from 0.92242\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2499 - accuracy: 0.9283 - val_loss: 0.2865 - val_accuracy: 0.9199\n","Epoch 28/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.9293\n","Epoch 00028: val_accuracy did not improve from 0.92242\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2485 - accuracy: 0.9293 - val_loss: 0.2892 - val_accuracy: 0.9195\n","Epoch 29/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.9305\n","Epoch 00029: val_accuracy did not improve from 0.92242\n","188/188 [==============================] - 33s 174ms/step - loss: 0.2477 - accuracy: 0.9305 - val_loss: 0.2850 - val_accuracy: 0.9216\n","Epoch 30/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.9291\n","Epoch 00030: val_accuracy improved from 0.92242 to 0.92267, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2472 - accuracy: 0.9291 - val_loss: 0.2819 - val_accuracy: 0.9227\n","Epoch 31/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.9304\n","Epoch 00031: val_accuracy did not improve from 0.92267\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2465 - accuracy: 0.9304 - val_loss: 0.2878 - val_accuracy: 0.9216\n","Epoch 32/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2457 - accuracy: 0.9300\n","Epoch 00032: val_accuracy improved from 0.92267 to 0.92292, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2457 - accuracy: 0.9300 - val_loss: 0.2829 - val_accuracy: 0.9229\n","Epoch 33/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.9304\n","Epoch 00033: val_accuracy did not improve from 0.92292\n","188/188 [==============================] - 33s 177ms/step - loss: 0.2444 - accuracy: 0.9304 - val_loss: 0.2869 - val_accuracy: 0.9202\n","Epoch 34/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.9305\n","Epoch 00034: val_accuracy did not improve from 0.92292\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2442 - accuracy: 0.9305 - val_loss: 0.2840 - val_accuracy: 0.9224\n","Epoch 35/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.9302\n","Epoch 00035: val_accuracy did not improve from 0.92292\n","188/188 [==============================] - 33s 177ms/step - loss: 0.2431 - accuracy: 0.9302 - val_loss: 0.2825 - val_accuracy: 0.9212\n","Epoch 36/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2427 - accuracy: 0.9311\n","Epoch 00036: val_accuracy did not improve from 0.92292\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2427 - accuracy: 0.9311 - val_loss: 0.2821 - val_accuracy: 0.9216\n","Epoch 37/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9311\n","Epoch 00037: val_accuracy improved from 0.92292 to 0.92308, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 175ms/step - loss: 0.2426 - accuracy: 0.9311 - val_loss: 0.2836 - val_accuracy: 0.9231\n","Epoch 38/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2416 - accuracy: 0.9317\n","Epoch 00038: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2416 - accuracy: 0.9317 - val_loss: 0.2834 - val_accuracy: 0.9210\n","Epoch 39/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9322\n","Epoch 00039: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2405 - accuracy: 0.9322 - val_loss: 0.2834 - val_accuracy: 0.9208\n","Epoch 40/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2393 - accuracy: 0.9313\n","Epoch 00040: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 33s 178ms/step - loss: 0.2393 - accuracy: 0.9313 - val_loss: 0.2828 - val_accuracy: 0.9220\n","Epoch 41/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.9319\n","Epoch 00041: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 33s 177ms/step - loss: 0.2392 - accuracy: 0.9319 - val_loss: 0.2816 - val_accuracy: 0.9223\n","Epoch 42/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2388 - accuracy: 0.9316\n","Epoch 00042: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 33s 176ms/step - loss: 0.2388 - accuracy: 0.9316 - val_loss: 0.2824 - val_accuracy: 0.9215\n","Epoch 43/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2379 - accuracy: 0.9317\n","Epoch 00043: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 33s 178ms/step - loss: 0.2379 - accuracy: 0.9317 - val_loss: 0.2823 - val_accuracy: 0.9224\n","Epoch 44/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9326\n","Epoch 00044: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 34s 179ms/step - loss: 0.2369 - accuracy: 0.9326 - val_loss: 0.2843 - val_accuracy: 0.9218\n","Epoch 45/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2368 - accuracy: 0.9321\n","Epoch 00045: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 33s 177ms/step - loss: 0.2368 - accuracy: 0.9321 - val_loss: 0.2910 - val_accuracy: 0.9197\n","Epoch 46/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9319\n","Epoch 00046: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 33s 177ms/step - loss: 0.2363 - accuracy: 0.9319 - val_loss: 0.2842 - val_accuracy: 0.9217\n","Epoch 47/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2352 - accuracy: 0.9332\n","Epoch 00047: val_accuracy improved from 0.92308 to 0.92317, saving model to mnist_hidden3_best.h5\n","188/188 [==============================] - 33s 177ms/step - loss: 0.2352 - accuracy: 0.9332 - val_loss: 0.2804 - val_accuracy: 0.9232\n","Epoch 48/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9331\n","Epoch 00048: val_accuracy did not improve from 0.92317\n","188/188 [==============================] - 33s 177ms/step - loss: 0.2349 - accuracy: 0.9331 - val_loss: 0.2793 - val_accuracy: 0.9224\n","Epoch 49/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2344 - accuracy: 0.9331\n","Epoch 00049: val_accuracy did not improve from 0.92317\n","188/188 [==============================] - 33s 177ms/step - loss: 0.2344 - accuracy: 0.9331 - val_loss: 0.2840 - val_accuracy: 0.9228\n","Epoch 50/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2340 - accuracy: 0.9338\n","Epoch 00050: val_accuracy did not improve from 0.92317\n","188/188 [==============================] - 33s 177ms/step - loss: 0.2340 - accuracy: 0.9338 - val_loss: 0.2808 - val_accuracy: 0.9229\n","Epoch 51/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9339\n","Epoch 00051: val_accuracy did not improve from 0.92317\n","188/188 [==============================] - 33s 177ms/step - loss: 0.2332 - accuracy: 0.9339 - val_loss: 0.2805 - val_accuracy: 0.9232\n","Epoch 52/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2324 - accuracy: 0.9336\n","Epoch 00052: val_accuracy did not improve from 0.92317\n","188/188 [==============================] - 33s 177ms/step - loss: 0.2324 - accuracy: 0.9336 - val_loss: 0.2816 - val_accuracy: 0.9227\n","Epoch 53/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2324 - accuracy: 0.9336\n","Epoch 00053: val_accuracy did not improve from 0.92317\n","188/188 [==============================] - 34s 178ms/step - loss: 0.2324 - accuracy: 0.9336 - val_loss: 0.2827 - val_accuracy: 0.9218\n","Epoch 54/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2315 - accuracy: 0.9337\n","Epoch 00054: val_accuracy did not improve from 0.92317\n","188/188 [==============================] - 34s 179ms/step - loss: 0.2315 - accuracy: 0.9337 - val_loss: 0.2843 - val_accuracy: 0.9217\n","Epoch 55/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9337\n","Epoch 00055: val_accuracy did not improve from 0.92317\n","188/188 [==============================] - 34s 179ms/step - loss: 0.2313 - accuracy: 0.9337 - val_loss: 0.2795 - val_accuracy: 0.9231\n","Epoch 56/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2308 - accuracy: 0.9345\n","Epoch 00056: val_accuracy did not improve from 0.92317\n","188/188 [==============================] - 33s 177ms/step - loss: 0.2308 - accuracy: 0.9345 - val_loss: 0.2854 - val_accuracy: 0.9205\n","Epoch 57/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9344\n","Epoch 00057: val_accuracy did not improve from 0.92317\n","188/188 [==============================] - 34s 179ms/step - loss: 0.2303 - accuracy: 0.9344 - val_loss: 0.2937 - val_accuracy: 0.9156\n","Epoch 00057: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0tNdg9yINe40","colab_type":"code","outputId":"8ab9e321-0f20-48e1-85c2-f71fea746c91","executionInfo":{"status":"ok","timestamp":1590939533986,"user_tz":-60,"elapsed":3819,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["mnist_hidden3_model.load_weights('mnist_hidden3_best.h5')\n","loss, acc = mnist_hidden3_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 3s 10ms/step - loss: 0.2641 - accuracy: 0.9252\n","Accuracy: 0.9251999855041504\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LBL4O4vNMjPu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":444},"outputId":"5ed52850-dcca-4d9b-cf97-3594c4e580b5","executionInfo":{"status":"ok","timestamp":1590953918369,"user_tz":-60,"elapsed":974,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_hidden3_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_hidden3_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_hidden3_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_hidden3_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":53,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABIEAAAGrCAYAAABXOYc2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdd3hUVf7H8feXVEgIBBJpCR1pKlKkCIKKWLBhWRV7r6xrV2zruvrTteCuZXdFVywoiB0VEVCKCiodJRQBQTqBhBJIz/n9cSYkQIAASSZMPq/nuc+duffO3O/kYdfJJ+d8jznnEBERERERERGR0FYt2AWIiIiIiIiIiEj5UwgkIiIiIiIiIlIFKAQSEREREREREakCFAKJiIiIiIiIiFQBCoFERERERERERKoAhUAiIiIiIiIiIlWAQiARERERERERkSpAIZCIHDQzW25mpwS7DhEREZHDlZlNMrN0M4sKdi0iEvoUAomIiIiIiASBmTUFTgAccE4F3je8ou4lIpWLQiARKVNmFmVm/zSzNYHtn4V/2TKzBDP7wsw2m1mamX1nZtUC5+43s9Vmts3MFplZ3+B+EhEREZFydyXwI/AmcFXhQTNLNrOPzSzVzDaZ2cvFzt1gZgsC35lSzKxT4Lgzs5bFrnvTzJ4IPD7RzFYFvm+tA4aZWXzge1lqYCTSF2aWVOz1dcxsWOD7XLqZfRo4/quZnV3suggz22hmHcvtpyQiZUYhkIiUtYeA7sCxQAegK/Bw4NzdwCogEagHPAg4M2sNDAKOc87VBE4Dllds2SIiIiIV7krg3cB2mpnVM7Mw4AtgBdAUaASMBDCzPwGPBV4Xhx89tKmU96oP1AGaADfifxccFnjeGMgEXi52/TtADaA9cATwQuD428Dlxa7rD6x1zs0uZR0iEkQaBigiZe0y4M/OuQ0AZvY34FXgESAXaAA0cc4tAb4LXJMPRAHtzCzVObc8GIWLiIiIVBQz64UPYEY55zaa2VLgUvzIoIbAvc65vMDl3wf21wPPOOemB54vOYBbFgB/dc5lB55nAh8Vq+dJYGLgcQPgDKCucy49cMnkwH448IiZxTnntgJX4AMjETkMaCSQiJS1hvi/XBVaETgG8Cz+y8o4M1tmZg8ABAKhO/B/2dpgZiPNrCEiIiIioesqYJxzbmPg+XuBY8nAimIBUHHJwNKDvF+qcy6r8ImZ1TCzV81shZltBaYAtQMjkZKBtGIB0E7OuTXAD8AFZlYbHxa9e5A1iUgFUwgkImVtDf6vWoUaB47hnNvmnLvbOdccP3z5rsLeP86595xzhX8Rc8A/KrZsERERkYphZtWBi4A+ZrYu0KfnTvxU+vVA4700b14JtNjL2+7AT98qVH+3826353cDrYFuzrk4oHdheYH71AmEPCV5Cz8l7E/ANOfc6r1cJyKVjEIgETlUEWYWXbgBI4CHzSzRzBKAR/HDhjGzs8yspZkZsAXIBwrMrLWZnRxoIJ2FH55cEJyPIyIiIlLuBuC/B7XD91E8FmiLnyo/AFgLPG1mMYHvWD0Dr3sduMfMOpvX0swK//g2B7jUzMLM7HSgz35qqIn/zrXZzOoAfy084ZxbC3wF/DvQQDrCzHoXe+2nQCfgL/geQSJymFAIJCKHagz+C0ThFg3MAOYBvwCzgCcC17YCJgAZwDTg3865ifh+QE8DG4F1+OaDgyvuI4iIiIhUqKuAYc65P5xz6wo3fGPmgcDZQEvgD/yiGhcDOOc+AJ7ETx3bhg9j6gTe8y+B123G92j8dD81/BOojv/+9SMwdrfzV+D7OS4ENuCn7hOoo7CfUDPg4wP87CISRObc7qMCRURERERERPbOzB4FjnTOXb7fi0Wk0tDqYCIiIiIiIlJqgelj1+FHC4nIYUTTwURERERERKRUzOwGfOPor5xzU4Jdj4gcGE0HExERERERERGpAjQSSERERERERESkCghaT6CEhATXtGnTYN1eREREytnMmTM3OucSg12H7ErfwURERELbvr6DBS0Eatq0KTNmzAjW7UVERKScmdmKYNcge9J3MBERkdC2r+9gmg4mIiIiEkRmdrqZLTKzJWb2QAnnm5jZN2Y2z8wmmVnSbufjzGyVmb1ccVWLiIjI4UghkIiIiEiQmFkY8ApwBtAOGGhm7Xa77DngbefcMcDjwFO7nf87oBV6REREZL8UAomIiIgET1dgiXNumXMuBxgJnLvbNe2AbwOPJxY/b2adgXrAuAqoVURERA5zCoFEREREgqcRsLLY81WBY8XNBc4PPD4PqGlmdc2sGvA8cM/+bmJmN5rZDDObkZqaWgZli4iIyOFIIZCIiIhI5XYP0MfMZgN9gNVAPnArMMY5t2p/b+CcG+qc6+Kc65KYqAXbREREqqqgrQ4mIiIiIqwGkos9Twoc28k5t4bASCAziwUucM5tNrMewAlmdisQC0SaWYZzbo/m0iIiIiKgEEhEREQkmKYDrcysGT78uQS4tPgFZpYApDnnCoDBwBsAzrnLil1zNdBFAZCIiIjsi6aDiYiIiASJcy4PGAR8DSwARjnn5pvZ42Z2TuCyE4FFZrYY3wT6yaAUKyIiIoc9jQQSERERCSLn3BhgzG7HHi32+EPgw/28x5vAm+VQnoiIiIQQjQQSEREREREREakCFAKJiIiIiIiIiFQBCoFERERERERERKoAhUAiIiIiIiIiIlWAQiARERERERERkSog9FYHW70atm6Ftm2DXYmIiIiIiIiIVCXOwR9/QGYmREUVbdHRfh8e3Bhmv3c3szeAs4ANzrmjSjhvwL+A/sAO4Grn3KyyLrTUHnoIJk6EFSuCVoKIiIiIiIiIVAFpafDzz3776Se/37hx79dXq+bDoF69YNy4iqszoDQR1JvAy8Dbezl/BtAqsHUD/hPYB0d0NGRlBe32IiIiIiIiIlJKzsHvv8OMGTBzJixcCLm5kJ9f8uYcxMRAXBzUqlXyPjLShy1mRfvij2Hv75+fDwUFfnPOb4WPC/c7dsCcOT70WbLEv58ZtGsHZ58NXbv6WrKzd92ysooeJycH5ce93xDIOTfFzJru45Jzgbedcw740cxqm1kD59zaMqrxwERH+x+oiIiIiIiIiJSN9HQYOxZycqBGjb1vERH7DmBSU33gUxj6zJjh3xv8a1u39r/Xh4XtukVG+r0ZbN/ug6MtW3w7mC1bfEBTkRo0gG7d4Lrr/L5zZx9AVXJlMRmtEbCy2PNVgWN7hEBmdiNwI0Djxo3L4NYl0EggERERERERqWqc8/vCkS5lYft2GD0aRozwAVBubtm9d3g4HHMMXHghdOniQ5SjjvJTpQ5U4eicLVv8lptb8giewsewZ8hUfKtWbc8gq/g+IgLi48vuZ1GBKrQjkXNuKDAUoEuXLq5cbhIV5UcCOVe2//hFRERERESk6snKgpUroX59qFkz2NUUKSiAefNg8mS/TZnimxE3bQpNmvh98cdNmkC9evv/PTknxwc+I0b4AGjHDmjUCP78Z7joIkhM9MdK2rZvh7y8koOXwn2tWj7wOfpoP4ijLJj5KWIxMdCwYdm8Z4gqixBoNVB8MltS4FhwFP4jys4uu39QIiIiIiIiEpqc8yHP0qV+itHu29rAJJdq1aBjR+jd22+9ekFCwr7fOy3N97hZuBA2bPDXJybCEUf4LTHRB0ulGcCQn+/70BQPfTZv9ueaNfO9aGrX9oskLV/u+9Wkpe36HmFhfspSSX104uJ84DV6tJ+eVbcuXHEFDBwIJ5zgP78c9soiBBoNDDKzkfiG0FuC1g8IFAKJiIiIiIhIyTIzYf58mDvXBypz5/rRNFu2FF1TrZpv2tusGZx+ut83buxDoilT4N//hhde8Ne2b+8Dkt69fQBTGPgUD372JyrKh0F16/qRMjk5RVtubtHj7GwfBAG0bAkXXAAnngh9+uy9yfDWrT4UKgyG1q4t6qGzdavf1q2DRYv84/x8OPNMH/z06+enPUlIKc0S8SOAE4EEM1sF/BWIAHDO/RcYg18efgl+ifhryqvYUikMfrKyfKIpIiIiIiIioWfNGvjuO/j+e1i1yoc3u/d1KXy8fbsPexYtKgpSYmKgQwe49FLfm6ZVKx/4JCfvO/zIzvbNjKdM8du778J//1t0vk4daNvWj8xp2xbatPFb/fqwaZNvjLxhw577tLSiBsiFW0TErs+POsoHTo0ale5nFBfnp10dffTB/5wlpJRmdbCB+znvgNvKrKJDVdhESs2hRURERESkKlu+3IcVJ5/sg4mDUdjj5WD/wF5Q4KcsZWfvOsKl+CiX3FyIjfUjaWrX9veKjNz1fZzzI2u+/74o+Pn9d3+uRg1o0cLfq3B5792X/I6K8gHK+efDscf68Kd584Ob4hQVBT17+m3wYP/zmTfP98Rp02bfU8RiYvyoIpEgqdDG0BWi+EggERERERGRqmT1avjgA3j/ffjxR38sKsqvwHT99X7qUGkaA3/9NQwf7vvDZGX5vjVJSX6UTHLyro+jo/19C7dVq4oer117cCtKVa9eFArFxcGSJX4UDfheOr16we23+32HDsGdthQeDp06Be/+IgcgdEOg7Ozg1iEiIiIiIlIRUlPhww998DNlih81c+yx8PTT0K2bPzd8uJ+21LKlD4OuvtqvFFXIOZg2zV83apQPXBIS4Lrr/MpSK1f6bdUqP+pl/fqiJcmLi4nxAVGjRr5fTaNG/j7R0SVPcYqIKJqutXmz71WzefOej885x/fe6dXLfwatBC1yUEI3BNJIIBERERERqayc86Nb6tY98KlaBQXwyy9+hagvv4RvvvFTntq0gcceg4svhtati64/8UR49lkfBr32GjzwADz8sO9Zc9llvjnyu+/CsmX+96kBA+Dyy+HUU/c+wiYnx/fkWbnS/+7VqJHf4uIU0IhUYqEXAqknkIiIiIiIVEbbtsGECfDVV35btcofb9YMunSBzp39vlMniI8vel1+vg9qii8Nnp7uz7VoAfff74Ofo4/eewBTvbpf7vuKK3xz5Ndfh7fegk8+8a/p2xcefRTOO88HOfsTGelHCDVteig/ERGpYKEXAmkkkIiIiIiIHIr8fD/KZdUq39Nm3bqS95mZPgRp0cJvzZsX7ZOT/TSnlBQYM8aHPt9/7/vj1KwJp5wCDz3kpzzNnOkbOH/wQVENLVr4QGj7dt8IuXAJ8xYtfFDTp4/fmjQ58M/XurUfGfTkk/6927aFhg3L5EcnIpVb6IZA6gkkIiIiIiJ7s3kzLF7sV5jafVuxYs9mxtWq+YbE9etDgwZ+SfGoKL8C17x58Nlnu74mPNyvclXYzPjoo+HOO+GMM+D44/dc/Qr8tbNm+UBoxgzf2Dkqyo/yKQx9Srs0eGlERvoRQCJSZYReCKTpYCIiIiIisjvn/KicL77w29SpvrdOoYQEP4Knc2e/klazZn40T4MGPvhJTPQje/YmP9+vhrV0qe+ts3Spb57cvbsPfpKS9l9j3brQr5/fRETKQeiFQJoOJiIiIiISmlJT/ZSpESP8FKoWLaBdO7+1bev3rVoV/WE4O9v30CkMfn7/3R/v2BEefBCOO86HPc2aQWzsodUWFgaNG/vtpJMO7b1ERMqJQiAREREREam8tm71zYtHjPBNlfPzoX17uPZaP22rsJdO4XLlYWE+HEpKgp9+8j11qlf3PXgeeADOPLNsp1SJiBxGQjcEUk8gEREREZHDS0GBD222bPH9cEaM8EugZ2f7Bsz33QcDB/r+OsVlZvoVrxYs8FO+UlJ8QHTllXDWWX5kTvXqQflIIiKVSeiFQOoJJCIiIiISPM75njgbNvjmy3vbtmzxo3wKty1b/BLqhSN6AOrVg5tu8sFPt277Xv782GP9JiIiexV6IZCmg4mIiIiIHBrn9h64lGT9ej9Va/x4v61ZU/J1kZFQu7ZfNat2bYiL842X4+L8seL7li2hd+99N2MWEZEDEnohUHi4X75R08FERERERPbOOb+aVfEpVIWPt23z068KmyYX35o39394nTKlKPT55Rf/nnXr+iXHTz4ZmjTxQU/xrfAPtiIiEhShFwKZ+f+4aCSQiIiIiEiRTZtg4kT45huYPbso7CkUH+9X1zr/fD8aZ/lyv5rW9OmQlrbre5n5ECkyEnr1gqee8suad+zo/yArIiKVUuiFQOD7AikEEhEREZGqbMcO+P57H/pMmOCDH+egZk3o3Nk3TS6+tPoRR+x9CtjWrT4QKtw2b4aePX0AVKNGxX4uERE5aKEZAmkkkIiIiIiEmrw8GDcOvvrKL5MeFlbylp/vV9aaOhVyciAiAo4/Hv72N79Mepcu/tiBiIuDDh38JiJVVmZuJtHh0diB9AwrQ7n5uWTkZFAruhbVTKMOD0bohkDqCSQiIiIioWDxYhg2DN5+2zdcjonx33cLCnzgs/vmnF8l6/bbfejTq5d/jUgZ256zndGLRlPgChh49MBK90u5c45pq6bx6sxXSUlN4a7ud3HJUZcELcA4XBW4AiYsm8CrM1/ls4WfUTOqJp0bdKZLwy47tya1muzz57olawurtq5i5daVbNqxiZz8HHILcsnJz9ljy8rLIj0znfSswBZ4nJaZRkZOBgBRYVE0i29Gi/gWNI9vvnNfuFWPqF4uP4tf1v/CsDnDmLR8Ejd1vokbO9942P17Ct0QSCOBRERERORwlZEBH3wAb7zhp3RVqwb9+8NLL8FZZ/lePFKuZq6ZybqMdfRt3pfo8NBqaL0+Yz1fLfmKL3/7ksnLJ9OqbivObHUmZ7Y6k2PqHbPPX2rzCvL49vdvGT5vOB8v+JjtudsBeGvuW7w14C0a1GxwQLVk5WXx+aLPObLukfu9d2ltydrC8HnD+e/M//Lrhl+pGVmTRnGNuPTjS/nnT/9kyKlD6Nm45yHfpzTyC/LZlLmJ9RnryXf5tE9sT0TYgY3Ec86xJG0J01ZNIy4qjjYJbWgR3+KA3+dAbdi+gWGzhzF01lCWpS8joUYCg7oOIjM3kxlrZzBk2hByC3IBqFO9Dl0adqFzg85EhUWxcuvKnaHPyi0r2ZazbT938wwjOjya+OrxxEfHE189nia1m3Bs9LE7n9eMrMnajLUsTV/KsvRlTFkxZY/3b1WnFccnH0+PpB4cn3w87RLbEVbt4FYaTM9MZ8SvIxg2Zxgz1swgoloEreq24uYvb2b04tH875z/UT+2/kG9dzCYcy4oN+7SpYubMWNG+bx5x46QnAyjR5fP+4uIiMh+mdlM51yXYNchuyrX72By8PLzYelSmDMHxo6FUaNg+3Zo3RquuQauuAIaNgx2lVVCemY690+4n9dmvQZAXFQc57Y+l4vaX0S/5v2ICo8KcoUHrsAVMHPNTL787Uu+/O1LZqzx/x/QsGZDTm52MgtSFzBz7UwAGtVsRP9W/enfqj+nND+F2MhYnHPMXjeb4fOGM+LXEazLWEetqFr8qd2fuPyYy1m4cSF3fn0nMZExDDt3GGcdedZ+a3LO8dmiz7jr67v4ffPvADSPb86A1gMY0GYAxycff0C/tDvnmL5mOq/OeJWR80eyI3cHnRt05uYuN3PJUZdQI6IG78x9hwe/fZA129bwp3Z/4ulTnqZ5fPOD+InuakvWFkYvGs3c9XNZv3096zPW79yn7kilwBXsvLZ6eHWOa3TcznCiR1IPEmMS9/gsizYtYtLySUxeMZnJyyezNmPtLteEVwunZZ2WtEloQ5u6bWib2JY2CW04pt4xhxRaOueYvGIy/53xXz5e8DG5Bbn0btKbmzvfzPltz9/l3392Xja/bPiFGWtmMHPNTGasncGvG34lryCP+rH1SYpLIjku2W+1/D4pLonEmESiwqKIDIskIiyCyLBI/7haxEEFNc45NmVuYmmaD4WWpC1h5tqZTF05ldQdqQDUjKxJ96Tu9EjqQY/kHrRLbEdsZCyxkbFEhu0Zqhe4Ar5Z9g3D5gzj4wUfk52fzTH1juG6jtdx6dGXUqd6HV75+RXum3AfMRExvHb2a5zX9ryD/rmXtX19BwvNEKhHD9/wbty48nl/ERER2S+FQJWTQqBKICMD5s2DuXP9NmeOX2J9xw5/PjYWLr4Yrr3Wf689zKYaHK6cc7w//33uGHsHG3ds5M7ud3JSs5P4KOUjPl74MZuzNlMrqhYD2gzg4vYX07d53xJ/eaxMft3wK89Pe54xv41hw/YNVLNqdE/qTv+W/TnzyDPpUK/DzpE36zLW8dVvfnTQuKXj2JazjciwSHo36c3qratZsHEBEdUiOOvIs7j8mMvp36r/LmHDgtQFXPrxpcxZN4fbjruNZ/s9u9cpOSmpKdwx9g7GLxtP+8T2PNX3KdZlrOPTRZ8yYdkEcvJzSKyRyDmtz2FAmwGc0vwUosOjKXAFpGWm7RKyrMtYx/rt6xm3dByz180mJiKGS4++lJs630Tnhp33uPf2nO08N/U5npn6DHkFedze9XYe6v0QtaNrH9DPdmv2Vj5f9DmjUkYxdslYcvJziA6Ppl5MPerF1qN+bH3/OPC8Xkw98l0+P636iamrpjJ77eydo2haxLfg+OTjaZvQltnrZjNlxRTWb18PQIPYBvRp2oc+TfrQq3EvMnMzWbhxIQs2LmDhxoUs3LiQ39J+I68gD/Ah3jP9nmHgUQMPaFRVgSvgvV/e44kpT7Bo0yJqR9fmqg5XcVPnm2ib2LbU75Odl42ZVYr/bTjnWJa+jKkrpzJt1TSmrpzKLxt+2SWUAx+oxUbGEhMR4/eRMWzYvoFVW1cRHx3PZUdfxjUdr6Fj/Y57/EwXpC7g8k8uZ9baWVxz7DX88/R/EhcVV5Efs0RVLwQ66SQ/R3ry5PJ5fxEREdkvhUCVk0KgIMjJgR9+8A2dx46FX3/1fXsAatf2/Xs6dCjat2vnV7s9TM1aO4t+7/SjS8Mu3NT5Js4+8uxyn7ZyqH5P/51bx9zK2CVj6dKwC6+d/RrH1j925/mc/BwmLJvAqPmj+HThp2zJ3kJ8dDzntD6Hk5qeRJ+mfWhau2nwPsBu1mes59GJj/L67NeJjYzlrCPPon/L/pze8nTq1qi739fn5Ofwwx8/MOa3MYxdOpb46HguP+ZyLmx3IXWq19nr67LzsnnwmwcZ8uMQ2ie2Z8QFIzi63tE7z2/O2szfJv2Nl35+iZpRNfnbiX/jli637PLvY2v2VsYuGcunCz/ly9++ZGv2VmIiYoiLimPD9g3ku/w97htRLYKjjjiKGzrdwGXHXFaqX8LXbFvDw98+zJtz3qRO9To80vsRuiV12znlKD46fo9/txk5GXyx+AtGzR/FmN/GkJ2fTVJcEn9q9ycuan8RXRt1LXVfpMzcTGauncm0ldOYumoq01ZOY/329STFJXFi0xPp08QHPy3rtNxvmJObn8vvm39n3vp5PP3908xcO5OeyT158YwX6dSg0z5f65xj7JKxDP5mMHPXz+XY+sdyZ/c7+VO7P5VbX51g2pa9jZ9X/8yy9GVsz93O9pztZORksD13131UWBQXt7+Yc9ucu9+RVTn5Ofx98t/5v+//j8a1GvP2gLc5ockJFfSJSlb1QqAzzoC0NPjpp/J5fxEREdkvhUClY2anA/8CwoDXnXNP73a+CfAGkAikAZc751aZ2bHAf4A4IB940jn3/v7upxCogqxa5UOfr77yy7Nv2+ZX5DrhBOjTx7cv6NDBtzAIoZE+G3dspPPQzuTk5xBeLZxVW1fRILYB13W8jus7XU+T2k3K/J75Bfmk7kjdZXTI+u3rSc9Mp3Gtxn66TEIbjog5Yo9fpnPzc3nhxxd4bNJjhFUL48mTn+S2427b55SU7Lxsxi8bz6j5o/jyty9Jy0wDoEmtJjtHbPRp0ofm8c0rvGFsZm4mL/z4Ak99/xRZeVkMOm4Qj/R5ZJ/BTXn4esnXXPXpVWzO2sxzpz7HLV1uYdicYTz4zYNs3LGRGzrdwBMnP7HHNKjdZedlM2n5JD5f/DlZeVm7jKopvo+Pjj/on/WcdXO46+u7mLh84h7nYiJiiK8eT53qdYiNjGXW2llk5WXRsGbDncFP96TuZdIQ2znH5qzN1I6ufUj/bgpcAcNmD2PwN4PZuGMj13e6nidPfrLEn/XPq3/m/gn3M2n5JJrHN+fJk5/kovYXVboG34eLaSunccUnV7AsfRn39byPv534t6BNH616IdCAAfD77354rYiIiASFQqD9M7MwYDHQD1gFTAcGOudSil3zAfCFc+4tMzsZuMY5d4WZHQk459xvZtYQmAm0dc5t3tc9FQKVo7Vr4ZVX4LPP/Ggf8CHPGWf4ps4nn+xbFoSovII8Tht+Gj/88QPfX/s9x9Y/lq9++4pXZ77KmN/GANC/VX9u6nwT/Vv1P+gmrQBjfhvDQ98+xOqtq9m4YyOOPX+nMWyX47Wja9MmoQ1tE3zvlIY1G/Ls1GeZt34eA9oM4KUzXiIpLumA6ihwBczfML+od8uKyWzcsRHw03J6Nu7JETWOICYyZo/pJvt6fqBLcBe4Akb8MoLB3wxm5daVnNfmPP5xyj9oVbfVAX2esrRh+wau+ewaxvw2hiNijmDD9g30atyLF09/kY4NOgatrpI455i7fi5rt63duQpV8dWp0jLT2JK1haOOOIqL219Mz8Y9K31QsjlrM49PfpyXfn6JmIgY/nbi37j1uFuJCItg8abFPPTtQ3yY8iGJNRJ5tM+j3Nj5xkoxhetwl5GTwd1f383QWUO5uP3FjLhgRFBWD6t6IdAll8Ds2bBoUfm8v4iIiOyXQqD9M7MewGPOudMCzwcDOOeeKnbNfOB059xK898ktzjn9pjrYGZzgQudc7/t654KgcrBkiXw7LPw5puQl+dH+vTv78Ofdu0Ou5E+ufm5ZOVlUTPqwAKre8fdy3PTnmPYucO4+tirdzm3YvMKXp/1Oq/Pfp11GetIikvi6b5Pc9kxlx1wfROWTeDM986kRXwLTmh8QokjQ+rF1KNmVM2dvWwKe6cUboVNdhvVbMTL/V9mQJsBB1xHSZxzpKSm7AyEfl79M1uytrA9dzs5+Tmlfp9qVo2YiBhiImNIqJGwS4PdpLikXZrszlo7i7vH3c30NdPp1KATQ04dQp+mfcrk8xwq5zMPd0YAACAASURBVBwv//wyb859k3t63KPl2YNgQeoC7vj6DsYtHUe7xHZ0a9SNt+e+TXR4NPcefy939bjrgP+3Lvv39PdPM/ibwbzS/xVuPe7WCr9/1QuBrr4aJk2C5cvL5/1FRERkvxQC7Z+ZXYgPeK4PPL8C6OacG1TsmveAn5xz/zKz84GPgATn3KZi13QF3gLaO7dbx0t//kbgRoDGjRt3XrFiRXl+rKpjzhx4+mm/lHtEhF/F6957ofmhrza0N/kF+SzetJjm8c3LfJpBRk4GQ2cOZci0IWzP3c67579L/1b9S/Xakb+OZOBHA7ntuNt4uf/Le70uNz+Xzxd/zjM/PMNPq3/i2X7Pcs/x95S6xqkrp9LvnX60rNOSiVdNPOhpTluytrA0fSlH1j2S2MjYg3qPA5Wbn1vUdyTQh6SwB8kefUmKnU/dkbpzue0N2zeU+N6Najbiqb5Pcdkxl1X6ESpS8ZxzfL74c+78+k7+2PIHN3e+mYd7P0y92HrBLi1kFbgCzh5xNhOWTWDqtVNLbFJenqpeCHTTTX4Y7rp15fP+IiIisl8KgfavlCFQQ+BloBkwBbgAOKpw2peZNQAmAVc5537c3z01EugQOQfffefDn6++8tO7br0V/vIXaNCg3G6bnpnO/2b/j1emv8LyzcuJDIukc4PORctMJ/egYc2DW0J+045NvPjTi7z080ukZ6VzYtMTSc9MZ976efy1z195pM8j+wwW5q2fR/fXu9OpQSe+verbUk0pyc7L5vJPLufDlA+5v+f9PNX3qf2OEJm1dhYnv3UyR8QcwXfXfFclf4HNysti9dbVO0OhlVtWEhMZw/WdrqdGRI1glyeVXG5+Lpl5mZVi9aqqYNOOTXR8tSPh1cKZddOsA16B7lDs6ztYeIVVUZGioyErK9hViIiIiOzPaiC52POkwLGdnHNrgPMBzCwWuKBYABQHfAk8VJoASA7RlCnw4IN+pa/ERPi//4NbbvErfJWTX9b/wks/v8TwecPJzMukd5PePNDzAZamL2Xqyqm8Mv0Vhvw4BIDGtRr7QCipB+0S29E8vjmNazUmvFrJX/lXblnJkGlDGDprKDtyd3Bu63N5oNcDdE/qzo7cHdzy5S08Nvkxpq+ZzjvnvUN89fg93iMtM43z3j+P+OrxfHjRh6XuKRIVHsXIC0ZyW/Xb+McP/2Djjo3896z/7rXWlNQUTht+GrWiazHhyglVMgACiA6PpkWdFrSo0yLYpchhKCIsotKv1BdK6taoy/sXvk/vN3tz7WfX8tFFH1WK6ZAKgURERESCZzrQysya4cOfS4BLi19gZglAWmCa12D8SmGYWSTwCfC2c+7DCq26qlmyBO67Dz75BJKS4OWX4dproXr5LJ+cV5DH6EWjefGnF5m8YjLVw6tz2dGX8eduf+aYesfscm1Ofg6z185m6sqpTFs1je9WfMfIX0fuPB9mYTSp3YQW8S1oHt+cFvEtaFK7CWOXjGX4vOEUuAIuPfpS7u95P+2PaL/zdTUiavDmuW/SrVE37hh7B11e68InF3+yy/3zC/K59KNLWbllJVOumUL92PoH9DnDqoXxnzP/Q2KNRJ747gnSMtN474L39liOeVn6Mvq904/wauF8c+U3NK7V+IDuIyISLD2Se/CPU/7B3ePu5l8//Ys7ut8R7JJCOATKzvbDdStB0iYiIiJSEudcnpkNAr7GLxH/hnNuvpk9Dsxwzo0GTgSeMjOHnw52W+DlFwG9gbpmdnXg2NXOuTkV+RlCWno6/P3vPvSJjIQnnoA774Qa5TPtZlv2NobOHMq/fvoXK7eupEmtJjxzyjNc1+m6vfa+iQyLpFtSN7oldeNO7gRg9dbVLN60mGXpy1iavnTn/oOUD3YuZx4dHs1NnW/inuPv2euy7WbGrcfdSsf6Hbnwgwvp/np3Xjv7tZ3NnB+Z+AhfL/2aoWcNpXtS94P6zGbG30/+O3Vr1OXOr++k/7v9+fSST3dOV1m9dTV93+5LVl4Wk6+eTMs6LQ/qPiIiwXJn9zuZsmIK946/lx5JPeiW1C2o9YRmT6D/+z946CE/GiiqbBvmiYiISOmoJ1DlpJ5ApZCTA//5Dzz+OGzeDNdd5x/XP7CRLqW1ccdGXvzpRV7++WXSs9I5qelJ/KXbXzjryLMOaRn1kmzO2szv6b+TXCuZhBoJpX7duox1XPzhxUxZMYXbu95Oj+QeDPxoIDd0uoGhZw8tk9remfsO13x2DcfWP5Yxl/kl5fu82YfVW1fz7VXf0qWh/u9ERA5P6ZnpdBraiQJXwOybZh90U/vSqpo9gUAhkIiIiIiUnnMwerRf4eu336BfP3j+eTj6aMBPf/ph5Q/ERMSQXCuZxBqJh9Tf4Y8tf/D81Od5bdZrZOZlcl6b83ig1wN0bdS1rD7RHmpH16Zjg44H/Lr6sfWZcMUE7p9wPy/8+AIv/vwi3Rp146UzXiqz2q7ocAV1qtfhwg8u5IRhJ1A9vDorNq/g68u/VgAkIoe1+OrxjLpwFD3f6MlVn17FZ5d8FrSV/EI/BKpVK7i1iIiIiEjlt2gRDBoEEyZA27YwZgycfvrO1gLLNy/nyk+u5Ls/vtv5kqiwKJLikkiulUxyXLJ/HJdMYkwi8dHxxFePp071OsRHxxMXFbczMFqQuoBnpj7D8HnDAbj8mMu57/j7aJvYtuI/9wGICItgyGlD6NqoK2/OeZP/nfO/Ml+m/swjz2T8FeM5672zyMzLZPQlozmhyQlleg8RkWA4rtFxPH/q89w+9naen/o89/a8Nyh1hHYIlJ0d3DpEREREpHLbscP3+nnuOd/r56WX4OabIdx/TXbO8dbct7j9q9sB+M+Z/6FeTL2iJbq3rmTllpVMXjGZNdvWkFeQV+Jtqlk1akfXplZULZZvXk71iOrcdtxt3NXjrsOu0fElR13CJUddUm7v36txL2bdNIstWVsOatSSiEhlNajrIKb8MYXB3wzm+OTj6dm4Z4XXEJohUOEUMK0QJiIiIiIlKZz69Ze/wIoVcOWV8MwzUK9o6fHU7anc9MVNfLLwE3o36c3bA97eaxNl8NPF1m9fz6Ydm0jPSictM430zHTSs9J37tMy07iqw1Xc1vW2A+rJU9U0j28e7BJERMqcmfH62a8ze+1sbv7yZubePLfCp4WFZghUfDqYiIiIiEhxy5bB7bfDl1/CUUfBlClwwq5Tjr5c/CXXjb6O9Kx0nu33LHd2v3O/TZrDqoXRsGZDGtZsWJ7Vi4jIYaxWdC0+veRTYiNjg9IXqFR3NLPTzWyRmS0xswdKON/EzL4xs3lmNsnMksq+1AOg6WAiIiIisrusLL/KV/v2MHmynwI2a9YuAVBGTgY3f3EzZ404i3qx9ZhxwwzuOf6eMl+lS0REqq6jjjiKprWbBuXe+x0JZGZhwCtAP2AVMN3MRjvnUopd9hzwtnPuLTM7GXgKuKI8Ci4VjQQSERERkeIWLoQLL4T58+Gii2DIEGjUaJdL5m+Yz4D3B7A0bSn3HX8fj5/0eJk3PhYREQmm0kwH6woscc4tAzCzkcC5QPEQqB1wV+DxRODTsizygKknkIiIiIgUGjkSrr8eqlf3q36dccYel+zI3cEFoy5gW/Y2Jl41kT5N+wShUBERkfJVmulgjYCVxZ6vChwrbi5wfuDxeUBNM6u7+xuZ2Y1mNsPMZqSmph5MvaWjkUAiIiIikp0Nt90GAwdChw4we3aJARDA3V/fzaJNi3j3/HcVAImISMgqqy5E9wB9zGw20AdYDeTvfpFzbqhzrotzrktiYmIZ3boE6gkkIiIiUrUtXw69esG//w133w2TJkFSyW0rP1/0Of+d+V/u7nE3fZv3rdAyRUREKlJppoOtBpKLPU8KHNvJObeGwEggM4sFLnDObS6rIg+YRgKJiIiIVF1ffOGXfC8ogE8+gQED9nrp+oz1XDf6OjrU68CTJz9ZgUWKiIhUvNKMBJoOtDKzZmYWCVwCjC5+gZklmO1c22ww8EbZlnmA1BNIREREpOrJy4PBg+Hss6FpU5g5c58BkHOOa0dfy7acbbx7/rtqAi0iIiFvvyGQcy4PGAR8DSwARjnn5pvZ42Z2TuCyE4FFZrYYqAcE988oGgkkIiIiUrVkZfl+P08/DTfeCFOnQosW+3zJv6f/mzG/jeGZU56h/RHtK6hQERGR4CnNdDCcc2OAMbsde7TY4w+BD8u2tEOgnkAiIiIiVYdzcMMNMGECvP46XHfdfl+SkprCPePv4fSWpzOo66AKKFJERCT4yqoxdOWikUAiIiIiVcfTT5P73nCeeLwvb3QysvL2/R0wOy+byz6+jNjIWIadOwwzq6BCRUREgis0Q6DwcDBTCCQiIiIS6j7+mPTHH+T0e+rxSME3XDf6Opr+sylPTHmCjTs2lviSRyY+wpx1c/jfOf+jfmz9Ci5YREQkeEIzBDLzo4EUAomIiIiErtmzWTboMo4fFM13sWm8ee6bTLhiAp0adOKRiY/Q+IXG3PLFLSzetHjnSyb+PpHnpj7HjZ1u5JzW5+zjzUVEREJPqXoCHZaio9UTSERERCRUrV3LDzecxoArcyioXYsJAz+ld5PeAPRt3pf5G+bzwo8v8MacN3h15quc3fpsbu58Mzd+cSOt6rZiyGlDgvwBREREKl5ojgQCv0y8RgKJiIiIhJ7MTN67pRcnn5FKfEISP97w084AqFD7I9rz+jmv88cdf/Bw74f54Y8f6P9ef9ZlrOPd898lJjImSMWLiIgET2iPBFIIJCIiIhJSXEEBf7+3K3/tuIzese35+JbJ1K1Rd6/X14utx+MnPc4DvR7g3XnvklAjgS4Nu1RgxSIiIpWHQiAREREROSxk52Vz/ZNdGZ74K1eGdWLoX6YSFR5VqtfWiKjBDZ1vKOcKRUREKrfQDoHUE0hEREQkJGzcsZHzXz6B71jIE2kdefCF6Vi10O1sICIiUh5CNwRSTyARERGRkLBo4yLOfOtUVm37g5ELWnPxu9NAAZCIiMgBC90QSNPBRERERA57k5ZP4vz3zyd823YmfhRDj0mT/R/7RERE5ICF7p9QNB1MRERE5LA2bPYwTn3nVOoTy0+v5NDjxr9DvXrBLktEROSwFdohkEYCiYiIiBx2ClwBD37zINeOvpY+jU9g6tsRNKvfBgYNCnZpIiIih7XQnQ6mnkAiIiIih53M3Eyu+vQqPkj5gBs63cArC5oTkTIYxo6FiIhglyciInJYC90QSCOBRERERA4r6zPWc+7Ic/l59c882+9Z7m52GXZZazjrLDjttGCXJyIictgL7RBIPYFEREREDgvL0pdx8lsns2H7Bj666CPOa3seXHut/6PekCHBLk9ERCQkhHYIpJFAIiIiIoeFByY8QFpmGlOumUKXhl1g+nQYNgzuvRdatQp2eSIiIiEhdBtDqyeQiIiIyGFh0cZFfJjyIYO6DvIBkHNw++1+JbCHHw52eSIiIiEj9EcCOQdmwa5GRERERPbi6R+eJjo8mju63+EPvPsu/PgjvPEGxMUFtzgREZEQErojgaKj/T43N7h1iIiIiMherdi8guHzhnNDpxs4IuYIyMiA+++H446Dq64KdnkiIiIhJbRHAoEfDRQZGdxaRERERKREz059FsO45/h7/IGnnoI1a+DDD6Fa6P69UkREJBhC97+sUVF+r75AIiIiIpXSuox1vD7rda7scCXJtZJh2TJ4/nm4/HLo0SPY5YmIiISc0A2Bio8EEhEREamkzOx0M1tkZkvM7IESzjcxs2/MbJ6ZTTKzpGLnrjKz3wLbYTd3asi0IeQW5PJAr8DHvuceCA+Hp58ObmEiIiIhKvRDoOzs4NYhIiIishdmFga8ApwBtAMGmlm73S57DnjbOXcM8DjwVOC1dYC/At2ArsBfzSy+omo/VGmZafxnxn+4uP3FtKzTEiZPhk8+gQcfhEaNgl2eiIhISArdEEjTwURERKTy6woscc4tc87lACOBc3e7ph3wbeDxxGLnTwPGO+fSnHPpwHjg9AqouUy89NNLZORkMLjXYH9g5EiIjYW77gpuYSIiIiEsdEMgTQcTERGRyq8RsLLY81WBY8XNBc4PPD4PqGlmdUv5WgDM7EYzm2FmM1JTU8uk8EOxLXsb//rpX5zT+hyOrne0PzhuHJx0UtF3OBERESlzCoFEREREKrd7gD5mNhvoA6wG8g/kDZxzQ51zXZxzXRITE8ujxgPy6sxXSc9K58FeD/oDy5b57dRTg1uYiIhIiAv9JeLVE0hEREQqr9VAcrHnSYFjOznn1hAYCWRmscAFzrnNZrYaOHG3104qz2LLQlZeFs9Pe56+zfrSLambPzh+vN/36xe8wkRERKqA0B0JpJ5AIiIiUvlNB1qZWTMziwQuAUYXv8DMEsys8DvbYOCNwOOvgVPNLD7QEPrUwLFK7Y3Zb7AuYx0PnfBQ0cFx4yA5GY48MniFiYiIVAGhGwJpOpiIiIhUcs65PGAQPrxZAIxyzs03s8fN7JzAZScCi8xsMVAPeDLw2jTg7/ggaTrweOBYpZWbn8szPzxDj6QenNj0RH8wPx++/dZPBTMLan0iIiKhTtPBRERERILIOTcGGLPbsUeLPf4Q+HAvr32DopFBld57v7zHii0reKX/K1hh4DNjBmzerKlgIiIiFUAjgURERESk3OUX5PPU90/RoV4H+rfqX3Ri3Dg/Aqhv3+AVJyIiUkWE7kgg9QQSERERqTQ+WfgJizYt4v0L3y8aBQS+KXSnTpCQELziREREqgiNBBIRERGRcvfuL+/SpFYTLmh7QdHBbdtg2jRNBRMREakgoR8CqSeQiIiISNDN3zCfro26ElYtrOjgpEmQl6cQSEREpIKEbggUEeHnl2skkIiIiEhQZeVlsTR9Ke0S2+16Yvx4qF4devYMTmEiIiJVTOiGQGa+L5BCIBEREZGgWrxpMQWuoOQQqE+fol6OIiIiUq5KFQKZ2elmtsjMlpjZAyWcb2xmE81stpnNM7P+Jb1PhYuOVggkIiIiEmQpqSkAu4ZAK1fCwoWaCiYiIlKB9hsCmVkY8ApwBtAOGGhmu/0Zh4eBUc65jsAlwL/LutCDEh2tnkAiIiIiQZaSmkI1q0arOq2KDo4f7/ennhqcokRERKqg0owE6goscc4tc87lACOBc3e7xgFxgce1gDVlV+Ih0EggERERkaBLSU2hZZ2WRIUXm/Y1fjw0aADt2wevMBERkSqmNCFQI2BlseerAseKewy43MxWAWOAP5f0RmZ2o5nNMLMZqampB1HuAVJPIBEREZGgS0lN2XUqWEEBTJjgp4KZBa8wERGRKqasGkMPBN50ziUB/YF3zGyP93bODXXOdXHOdUlMTCyjW++DRgKJiIiIBFVOfg6/pf1Gu4RiIdCcObBxo/oBiYiIVLDShECrgeRiz5MCx4q7DhgF4JybBkQDCWVR4CFRTyARERGRoFqStoS8grxdRwIV9gM65ZTgFCUiIlJFlSYEmg60MrNmZhaJb/w8erdr/gD6AphZW3wIVAHzvfZDI4FEREREgqrElcHGjYNjjoH69YNUlYiISNW03xDIOZcHDAK+BhbgVwGbb2aPm9k5gcvuBm4ws7nACOBq55wrr6JLTT2BRERERIIqJTUFw2id0Nof2LEDvv9eU8FERESCILw0FznnxuAbPhc/9mixxylAz7ItrQxER8P69cGuQkRERKTKSklNoVl8M2pE1PAHvvsOcnIUAomIiARBWTWGrpzUE0hEREQkqPZYGWzcOD9a+4QTgleUiIhIFRXaIZCmg4mIiIgETV5BHos2Ldp1ZbDx46FXL6hRI3iFiYiIVFGhHQKpMbSIiIhI0Pye/js5+TlFI4HWroVfftFUMBERkSAJ/RBI08FEREREgmKPlcEmTPD7U08NUkUiIiJVW+iHQBoJJCIiIhIUhSFQm4Q2/sD48ZCYCB06BLEqERGRqiu0Q6DCnkCVYLV6ERERkaomZWMKyXHJ1Iyq6b+PjR8PfftCtdD+CioiIlJZhfZ/gaOj/ReO3NxgVyIiIiJS5eyyMtivv8K6dZoKJiIiEkShHwKB+gKJiIiIVLACV8CC1AVFIdD48X6vptAiIiJBUzVCIPUFEhEREalQKzavIDMvsygEmj4dmjaFpKSg1iUiIlKVhXYIFBXl9wqBRERERCrUHiuDZWRAfHwQKxIREZHQDoE0EkhEREQkKApDoLYJbf2BzEyoXj2IFYmIiEjVCIHUE0hERESkQqVsTKFBbAPiqwdG/2RlFX03ExERkaCoGiGQRgKJiIiIVKhdVgYDjQQSERGpBEI7BFJPIBEREZEK55xTCCQiIlIJhXYIpJFAIiIiIhVu1dZVZORkKAQSERGpZKpGCKSeQCIiIiIVZo+VwUAhkIiISCVQNUIgjQQSERERqTAlhkBqDC0iIhJ0oR0CqSeQiIiISIVbsHEBCTUSSKiRUHRQI4FERESCLrRDII0EEhEREalwezSFzs+HnByFQCIiIkFWNUIg9QQSERERqRA7VwZL2G0qGCgEEhERCbLQDoE0HUxEREQqOTM73cwWmdkSM3ughPONzWyimc02s3lm1j9wPMLM3jKzX8xsgZkNrvjq97R++3rSs9L37AcECoFERESCLLRDIE0HExERkUrMzMKAV4AzgHbAQDNrt9tlDwOjnHMdgUuAfweO/wmIcs4dDXQGbjKzphVR977sdWUwUGNoERGRIAvtECgy0u8VAomIiEjl1BVY4pxb5pzLAUYC5+52jQPiAo9rAWuKHY8xs3CgOpADbC3/kvdtnyGQRgKJiIgEVWiHQGb+L07qCSQiIiKVUyNgZbHnqwLHinsMuNzMVgFjgD8Hjn8IbAfWAn8Azznn0kq6iZndaGYzzGxGampqGZa/p5TUFGpH16Z+bP2igwqBREREKoXQDoHA9wXSSCARERE5fA0E3nTOJQH9gXfMrBp+FFE+0BBoBtxtZs1LegPn3FDnXBfnXJfExMRyLbZwZTAzKzqoEEhERKRSCP0QKDpaIZCIiIhUVquB5GLPkwLHirsOGAXgnJsGRAMJwKXAWOdcrnNuA/AD0KXcK96PPVYGg6LvYuoJJCIiElRVIwTSdDARERGpnKYDrcysmZlF4hs/j97tmj+AvgBm1hYfAqUGjp8cOB4DdAcWVlDdJUrdnkrqjtRd+wGBRgKJiIhUElUjBNJIIBEREamEnHN5wCDga2ABfhWw+Wb2uJmdE7jsbuAGM5sLjACuds45/KpisWY2Hx8mDXPOzav4T1FkwcYFAAqBREREKqnwYBdQ7tQTSERERCox59wYfMPn4sceLfY4BehZwusy8MvEVxolrgwGCoFEREQqCY0EEhEREZEykZKaQmxkLElxSbueKPwuphBIREQkqKpGCKSeQCIiIiLlLiU1hbYJbXddGQyKRgKpMbSIiEhQVY0QSCOBRERERMrdgo0L9pwKBpoOJiIiUkmEfgiknkAiIiIi5W5z1mbWbFujEEhERKQSC/0QSCOBRERERMrdgtS9rAwGPgQKD/ebiIiIBE3VCIHUE0hERESkXO11ZTDwf5BTPyAREZGgqxohkEYCiYiIiJSrlNQUqodXp0mtJnuezMzUVDAREZFKoFQhkJmdbmaLzGyJmT1QwvkXzGxOYFtsZpvLvtSDpJ5AIiIiIuUuZWMKbRLaEFYtbM+TCoFEREQqhf1OzDazMOAVoB+wCphuZqOdcymF1zjn7ix2/Z+BjuVQ68HRSCARERGRcpeSmsIJjU8o+aRCIBERkUqhNN35ugJLnHPLAMxsJHAukLKX6wcCfy2b8spAYU8g58As2NWIiIiIhKTnT32eejH1Sj6ZlaUQSEREpBIoTQjUCFhZ7PkqoFtJF5pZE6AZ8O1ezt8I3AjQuHHjAyr0oEVFQUEB5OVBRETF3FNERESkirmw3YV7P5mZqcbQIiIilUBZN4a+BPjQOZdf0knn3FDnXBfnXJfExMQyvvVeFH7h0JQwERERkeDQdDAREZFKoTQh0GogudjzpMCxklwCjDjUosqUQiARERGR4FIIJCIiUimUJgSaDrQys2ZmFokPekbvfpGZtQHigWllW+IhKgyBsrODW4eIiIhIVaUQSEREpFLYbwjknMsDBgFfAwuAUc65+Wb2uJmdU+zSS4CRzjlXPqUepKgov9dIIBEREZHgyMpSTyAREZFKoDSNoXHOjQHG7Hbs0d2eP1Z2ZZUhTQcTERERCS6NBBIREakUyroxdOWj6WAiIiIiwaUQSEREpFKoOiGQRgKJiIiIBIdCIBERkUoh9EMg9QQSERERCR7n/IhshUAiIiJBF/ohkEYCiYiIiARP4XcwNYYWEREJuqoTAqknkIiIiEjFy8z0e40EEhERCbqqEwJpJJCIiIhIxVMIJCIiUmmEfgiknkAi/8/enYdHVZ/9H//c2QmEPRGSsInIJioYcMGKFKy4Qd0qWBW6qF2sS/VpfX6PVWvr9dSl1WqprdbWpa3UlQctliqidYfIKpsiICSARCCBhMn+/f1xZpJJCJCEmcxJ5v26rnOdM2fOnHznaPTw4f7eBwCA2CEEAgDANzp+CEQlEAAAQOzQEwgAAN+InxCInkAAAABtj0ogAAB8I35CICqBAAAA2h4hEAAAvtHxQ6CUFG9NCAQAAND2CIEAAPCNjh8CmXnNoQmBAAAA2l7oHowQCACAmOv4IZDkTQmjJxAAAEDbC1UC0RgaAICYi58QiEogAACAtsd0MAAAfCM+QiCmgwEAAMQGIRAAAL4RHyEQlUAAAACxQQgEAIBvxE8IRE8gAACAthf6izh6AgEAEHPxEQIxHQwAACA2AgEpIUFKTo71SAAAiHvxEQIxHQwAACA2AgFvKphZrEcCAEDci58QiOlgAAAAbS8UAgEAgJiLnxCISiAAAOBDZjbFzNab2QYzu7WJ9/ub2SIzW2ZmK83s3LD3Z5FOcAAAIABJREFUjjez981stZmtMjP/Nd4pLycEAgDAJ5JiPYA2QU8gAADgQ2aWKGm2pLMkFUhaYmbznHNrwg67TdKzzrlHzGyEpPmSBppZkqS/SrrSObfCzHpJqmrjr3B4gQBNoQEA8AkqgQAAAGJnnKQNzrmNzrlKSXMkTWt0jJPUNbjdTdK24PbXJK10zq2QJOfcLudcTRuMuWWYDgYAgG/ETwhETyAAAOA/OZK2hr0uCO4Ld6ekK8ysQF4V0I+C+4+V5MxsgZktNbOfHOyHmNk1ZpZvZvlFRUWRG31zEAIBAOAb8RMCUQkEAADapxmSnnDO5Uo6V9LTZpYgb1r/6ZK+GVxfaGaTmjqBc+5R51yecy4vMzOzrcbtIQQCAMA34iMEoicQAADwp0JJ/cJe5wb3hfuOpGclyTn3vqQ0Sb3lVQ39xzn3pXNuv7wqoTFRH3FLlZfTEwgAAJ+IjxCISiAAAOBPSyQNMbNBZpYiabqkeY2O2SJpkiSZ2XB5IVCRpAWSRplZerBJ9ARJa+Q3VAIBAOAb8fF0sLQ0qbZWqq6WkuLjKwMAAP9zzlWb2XXyAp1ESX92zq02s7sk5Tvn5km6WdJjZnaTvCbRs5xzTtIeM/uNvCDJSZrvnPtnbL7JIRACAQDgG/GRiIRKkMvLpS5dYjsWAACAMM65+fKmcoXvuz1se42k8Qf57F/lPSbevwiBAADwjfiYDpaa6q2ZEgYAANC2yssJgQAA8In4CIHCK4EAAADQdgIBGkMDAOAT8RUCVVTEdhwAAADxxDmmgwEA4CPxFQJRCQQAANB2Qn8BRwgEAIAvxEcIRE8gAACAthcIeGtCIAAAfCE+QiAqgQAAANpe6N6LEAgAAF9oVghkZlPMbL2ZbTCzWw9yzDfMbI2ZrTazv0d2mEeInkAAAABtL1QJRGNoAAB8IelwB5hZoqTZks6SVCBpiZnNc86tCTtmiKT/ljTeObfHzLKiNeBWYToYAABA22M6GAAAvtKcSqBxkjY45zY65yolzZE0rdExV0ua7ZzbI0nOuZ2RHeYRYjoYAABA2yMEAgDAV5oTAuVI2hr2uiC4L9yxko41s3fN7AMzm9LUiczsGjPLN7P8oqKi1o24NZgOBgAA0PboCQQAgK9EqjF0kqQhks6UNEPSY2bWvfFBzrlHnXN5zrm8zMzMCP3oZqASCAAAoO3REwgAAF9pTghUKKlf2Ovc4L5wBZLmOeeqnHObJH0iLxTyB3oCAQAAtD2mgwEA4CvNCYGWSBpiZoPMLEXSdEnzGh0zV14VkMyst7zpYRsjOM4jQyUQAABA2yMEAgDAVw4bAjnnqiVdJ2mBpLWSnnXOrTazu8xsavCwBZJ2mdkaSYsk/Zdzble0Bt1i9AQCAABoe/QEAgDAVw77iHhJcs7NlzS/0b7bw7adpB8HF/9hOhgAAEDboxIIAABfiVRjaH8zk1JSCIEAAADaEo2hAQDwlfgIgSTv5oMQCAAAoO1QCQQAgK/EVwhETyAAAIC2EwjUV2QDAICYi68QiEogAACAtlNe7lUBmcV6JAAAQPEUAqWmEgIBAAC0pUCAfkAAAPhI/IRAVAIBAAC0rUCAfkAAAPhIfIVA9AQCAABoO4RAAAD4SnyFQFQCAQAAtJ1QTyAAAOAL8RMC0RMIAACgbVEJBACAr8RPCEQlEAAAQNuiMTQAAL4SXyEQPYEAAADaDpVAAAD4SvyEQEwHAwAAaFuEQAAA+Er8hEBMBwMAAGhbNIYGAMBXCIEAAAAQHfQEAgDAV+IrBKInEAAAQNthOhgAAL4SPyEQPYEAAADaFiEQAAC+Ej8hUFqaVFMjVVfHeiQAAAAdn3P0BAIAwGfiKwSSmBIGAADQFqqqpNpaQiAAAHwk/kIgpoQBAABEXyDgrWkMDQCAb8RPCJSa6q0JgQAAAKIvFAJRCQQAgG/ETwhEJRAAAPAhM5tiZuvNbIOZ3drE+/3NbJGZLTOzlWZ2bhPvl5rZLW036mYgBAIAwHfiLwSiJxAAAPAJM0uUNFvSOZJGSJphZiMaHXabpGedc6MlTZf0+0bv/0bSq9Eea4uF/uKNEAgAAN+IvxCISiAAAOAf4yRtcM5tdM5VSpojaVqjY5ykrsHtbpK2hd4ws69L2iRpdRuMtWXoCQQAgO/ETwhETyAAAOA/OZK2hr0uCO4Ld6ekK8ysQNJ8ST+SJDPrIumnkn5+uB9iZteYWb6Z5RcVFUVi3IfHdDAAAHwnfkIgKoEAAED7NEPSE865XEnnSnrazBLkhUMPOOdKD3cC59yjzrk851xeZmZmdEcbQggEAIDvJMV6AG2GnkAAAMB/CiX1C3udG9wX7juSpkiSc+59M0uT1FvSyZIuMbN7JXWXVGtm5c6530V/2M1ATyAAAHwn/kIgKoEAAIB/LJE0xMwGyQt/pku6vNExWyRNkvSEmQ2XlCapyDn3ldABZnanpFLfBEASlUAAAPhQ/EwHoycQAADwGedctaTrJC2QtFbeU8BWm9ldZjY1eNjNkq42sxWSnpE0yznnYjPiFqAxNAAAvkMlEAAAQAw55+bLa/gcvu/2sO01ksYf5hx3RmVwR4JKIAAAfCd+KoHoCQQAANB2CIEAAPCd+AuBqAQCAACIPhpDAwDgO/ETAtETCAAAoO2EKoFC92AAACDmCIEAAAAQeYGAV4ltFuuRAACAoPgJgRISpJQUegIBAAC0hUCAqWAAAPhM/IRAklcNRCUQAABA9JWXEwIBAOAz8RUCpaURAgEAALQFKoEAAPCdZoVAZjbFzNab2QYzu7WJ92eZWZGZLQ8u3438UCMgLY3pYAAAAG0h1BMIAAD4RtLhDjCzREmzJZ0lqUDSEjOb55xb0+jQfzjnrovCGCOHSiAAAIC2QSUQAAC+05xKoHGSNjjnNjrnKiXNkTQtusOKEnoCAQAAtA1CIAAAfKc5IVCOpK1hrwuC+xq72MxWmtnzZtavqROZ2TVmlm9m+UVFRa0Y7hGiEggAAKBt0BgaAADfiVRj6JclDXTOHS/pNUlPNnWQc+5R51yecy4vMzMzQj+6BegJBAAA0DboCQQAgO80JwQqlBRe2ZMb3FfHObfLORdKV/4k6aTIDC/CqAQCAABoG0wHAwDAd5oTAi2RNMTMBplZiqTpkuaFH2BmfcNeTpW0NnJDjCB6AgEAALQNQiAAAHznsE8Hc85Vm9l1khZISpT0Z+fcajO7S1K+c26epOvNbKqkakm7Jc2K4phbj0ogAACAtkFPIAAAfOewIZAkOefmS5rfaN/tYdv/Lem/Izu0KKAnEAAAQNugEggAAN+JVGPo9oFKIAAAgLZBY2gAAHwnvkIgegIBAABEX1WVVFNDJRAAAD4TXyEQlUAAAADRFwh4a0IgAAB8Jf5CIHoCAQAARFfoL90IgQAA8JX4C4Gqq70FAAAA0UElEAAAvhRfIVBqqremGggAACB6QiEQjaEBAPCV+AqBQjci9AUCAACIHiqBAADwpfgMgagEAgAAiB56AgEA4EvxFQKFpoNRCQQAABA9VAIBAOBL8RUCMR0MAAAg+ugJBACAL8VnCMR0MAAAgOihEggAAF+KzxCISiAAAIDoIQQCAMCX4isEoicQAABA9NEYGgAAX4qvEIhKIAAAgOijEggAAF/qcCHQXW/dpQueuaDpN+kJBAAAEH00hgYAwJc6XAhUXl2uf234l8qrm6j2oRIIAAAg+giBAADwpQ4XAo3NHqvq2mot37H8wDfpCQQAABB95eXefVdCh7vVBACgXetw/2cemzNWkrSkcMmBb1IJBAAAEH2BAP2AAADwoQ4XAuVk5Khvl75avG3xgW/SEwgAACD6AgGmggEA4EMdLgQyM43NGUslEAAAaBfMbIqZrTezDWZ2axPv9zezRWa2zMxWmtm5wf1nmdlHZrYquP5q24/+IKgEAgDAlzpcCCR5fYHW71qvkvKShm/QEwgAAPiImSVKmi3pHEkjJM0wsxGNDrtN0rPOudGSpkv6fXD/l5IucM6NkjRT0tNtM+pmIAQCAMCXOmwIJEkfbf+o4RsJCVJyMiEQAADwi3GSNjjnNjrnKiXNkTSt0TFOUtfgdjdJ2yTJObfMObctuH+1pE5mltoGYz688nJCIAAAfKhjhkDB5tCLCw/SF4ieQAAAwB9yJG0Ne10Q3BfuTklXmFmBpPmSftTEeS6WtNQ51+RNjpldY2b5ZpZfVFR05KM+HCqBAADwpQ4ZAvXs1FODewzWkm0H6QtEJRAAAGg/Zkh6wjmXK+lcSU+bWd09nJmNlHSPpGsPdgLn3KPOuTznXF5mZmbUB0xjaAAA/KlDhkCSDt4cOjWVEAgAAPhFoaR+Ya9zg/vCfUfSs5LknHtfUpqk3pJkZrmSXpJ0lXPus6iPtrmoBAIAwJc6bgiUPVZb927VjtIdDd+gEggAAPjHEklDzGyQmaXIa/w8r9ExWyRNkiQzGy4vBCoys+6S/inpVufcu2045sOjJxAAAL7UYUOgcTnjJOnAaiB6AgEAAJ9wzlVLuk7SAklr5T0FbLWZ3WVmU4OH3SzpajNbIekZSbOccy74uWMk3W5my4NLVgy+xoGoBAIAwJeSYj2AaBndZ7QSLEFLti3RBUMvqH+D6WAAAMBHnHPz5TV8Dt93e9j2Gknjm/jcLyX9MuoDbA16AgEA4EsdthKoc0pnjcwceWBzaKaDAQAARBeVQAAA+FKHDYEkry/Q4sLF8iqmg5gOBgAAEF2EQAAA+FKHDoHG5YzT7sBubSreVL+TSiAAAIDoqa72FkIgAAB8p0OHQGNzxkpq1ByankAAAADRE7rPIgQCAMB3OnQINCprlFITUxv2BaISCAAAIHoCAW9NY2gAAHynQ4dAyYnJOrHPiVpcuLh+Jz2BAAAAoicUAlEJBACA73ToEEjy+gIt3b5UNbU13g4qgQAAAKKH6WAAAPhWhw+BxmaPVVlVmdZ+udbbQU8gAACA6KESCAAA32pWCGRmU8xsvZltMLNbD3HcxWbmzCwvckM8MqHm0HVTwqgEAgAAiB56AgEA4FuHDYHMLFHSbEnnSBohaYaZjWjiuAxJN0j6MNKDPBLH9jpWXVO71j8hLC3Ne2xpTU1sBwYAANARUQkEAIBvNacSaJykDc65jc65SklzJE1r4rhfSLpHkq/KbBIsQXnZefVPCAv9rRTNoQEAACKPnkAAAPhWc0KgHElbw14XBPfVMbMxkvo55/55qBOZ2TVmlm9m+UVFRS0ebGuNzR6rlV+sVHl1udcTSGJKGAAAQDRQCQQAgG8dcWNoM0uQ9BtJNx/uWOfco865POdcXmZm5pH+6GYbmz1WVbVVWrFjRX0lUOgGBQAAAJFDCAQAgG81JwQqlNQv7HVucF9IhqTjJL1pZpslnSJpnh+bQy/ZtkQaMsTb+f77MRwRAABAB0VjaAAAfKs5IdASSUPMbJCZpUiaLmle6E3nXIlzrrdzbqBzbqCkDyRNdc7lR2XErdCvaz8d1fkoLwSaMEHq21f6619jPSwAAICOh0ogAAB867AhkHOuWtJ1khZIWivpWefcajO7y8ymRnuAkWBmGpsz1ntCWGKiNH26NH++tHt3rIcGAADQsdAYGgAA32pWTyDn3Hzn3LHOucHOubuD+253zs1r4tgz/VQFFDI2e6zWfblOeyv2St/8plRVJT3/fKyHBQAA0LEwHQwAAN864sbQ7cXY7LFycvpo20fSmDHS0KHS3/4W62EBAAB0LIGAlJzsVV8DAABfiZ8QKLw5tJl0xRXSf/4jbdkS45EBAAB0IIEAU8EAAPCpuAmBeqf31qDug7wQSJIuv9xbP/NM7AYFAADQ0ZSXEwIBAOBTcRMCSV410OLCxd6Lo4+WTj2VKWEAAACRRCUQAAC+FV8hUPZYbSnZop1lO70d3/ymtGqVtHJlbAcGAADQURACAQDgW3EVAo3LGSdJ3qPiJekb3/CaFlINBAAAEBmBAE8GAwDAp+IqBBrTd4wSLKF+SlhmpnT22V5foNra2A4OAACgI6ASCAAA34qrEKhLShcN7z28vjm05E0J27pVevvt2A0MAACgo6AxNAAAvhVXIZDkNYdesm2JnHPejmnTpM6dmRIGAAAQCVQCAQDgW3EXAp2Sc4q+3P+lFm1e5O3o3Fn6+tel556TKipiOzgAAID2jhAIAADfirsQ6Irjr9DRPY7W1S9frbLKsuDOK6TiYunVV2M7OAAAgPaOxtAAAPhW3IVAnVM66/Gpj2vjno36nzf+x9s5ebKUlSX99a+xHRwAAEB7R08gAAB8K+5CIEk6c+CZ+kHeD/TQhw/p3S3vSklJ0mWXSa+8IpWUxHp4AAAA7RfTwQAA8K24DIEk6Z6z7lH/bv317XnfVqAq4D0lrKJCeuGFWA8NAACg/SIEAgDAt+I2BOqS0kV/mvonfbLrE93x5h3SuHHS4ME8JQwAAKC1amqkykp6AgEA4FNxGwJJ0uSjJ+vqMVfr1+//Wh8WLvaqgRYtkgoLYz00AACA9qe83FtTCQQAgC/FdQgkSfeddZ+yM7L17XnfVsX0SyXnpDlzYj0sAACA9ocQCAAAX4v7EKhbWjc9dsFjWlO0Rndte0bKy2NKGAAAQGsEAt6aEAgAAF+K+xBIkqYcM0WzTpyle969Rx9NP0NatkxatSrWwwIAAGhfCIEAAPA1QqCg33ztN8rqnKVvpbyqyu4Z0lVXSWVlsR4WAADo4MxsipmtN7MNZnZrE+/3N7NFZrbMzFaa2blh7/138HPrzezsth15E0IhEI2hAQDwJUKgoB6deuiP5/9Rq3av1f/eN1VasUKaNcvrEQQAABAFZpYoabakcySNkDTDzEY0Ouw2Sc8650ZLmi7p98HPjgi+HilpiqTfB88XO/QEAgDA1wiBwlww9AJ9c9Q39cvt/9Ab/3uN9Pzz0i9+EethAQCAjmucpA3OuY3OuUpJcyRNa3SMk9Q1uN1N0rbg9jRJc5xzFc65TZI2BM8XO0wHAwDA1wiBGvntlN9qSM8h+lrFn/TAj/Lk7rhDevHFWA8LAAB0TDmStoa9LgjuC3enpCvMrEDSfEk/asFnJUlmdo2Z5ZtZflFRUSTG3TRCIAAAfI0QqJFe6b304Xc/1LRh0/TjXvmacU1PlX37Cm96GAAAQNubIekJ51yupHMlPW1mLbqHc8496pzLc87lZWZmRmWQkugJBACAzxECNSEjNUPPX/q8fjXpV3oup1inzKzSp1eeK+3cGeuhAQCAjqVQUr+w17nBfeG+I+lZSXLOvS8pTVLvZn62bVEJBACArxECHYSZ6aen/1QLrlig7Ud1Vt552/Ty9yZKlZWxHhoAAOg4lkgaYmaDzCxFXqPneY2O2SJpkiSZ2XB5IVBR8LjpZpZqZoMkDZG0uM1G3hQaQwMA4GuEQIcx+ejJ+ugHKzSk2yBNPWGNbv/vU1RTUx3rYQEAgA7AOVct6TpJCyStlfcUsNVmdpeZTQ0edrOkq81shaRnJM1yntXyKoTWSPqXpB8652ra/luEoRIIAABfS4r1ANqDAd0H6J1b1ugHd52sX3RdpiX3nKCnb3xLvdN7x3poAACgnXPOzZfX8Dl83+1h22skjT/IZ++WdHdUB9gShEAAAPgalUDNlJaUpsfvWKo/fD5KC8vXaMCvc3X9q9dr055NLT5XRXWFisqi+GQOAACAWKAxNAAAvkYI1AKWmKhrf/uulr85TJcsrdAjH87WMQ8fo+nPT1f+tvxDfjZQFdDcdXN15UtXKuv+LB11/1H68YIfq6yyrI1GDwAAEGXl5VJSkrcAAADfIQRqqYwMjViwVE/mXqdND9TqlnU99er6f2rsY2N15hNn6p+f/FO1rlaSVFpZqmdXP6vLnr9Mmfdl6sJ/XKj5n87XxcMv1ndGf0cPfPCAjv/D8Xpj0xsx/lIAAAAREAgwFQwAAB/jr2lao1Mn6eGHlXvuubrnW9/S/7xSqcf+Z6oe3LNU5z9zvkZkjtCQnkO04LMFKq8uV2Z6pq44/gpdPPxinTnwTCUnJkuSrjj+Cn335e9q0lOTdPWYq3XfWfepW1q3GH85AACAViIEAgDA16gEOhLnnCOtWqWuE6fo5lvnaeO/hurpMx9SamKqlm5fqu+O/q7enPmmtt+8XX84/w86a/BZdQGQJE0YOEErvrdC/3Xaf+nxZY9rxO9H6OX1L8fwCwEAAByBQIB+QAAA+Bgh0JHKzJTmzpX+8Aclv/2errjoTi3NvE1bbtqih899WBMGTlBiQuJBP56enK57z7pXH3znA/Xq1EtT50zV5S9cTuNoAADQ/lAJBACArxECRYKZdO210rJl0tFHSxdfLH3zm9KGDc0+xdicscq/Jl8/P/Pnen7N8xo+e7iuf/V6Ldy4UFU1VVEcPAAAQISUlxMCAQDgY80KgcxsipmtN7MNZnZrE+9/z8xWmdlyM3vHzEZEfqjtwNCh0nvvST/7mfTii9KwYdLMmdKnnzbr4ymJKbp9wu1adu0ynd7/dD229DFNfnqyMu/L1OUvXK45H89RSXlJlL8EAABAK1EJBACArx02BDKzREmzJZ0jaYSkGU2EPH93zo1yzp0o6V5Jv4n4SNuL5GTprrukTZukG26QnnvOC4OuvFJav75ZpxiZNVJzp8/Vrp/s0tzL5uqi4Rfp9Y2va8YLM9T7vt466+mz9LvFv9PGPRuj/GUAAABagBAIAABfa04l0DhJG5xzG51zlZLmSJoWfoBzbm/Yy86SXOSG2E716SP9+tdeGHTTTdILL0gjRnjTxNata9Yp0pPTNW3YNP152p+1/ebteudb7+imU27S1pKt+tGrP9LghwZryMNDdN386/Ty+pdVWlka5S8FAABwCDSGBgDA18y5Q+c1ZnaJpCnOue8GX18p6WTn3HWNjvuhpB9LSpH0VefcAXOgzOwaSddIUv/+/U/6/PPPI/Il2oWdO6X775dmz/ZukC65RPrWt6TJk73qoRb6ZNcn+teGf2nBZwv05uY3tb9qv5ITkjW+/3idPfhsnT34bB3T8xiVVJSopLxExeXFdduhdXVttbIzspXbNbduyUjNiMKXBwDEIzP7yDmXF+txoKG8vDyXn58fnZMfd5xXAf3889E5PwAAOKxD3YNFLAQKO/5ySWc752Ye6rxRvQHxs6Iir0Loj3+Uiou9p4t94xvS5ZdLp57qNZluoYrqCr2z5R0t+GyBFny2QCu/WNnq4XVN7VoXCPXr2k9H9zhax/Q8Rsf0PEaDewxWt7RurT53pG3as0nPr3lep/Y7VeP7jZe14todzsY9G1W4t1Dj+49XgtFHHQBaghDIn6J6DzZ4sHTaadLTT0fn/AAA4LAOdQ+W1IzPF0rqF/Y6N7jvYOZIeqT5w4szmZnSr34l/fzn0r/+Jf3979Ljj3sVQgMHSjNmeIHQccc1+5SpSamadPQkTTp6ku49615t37ddr218TV+UfqFuad3ULbWbuqd1P2A7wRK0bd82FewtaHJZsWOFvij7osHP6p3euy4QGtxjsHqn91aXlC7KSM1Ql5Qu3nZK/XZaUprMTAmW0ORishaFN845vbHpDT28+GHNWz9PLjjz8NTcU/WT8T/R1KFTjyis2Vm2U29sekOvb3xdCzct1ObizZKk0/qdptnnztaJfU5s9bkBwE9qXa3mfzpfr3zyih4575GoBOmIQ/QEAgDA15pTCZQk6RNJk+SFP0skXe6cWx12zJDQ9C8zu0DSHYf7m7+4rQRqyr590ty5XiD02mtSTY00apR00UXStGnSiSe2qkIoEkorS7Vxz0Zt2L1Bn+3+zFvv8dZbSrbUhTCt1S21m07vf7q+0v8rOmPAGTop+ySlJKYccFxZZZn+uvKvenjxw1pdtFqZ6Zm69qRrddUJV+nfn/1bv37/19pUvElDew3VLafdoiuPv1KpSamH/fm79u/S+wXva+HGhVq4aaFW7VxVN66JgyZq0qBJSkpI0s8W/Uy7A7v1w7E/1F0T71L3tO5H9L2dc9pZtlMf7/xYq4tWq2tqV1028jJ1SubGGR3ftn3b9P7W9zVh4AT1Tu8d6+HEnUBVQE+vfFoPfPCA1n25Trldc/XBdz5QTteciP8sKoH8Kar3YD16eA/DeOih6JwfAAAc1hFNBwue4FxJD0pKlPRn59zdZnaXpHzn3Dwz+62kyZKqJO2RdF14SNQUQqCD2LnTe6LYnDnSu+9Kzkn9+klTp3qB0IQJUsqBIUksVNZUam/FXpVWlmpfxT6VVpZ625X76vZV1lSq1tXWLU6uwevCvYV6e8vbWvvlWklSp6ROOiX3FJ0x4AydMeAMZWdk6/Glj+tPy/6k4vJije4zWjecfIMuO+4ypSXVN56srq3WC2te0D3v3qNlO5apT5c+uvHkG3Vt3rXqntZdZZVlWl20Wh/v/LhuWbVzlXaU7pAkpSam6vT+p2vSoEmafPRkjek7RokJiXXn3xPYo9veuE1/+OgP6p3eW/eddZ+uPP7KZv3NeUl5iVbtXFX3c0Pj+HL/lw2O69Wpl6496Vr9cNwPlZ2RHYl/RFFVUV1RFxB+uvtT7SjdoVFZozS+/3gN6j6IqgI0sL9qv+aum6unVjyl1za+plpXq+SEZJ137HmaecJMnTvk3CYDYETOzrKd+v2S3+v3S36vov1FGtN3jG4+9WZdOuJSJSe2vDddcxAC+VNU78HS0ryno95zT3TODwAADuuIQ6BoIARqhqIi6ZVXpP/7P+nf//ZKrLt2lc45xwuEJk/2ppd1ADvLduqdLe/oP5//R29veVvLdyxXrauVJCVaoi4ecbGuH3e9Tut32iHDBeecFm5aqHvfvVevbXxNGSkZyuycqY17NtYd0ympk0ZkjtBxWcdpVNYoje47Wqf1O61BqHQwS7cv1Q/++QN9WPihTu9/umafO1vHH3V83fvl1eVasWOFFhcu1uJti7W4cLE+2fVJ3ftdUrrouKwQqUSwAAAe7klEQVTjdFzmcd466ziNzBqp9V+u1wMfPKB56+cpKSFJlx13mW465SaN6TumNZezVWpqa+pCvH0V+xqsSytLVVRWpE93f1oX+mwp2VL3z0jy/jnVuBpJUp8ufXRav9M0vt94ndbvNI3pO6bVf8DfXLxZL619SS+ue1Hb9m3ThAETNPnoyZo0aJKO6nJUs86xo3SHPij4QMt3LFfv9N4a1nuYhvUeppyMHMKqKKp1tfrP5//RUyue0nNrnlNpZakGdBugK4+/Ul8d9FXN/3S+nl75tL4o+0K903trxnEzNOvEWRrdZzT/XCJo3Zfr9MD7D+jJFU+qoqZC5x97vm4+9WZNGDAh6teZEMifonYP5pyUkCDdcYd0552RPz8AAGgWQqCOIBCQXn/dC4ReftmrGJK8J3Ccfrr0la9460GDYjZ1LJJKykv03tb39Nmez/T1YV9XbtfcFp9j2fZl+u2Hv9X+qv0alTXKC32OGqVB3Qc1qPJpqVpXq78s+4t++vpPVVxerO+O+a4SLEGLCxdr5RcrVVVbJckLQsbljNPY7LEa03eMjss6Tv269jvkH7o27N6ghz98WH9e/meVVpbqjAFn6KZTbtIFx15w2DGXV5dr+77t2rZvm7aXeuvQ9vZ921VaWary6nJV1FSoorqiwbq8ulyVNZWH/e7dUrtpSK8hGtLTW47peUzd6+5p3fXxzo/13tb39F7Be3p3y7vaVLxJkpSWlKa87DyN7jNaIzNHamTWSI3MHKkenXo0+XPWFq3Vi2tf1IvrXtTS7UslSSccdYIG9Riktza/pT3leyRJo7JGafLRkzX56Mk6Y8AZ6pLSReXV5Vq6fak+LPhQHxR+oA8KPtCWki1N/pzOyZ01tPdQDe01VMN6D9PQXkPVv1t/ZXXOUmbnTGWkZET0D8nVtdXaHdgtyQvNkhKSlJSQpMQEbzvREttt+FHralVUVqTCfYUq3FuoDwo+0F9X/VVbSrYoIyVDl464VFedcJW+MuArDfp2VddW69+f/VtPrnhS/7fu/1RRU6Hjso7TzBNm6muDv6YRmSOUlNCc9nUts69inz7Z9YmO7nH0Qf89bGvOOe2t2KudZTvrluLyYp3Y50Sd0OeEFvU7K9xbqLnr5urFdS/qjU1vKDUxVVedcJVuOuUmDc8cHsVv0RAhkD9F7R4sEJDS073ehz/9aeTPDwAAmoUQqKOpqZGWLJHeekt6+21v2lhxsfdedrYXBp1+ujR+vNdbqBWPoMfh7Q7s9qaI5f9BGakZysvO07jscV7wkzP2iKpMisuL9fjSx/XQ4oe0pWSLenbqqbSkNDnn6qbVOefqptdV11Zrb8XeA86TnJCsvhl91bdLX3VN7arUpFSlJqbWr8O205PTlZGaUdfYu/F2z0491atTrxZ9p+37tnuhUDAY+njnxyqtLK17v2+XvnWB0MjMkdpcvFkvrntR675cJ8lr+H3R8It04bALNbjnYElexdLyHcv1+sbX9fqm1/X252+roqZCSQlJGtJziDbs3lAXxA3oNkAn556sU3JO0cm5J2t0n9EqLi/Wui/Xaf2u9Q3Wnxd/fkCPq9TEVGV2zlRmemZdMNQ9tbuSE5OVkpii5ITgOjG5btvJ6cv9X6qorEg79+/01mU7VbS/qC4AOpRQ0/RwpobXPCkhSWlJaUpLSlNqUmrddmhJT05X97Tu6pHWQz079VSPtB7q0alH3bpraldVVFfUTeEsqyqr2y6tLFVZZZlqXE1d4/bG6wRL0N6KvSrcV6ht+7apcG+htpduV3VtdYPv8bXBX9NVx1+lacOmKT05/bDffU9gj/6x+h96csWT+qDgA0le5d7ovqOV1zdPedl5GpszVsf2OrbFDeDLKsv07tZ3tWjTIi3avEj52/LrKtdyMnLqKvNCy/Dew9U5pXOLfkZI6PexpLxExeXFKi4v1p7yPd46sKd+u3yPdgd21/07srNspypqKpo8Z89OPTVxoNenbNLRkzSk55ADfhc/2fVJXdXc4sLFkqRhvYdp+sjp+v7Y7yurc1arvs+RIATyp6jdg+3eLfXqJf32t9L110f+/AAAoFkIgTq62lppzRovEHrnHW/ZEqx8SE31GkuPGyeNHestxx7rlWsjIvZW7FWXlC5ReYR8dW21Xlr7khZ8tkCSGvwhPHw70RKV1TlL2RnZys7IVt+MvsrOyFbPTj199Wj7WlerrSVb63ojrS5ardU7V2tN0RoFqgNKtESdOfBMXTT8In192Neb1RspUBXQe1vf0+sbX9eqnas0KmuUTs49WSfnnKy+GX2bPbZAVUCf7v5UhXsLVbQ/GNyUFdVv7y9SUVmRisuLVVVbpaqaKlXWVDbZHN1k6pXeywuOggFSaLtXei8lWIKqa6tVU1uj6tpqb9vVb4f/d7mp81fVVNVVcDW1lFWV1QUOxeXFLW7gnpqYqsSExLqgsfG61tWqS0oX5WTkKKdrjnIycpSdkV33OjsjW4N7DFav9F4t+rnhNu7ZqPe3vq/8bfnK356vpduXan/VfklSRkqGxvQdo8E9BqtbWvCJh6ndDngC4p7AHr25+U0t2rxIiwsXq6q2SkkJSRqXM04TB07UiX1O1KY9m/Rxkdeva03RGpVXl9f9MxzYfaC6pXVTUkKSkhOSvXVict3r5MRkBaoCKqkoUUl5Sd26rKrskN8twRLUPa17XVB3VJejvH9H0rPq/l0JLZ1TOmtx4WIt3LRQCzcu1Na9WyVJuV1z9dVBX9WZA87Uxj0b9dK6l7S6yGvFl5edpwuHXagLh13YplU/TSEE8qeo3YMVFkq5udIf/yhdc03kzw8AAJqFECgebdkiffihtHixVzWUny+VBf9g0rWrlJcnnXSSNGKENHy4N62sW7fYjhlxq9bV6vPiz9UtrZt6duoZ6+G0SE1tjapqvUCoqqZKTk490noc0ZTDSKp1tSopL9Ge8j11VSh7K/YqLSlNXVK6HLCkJ6dHZfrVkaqurda6L9d5odC2fC3ZtkQFewsOG7okWqLysvM0ceBETRw0UeP7jT9ohU9NbY0+2/NZXRP3tV+uVVllmaprq1VVW+Wta6rqwrqq2iqlJqY2DJ/CwqjQOlSF1T2tu3p06tHq0Ng5pw27N+iNTW9o4aaFemPTG9oV2KUES9AZA87QRcO88LRft34tPne0EAL5U9TuwT77TDrmGOmpp7wnhAEAgJggBII3hWzdOi8QCgVDK1dKlWF9YLKzvTBo+PCGS58+HaLPEICOqaqmypt+1agiJy0pTaf2O1VdU7vGeohRUetqtbZorY7qcpR6p/eO9XCaRAjkT1G7B/v4Y28a+nPPSZdcEvnzAwCAZjnUPZj//roX0ZGYKI0c6S2zZnn7qqulTZuktWsbLk89Je3bV//Zrl29cKjxMniwbx5XDyB+JScmq1d6ryOaftYeJViCRmaNjPUwgHqBgLfu1Cm24wAA+FJVVZUKCgpUXl4e66F0GGlpacrNzVVyC/oAEwLFs6QkacgQb5k6tX6/c968/rVrpfXrvQqideukhQu9gCgkMVHKyZH69fN6APTr13DJzZWysug/BABAPAiFQGlpsR0HAMCXCgoKlJGRoYEDB7bbJ+L6iXNOu3btUkFBgQYNGtTszxEC4UBmXoCTmyuddVbD9/btaxgMbdkibd3q9RyaO1eqaPRkm7Q0r2IotBxzTP26f3+eXAYAQEdBJRAA4BDKy8sJgCLIzNSrVy8VFRW16HOEQGiZjAyvqXReE9MLnZO+/NILhULL5s3Shg1es8jXXqu/QZS8SqLcXOmoo7wlK6vp7T59pB496EsEAICfhcr7CYEAAAdBABRZrbmehECIHDMpM9Nbxow58H3npO3b60Ohzz6TPv9c2rnTqyjKz/e2a2oO/GxKihcG9e3rLaHtPn3qp6Ll5krduxMWAQAQC1QCAQDge4RAaDtm3hPIsrOlM85o+pjaWmnPHumLL+qXHTu88Gj7dm97wwbp7belXbsO/HznzvVT2ULBUJ8+Uq9eUu/e3hLa5iYVAIDIIQQCAPjYrl27NGnSJEnSjh07lJiYqMzMTEnS4sWLlXKIhx7l5+frqaee0kMPPdQmY40mQiD4S0KCF9L06iWNGHHoYysrvZCooMBbtm5tuP3669K2bV6w1JROnepDoV69pJ49m97u3dsLrvr04WloAAAcDI2hAQA+1qtXLy1fvlySdOedd6pLly665ZZb6t6vrq5WUlLTEUleXp7ymmqJ0g4RAqH9SkmpfxLZwVRXS7t3e72Kdu3y1uHb4euCAm+9e/fBg6OsLO+JaNnZ9evsbG8aWkaGt3TtWr+dkUFwBACID/QEAgA01403SsFAJmJOPFF68MEWfWTWrFlKS0vTsmXLNH78eE2fPl033HCDysvL1alTJ/3lL3/R0KFD9eabb+r+++/XK6+8ojvvvFNbtmzRxo0btWXLFt144426/vrrI/tdoogQCB1bUpIX3GRlNf8ztbVSSYkXBu3aJRUVeRVF27ZJhYX1y+LF3nuHk5LiVRSFml2HxhP+unt377iDLZ06Samprb8OAABEG9PBAADtUEFBgd577z0lJiZq7969evvtt5WUlKTXX39d/+///T+98MILB3xm3bp1WrRokfbt26ehQ4fq+9//vpLbyZOvCYGAxhISvKeR9ejhPc7+UCorvT5FJSXSvn1NL3v3emHSzp3e8skn3jS28CelNUenTt6YevY8cN2zpxcohRpn9+3rNeg+SDkjAAARFwh4T/5sJzfBAIAYamHFTjRdeumlSkxMlCSVlJRo5syZ+vTTT2VmqqqqavIz5513nlJTU5WamqqsrCx98cUXys3Nbcthtxp/QgSOREqK1L9/6z5bVuaFQTt3eiFSVZVUUeEFS42X/ful4mKvOmnPHm+9cWP99v79B54/IcELgkJPUQtNTTvYkp5eP5Wta9emF6qRAAAHEwjQDwgA0O507ty5bvtnP/uZJk6cqJdeekmbN2/WmWee2eRnUsP+XJSYmKjq6upoDzNiCIGAWOncWTr6aG85UuXlXqAU/hS10Hbo9eefHxguhUKn5v5Hq0uX+mbZ4Y2zQ9sZGd736tKl6XVamhc4mR35dwYA+EsgwFQwAEC7VlJSopycHEnSE088EdvBRAkhENARpKVJAwZ4S2vU1no373v31k9hCy2h18XFBzbU3rDB2y4padnPS072qopCVUih7U6dvLDoUEt6esOlU6f67c6d66fI0ZAbANpWeTkhEACgXfvJT36imTNn6pe//KXOO++8WA8nKsw5F5MfnJeX5/Lz82PyswFEWFWVNzWttNSb5tZ4HdquqKivPmpqHQg0PD60HVpa8t+rzp3r+yWFlh49vMApMdFbkpIO3O7cWerWzWvW3b37gdv0ugCazcw+cs51jOepdiBRuwebMUNaulRavz7y5wYAtHtr167V8OHDYz2MDqep63qoezAqgQAcueTklj+FraWc8/6WORDwlv3765fQ69LS+j5JjZe1a711ZaVUU+Mt1dX127W1zRtHqHopObnpdVKSN90tNOUttB16nZDgTY/r1u3gS9euB+6jzwbQYZnZFEm/lZQo6U/OuV81ev8BSRODL9MlZTnnugffu1fSeZISJL0m6QYXq7/hYzoYAAC+RwgEoH0w8/5wEa0/YDjnhUFlZd70tuJib2m8XVrqVT5VVja9rqqqr1hyrn4J/xmlpVJhoXe+vXu914eTklIfDnXvXv8Eu6aWrl0bhk+NA6mEhPrQKjzAatwonP5NQNSZWaKk2ZLOklQgaYmZzXPOrQkd45y7Kez4H0kaHdw+TdJ4SccH335H0gRJb7bJ4BujMTQAAL5HCAQAkhd2JCXVV9609qlvrVFT44VBJSX1S+PX4fuKi72Kp4KC+ifGHeTxlUckKcmrWmpqycg48Ely4a/T073pdQkJTS+JifX9oEI9oULbCQmR/y6Af42TtME5t1GSzGyOpGmS1hzk+BmS7ghuO0lpklIkmaRkSV9EdbSHQk8gAAB8jxAIAGItMbG+iqc1nPOmw+3Z4y379jWsQmpcmVRb602Fa/y0uFA1U0VF/fS6UH+m8KWw0FuHmocHApG7FpIXPqWmNj1tLnyqXHq6V8UUPiUvfAkPlhoHTeGNycM/Q+UT2l6OpK1hrwskndzUgWY2QNIgSW9IknPufTNbJGm7vBDod865tQf57DWSrpGk/tEKuQMB74mRAADAtwiBAKC9M6t/elpubtv//OrqA58qF2rkXVt74BLqwxRqCB7eHDx82bevYSXUtm31282ZQtcaiYkNQ6T0dC+M6ty5vgoqtH2wJ9aFL6EpdQdbkpLqzxM6J5VQOLjpkp53ztVIkpkdI2m4pNAv/mtm9hXn3NuNP+ice1TSo5LXGDoqo6MnEAAAvkcIBAA4MklJR1bJ1Bo1Nd7Uk1AfpvCeTKGlcbjUVNDU+DPhS2WlVxEVXg21bVvDCqn9+yM/Fa9Tp4YhU6jheOgpdqEl9DpU0RRe5dRU5VNoSUs7cN/48d65EAuFkvqFvc4N7mvKdEk/DHt9oaQPnHOlkmRmr0o6VdIBIVCbIAQCAMD3uOMDALQ/iYleQOIHVVUHPrEuFB5VVh44NS98qa4+cOpd43V19YHL/v1eEBYKq5oKuCorvWObo6yMECh2lkgaYmaD5IU/0yVd3vggMxsmqYek98N2b5F0tZn9r7zpYBMkPRj1ER8MjaEBAD43ceJE3XrrrTr77LPr9j344INav369HnnkkQOOP/PMM3X//fcrLy9P5557rv7+97+re/fuDY6588471aVLF91yyy0H/blz587VscceqxEjRkiSbr/9dp1xxhmaPHlyhL5Z83HHBwDAkQhNH+vaNdYjOVDjaXfl5QeGRRUV/ME9hpxz1WZ2naQF8h4R/2fn3Gozu0tSvnNuXvDQ6ZLmNHr8+/OSvipplbwm0f9yzr3chsNv6Pnn27YiEACAFpoxY4bmzJnTIASaM2eO7r333sN+dv78+a3+uXPnztX5559fFwLdddddrT7XkSIEAgCgo0pM9KbnMEXH15xz8yXNb7Tv9kav72ziczWSro3q4Fri1FNjPQIAQDtx479u1PIdyyN6zhP7nKgHpxy6IPaSSy7RbbfdpsrKSqWkpGjz5s3atm2bnnnmGf34xz9WIBDQJZdcop///OcHfHbgwIHKz89X7969dffdd+vJJ59UVlaW+vXrp5NOOkmS9Nhjj+nRRx9VZWWljjnmGD399NNavny55s2bp7feeku//OUv9cILL+gXv/iFzj//fF1yySVauHChbrnlFlVXV2vs2LF65JFHlJqaqoEDB2rmzJl6+eWXVVVVpeeee07Dhg074utE90kAAAAAANDh9ezZU+PGjdOrr74qyasC+sY3vqG7775b+fn5Wrlypd566y2tXLnyoOf46KOPNGfOHC1fvlzz58/XkiVL6t676KKLtGTJEq1YsULDhw/X448/rtNOO01Tp07Vfffdp+XLl2vw4MF1x5eXl2vWrFn6xz/+oVWrVqm6urrBtLTevXtr6dKl+v73v6/7778/IteASiAAAAAAANBmDlexE02hKWHTpk3TnDlz9Pjjj+vZZ5/Vo48+qurqam3fvl1r1qzR8ccf3+Tn3377bV144YVKT0+XJE2dOrXuvY8//li33XabiouLVVpa2mDaWVPWr1+vQYMG6dhjj5UkzZw5U7Nnz9aNN94oyQuVJOmkk07Siy++eMTfXaISCAAAAAAAxIlp06Zp4cKFWrp0qfbv36+ePXvq/vvv18KFC7Vy5Uqdd955Ki8vb9W5Z82apd/97ndatWqV7rjjjlafJyQ1NVWSlJiYqOrmPvDjMAiBAAAAAABAXOjSpYsmTpyob3/725oxY4b27t2rzp07q1u3bvriiy/qpoodzBlnnKG5c+cqEAho3759evnl+mcy7Nu3T3379lVVVZX+9re/1e3PyMjQvn37DjjX0KFDtXnzZm3YsEGS9PTTT2vChAkR+qZNIwQCAAAAAABxY8aMGVqxYoVmzJihE044QaNHj9awYcN0+eWXa/z48Yf87JgxY3TZZZfphBNO0DnnnKOxY8fWvfeLX/xCJ598ssaPH9+gifP06dN13333afTo0frss8/q9qelpekvf/mLLr30Uo0aNUoJCQn63ve+F/kvHMYaPmm07eTl5bn8/PyY/GwAABB9ZvaRcy4v1uNAQ9yDAQBiYe3atRo+fHish9HhNHVdD3UPRiUQAAAAAABAHCAEAgAAAAAAiAPNCoHMbIqZrTezDWZ2axPv/9jM1pjZSjNbaGYDIj9UAAAAAADQXsWqHU1H1ZrredgQyMwSJc2WdI6kEZJmmNmIRoctk5TnnDte0vOS7m3xSAAAAAAAQIeUlpamXbt2EQRFiHNOu3btUlpaWos+l9SMY8ZJ2uCc2yhJZjZH0jRJa8J++KKw4z+QdEWLRgEAAAAAADqs3NxcFRQUqKioKNZD6TDS0tKUm5vbos80JwTKkbQ17HWBpJMPcfx3JL3a1Btmdo2kaySpf//+zRwiAAAAAABoz5KTkzVo0KBYDyPuRbQxtJldISlP0n1Nve+ce9Q5l+ecy8vMzIzkjwYAAAAAAMAhNKcSqFBSv7DXucF9DZjZZEn/I2mCc64iMsMDAAAAAABAJDSnEmiJpCFmNsjMUiRNlzQv/AAzGy3pj5KmOud2Rn6YAAAAAAAAOBLWnM7cZnaupAclJUr6s3PubjO7S1K+c26emb0uaZSk7cGPbHHOTT3MOYskfX5Eoz+43pK+jNK54wnXMXK4lpHBdYwcrmVkcB0PbYBzjvnfPsM9WLvAdYwcrmVkcB0jg+sYOVzLQzvoPVizQqD2xszynXN5sR5He8d1jByuZWRwHSOHaxkZXEegIX4nIoPrGDlcy8jgOkYG1zFyuJatF9HG0AAAAAAAAPAnQiAAAAAAAIA40FFDoEdjPYAOgusYOVzLyOA6Rg7XMjK4jkBD/E5EBtcxcriWkcF1jAyuY+RwLVupQ/YEAgAAAAAAQEMdtRIIAAAAAAAAYQiBAAAAAAAA4kCHC4HMbIqZrTezDWZ2a6zH016Y2Z/NbKeZfRy2r6eZvWZmnwbXPWI5xvbAzPqZ2SIzW2Nmq83shuB+rmULmVmamS02sxXBa/nz4P5BZvZh8Hf8H2aWEuuxtgdmlmhmy8zsleBrrmMLmdlmM1tlZsvNLD+4j99tQNx/HQnuwSKDe7DI4P4rsrj/igzuwSKrQ4VAZpYoabakcySNkDTDzEbEdlTtxhOSpjTad6ukhc65IZIWBl/j0Kol3eycGyHpFEk/DP47yLVsuQpJX3XOnSDpRElTzOwUSfdIesA5d4ykPZK+E8Mxtic3SFob9prr2DoTnXMnOufygq/53Ubc4/7riD0h7sEigXuwyOD+K7K4/4oc7sEipEOFQJLGSdrgnNvonKuUNEfStBiPqV1wzv1H0u5Gu6dJejK4/aSkr7fpoNoh59x259zS4PY+ef/RzxHXssWcpzT4Mjm4OElflfR8cD/XshnMLFfSeZL+FHxt4jpGCr/bAPdfR4R7sMjgHiwyuP+KHO6/oo7f7VbqaCFQjqStYa8LgvvQOkc557YHt3dIOiqWg2lvzGygpNGSPhTXslWCJbTLJe2U9JqkzyQVO+eqg4fwO948D0r6iaTa4Ote4jq2hpP0bzP7yMyuCe7jdxvg/isa+G/LEeAe7Mhw/xUx3H9FDvdgEZQU6wGgfXDOOTNzsR5He2FmXSS9IOlG59xeL/j3cC2bzzlXI+lEM+su6SVJw2I8pHbHzM6XtNM595GZnRnr8bRzpzvnCs0sS9JrZrYu/E1+twFEA/9taRnuwY4c919HjvuviOMeLII6WiVQoaR+Ya9zg/vQOl+YWV9JCq53xng87YKZJcu7+fibc+7F4G6u5RFwzhVLWiT9/3bunzWKKArD+HOICCKC+KcTESGtpSBaiKCFSCoRQSFfIo02gpBW8ANoqZBGzQfQwtLCQsE2Fhb6DaxeizvBoEIY94Zld55fs7PsFpcDd3g5M+dyCTheVbsNbPf4/i4Da1W1QxvRuAY8xTqOluTb8PmDFoov4t6WwPx1ELy3/AczWF/mr5mYvzoyg/W1bE2gD8DqcOr6YeAusD3nNS2ybWB9uF4H3sxxLQthmPV9BnxJ8mTPT9ZypKo6PTyBoqqOANdp8/3vgNvD36zlPpI8SHImyTnaPfFtkntYx1Gq6mhVHdu9Bm4An3FvS2D+OgjeW0Yyg/Vh/urD/NWPGay/Spbrramqukmbv1wBnifZnPOSFkJVvQSuAqeA78Aj4DWwBZwFvgJ3kvx5cKH2qKorwHvgE7/nfx/SZtKt5QhVdYF2yNsKrWG9leRxVZ2nPVE5AXwE7if5Ob+VLo7hdeSNJLes4zhDvV4NXw8BL5JsVtVJ3NuS+WsGZrA+zGB9mL/6M3/NxgzW39I1gSRJkiRJkvS3ZRsHkyRJkiRJ0j/YBJIkSZIkSZoAm0CSJEmSJEkTYBNIkiRJkiRpAmwCSZIkSZIkTYBNIEmSJEmSpAmwCSRJkiRJkjQBvwCCLXvd3sIpvgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 1440x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"vuXj5QS-YwtD","colab_type":"text"},"source":["#### Regularization\n"]},{"cell_type":"markdown","metadata":{"id":"4j0ae6gLY7n7","colab_type":"text"},"source":["##### **Weight Regularization**\n","The Occam's Razor principle states: given two explanations for something, the explanation most likely to be correct is the \"simplest\" one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simpler models are less likely to overfit than complex ones.\n","\n","A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more \"regular\". This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:\n","\n","*  **L1 regularization**, where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n","*   **L2 regularization**, where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the squared \"L2 norm\" of the weights). L2 regularization is also called weight decay in the context of neural networks. \n","\n","L1 regularization pushes weights towards exactly zero encouraging a sparse model. L2 regularization will penalize the weights parameters without making them sparse since the penalty goes to zero for small weights, one reason why L2 is more common.\n"]},{"cell_type":"markdown","metadata":{"id":"rM-CiwEAaYtd","colab_type":"text"},"source":["Let's then proceed in making a L2 regularization."]},{"cell_type":"code","metadata":{"id":"eI5kFFkNbMZX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":274},"outputId":"d3c27120-71f7-43d6-d070-c70fe15e4d0b","executionInfo":{"status":"ok","timestamp":1590941070489,"user_tz":-60,"elapsed":772,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["mnist_hidden_kr_model = tf.keras.Sequential(name='mnist_hidden')\n","mnist_hidden_kr_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_hidden_kr_model.add(tf.keras.layers.Dense(32, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.01), name='hidden'))\n","mnist_hidden_kr_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_hidden_kr_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_hidden_kr_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_hidden_kr_model.summary()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Model: \"mnist_hidden\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","hidden (Dense)               (None, 28, 28, 32)        64        \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                250890    \n","=================================================================\n","Total params: 250,954\n","Trainable params: 250,954\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Crwju3kWev0H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"1e80be7b-ec24-444c-bedf-00fa1798c73d","executionInfo":{"status":"ok","timestamp":1590942094579,"user_tz":-60,"elapsed":843179,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_hidden_kr__best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_hidden_kr_model_train = mnist_hidden_kr_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","187/188 [============================>.] - ETA: 0s - loss: 1.3540 - accuracy: 0.7558\n","Epoch 00001: val_accuracy improved from -inf to 0.83742, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 1.3524 - accuracy: 0.7560 - val_loss: 0.8044 - val_accuracy: 0.8374\n","Epoch 2/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.6581 - accuracy: 0.8550\n","Epoch 00002: val_accuracy improved from 0.83742 to 0.86658, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.6581 - accuracy: 0.8549 - val_loss: 0.5729 - val_accuracy: 0.8666\n","Epoch 3/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.5305 - accuracy: 0.8749\n","Epoch 00003: val_accuracy improved from 0.86658 to 0.87958, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.5304 - accuracy: 0.8749 - val_loss: 0.5041 - val_accuracy: 0.8796\n","Epoch 4/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4824 - accuracy: 0.8840\n","Epoch 00004: val_accuracy improved from 0.87958 to 0.88767, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.4825 - accuracy: 0.8839 - val_loss: 0.4711 - val_accuracy: 0.8877\n","Epoch 5/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4567 - accuracy: 0.8894\n","Epoch 00005: val_accuracy improved from 0.88767 to 0.89267, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.4567 - accuracy: 0.8893 - val_loss: 0.4519 - val_accuracy: 0.8927\n","Epoch 6/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4403 - accuracy: 0.8930\n","Epoch 00006: val_accuracy improved from 0.89267 to 0.89525, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.4402 - accuracy: 0.8930 - val_loss: 0.4386 - val_accuracy: 0.8953\n","Epoch 7/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4278 - accuracy: 0.8966\n","Epoch 00007: val_accuracy improved from 0.89525 to 0.89767, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.4280 - accuracy: 0.8966 - val_loss: 0.4291 - val_accuracy: 0.8977\n","Epoch 8/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4188 - accuracy: 0.8991\n","Epoch 00008: val_accuracy improved from 0.89767 to 0.89992, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.4188 - accuracy: 0.8991 - val_loss: 0.4209 - val_accuracy: 0.8999\n","Epoch 9/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.9009\n","Epoch 00009: val_accuracy improved from 0.89992 to 0.90042, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.4112 - accuracy: 0.9010 - val_loss: 0.4147 - val_accuracy: 0.9004\n","Epoch 10/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4049 - accuracy: 0.9024\n","Epoch 00010: val_accuracy improved from 0.90042 to 0.90200, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.4047 - accuracy: 0.9024 - val_loss: 0.4096 - val_accuracy: 0.9020\n","Epoch 11/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3991 - accuracy: 0.9043\n","Epoch 00011: val_accuracy improved from 0.90200 to 0.90267, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3992 - accuracy: 0.9043 - val_loss: 0.4044 - val_accuracy: 0.9027\n","Epoch 12/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3943 - accuracy: 0.9054\n","Epoch 00012: val_accuracy improved from 0.90267 to 0.90408, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3945 - accuracy: 0.9054 - val_loss: 0.4006 - val_accuracy: 0.9041\n","Epoch 13/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3903 - accuracy: 0.9065\n","Epoch 00013: val_accuracy improved from 0.90408 to 0.90442, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3901 - accuracy: 0.9065 - val_loss: 0.3971 - val_accuracy: 0.9044\n","Epoch 14/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3862 - accuracy: 0.9069\n","Epoch 00014: val_accuracy improved from 0.90442 to 0.90592, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3862 - accuracy: 0.9070 - val_loss: 0.3935 - val_accuracy: 0.9059\n","Epoch 15/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3829 - accuracy: 0.9080\n","Epoch 00015: val_accuracy improved from 0.90592 to 0.90600, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3827 - accuracy: 0.9080 - val_loss: 0.3908 - val_accuracy: 0.9060\n","Epoch 16/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3795 - accuracy: 0.9087\n","Epoch 00016: val_accuracy improved from 0.90600 to 0.90758, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3795 - accuracy: 0.9086 - val_loss: 0.3878 - val_accuracy: 0.9076\n","Epoch 17/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3765 - accuracy: 0.9099\n","Epoch 00017: val_accuracy did not improve from 0.90758\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3765 - accuracy: 0.9099 - val_loss: 0.3855 - val_accuracy: 0.9074\n","Epoch 18/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3737 - accuracy: 0.9103\n","Epoch 00018: val_accuracy improved from 0.90758 to 0.90808, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3738 - accuracy: 0.9103 - val_loss: 0.3835 - val_accuracy: 0.9081\n","Epoch 19/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3712 - accuracy: 0.9107\n","Epoch 00019: val_accuracy improved from 0.90808 to 0.90942, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3713 - accuracy: 0.9107 - val_loss: 0.3811 - val_accuracy: 0.9094\n","Epoch 20/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3690 - accuracy: 0.9113\n","Epoch 00020: val_accuracy improved from 0.90942 to 0.91000, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3689 - accuracy: 0.9113 - val_loss: 0.3789 - val_accuracy: 0.9100\n","Epoch 21/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.9119\n","Epoch 00021: val_accuracy improved from 0.91000 to 0.91042, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3665 - accuracy: 0.9120 - val_loss: 0.3775 - val_accuracy: 0.9104\n","Epoch 22/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3645 - accuracy: 0.9123\n","Epoch 00022: val_accuracy did not improve from 0.91042\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3645 - accuracy: 0.9122 - val_loss: 0.3751 - val_accuracy: 0.9103\n","Epoch 23/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3627 - accuracy: 0.9125\n","Epoch 00023: val_accuracy improved from 0.91042 to 0.91133, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 59ms/step - loss: 0.3625 - accuracy: 0.9126 - val_loss: 0.3735 - val_accuracy: 0.9113\n","Epoch 24/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3606 - accuracy: 0.9133\n","Epoch 00024: val_accuracy did not improve from 0.91133\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3607 - accuracy: 0.9133 - val_loss: 0.3721 - val_accuracy: 0.9108\n","Epoch 25/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.9131\n","Epoch 00025: val_accuracy improved from 0.91133 to 0.91158, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3588 - accuracy: 0.9131 - val_loss: 0.3704 - val_accuracy: 0.9116\n","Epoch 26/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3572 - accuracy: 0.9137\n","Epoch 00026: val_accuracy did not improve from 0.91158\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3571 - accuracy: 0.9137 - val_loss: 0.3692 - val_accuracy: 0.9113\n","Epoch 27/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3553 - accuracy: 0.9137\n","Epoch 00027: val_accuracy did not improve from 0.91158\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3556 - accuracy: 0.9136 - val_loss: 0.3678 - val_accuracy: 0.9112\n","Epoch 28/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3536 - accuracy: 0.9141\n","Epoch 00028: val_accuracy improved from 0.91158 to 0.91242, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3539 - accuracy: 0.9141 - val_loss: 0.3665 - val_accuracy: 0.9124\n","Epoch 29/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3522 - accuracy: 0.9146\n","Epoch 00029: val_accuracy did not improve from 0.91242\n","188/188 [==============================] - 11s 59ms/step - loss: 0.3526 - accuracy: 0.9145 - val_loss: 0.3653 - val_accuracy: 0.9118\n","Epoch 30/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3512 - accuracy: 0.9148\n","Epoch 00030: val_accuracy did not improve from 0.91242\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3512 - accuracy: 0.9147 - val_loss: 0.3641 - val_accuracy: 0.9122\n","Epoch 31/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3497 - accuracy: 0.9151\n","Epoch 00031: val_accuracy improved from 0.91242 to 0.91292, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3498 - accuracy: 0.9151 - val_loss: 0.3630 - val_accuracy: 0.9129\n","Epoch 32/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.9151\n","Epoch 00032: val_accuracy did not improve from 0.91292\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3485 - accuracy: 0.9152 - val_loss: 0.3619 - val_accuracy: 0.9127\n","Epoch 33/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3471 - accuracy: 0.9157\n","Epoch 00033: val_accuracy did not improve from 0.91292\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3473 - accuracy: 0.9157 - val_loss: 0.3607 - val_accuracy: 0.9127\n","Epoch 34/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3461 - accuracy: 0.9158\n","Epoch 00034: val_accuracy improved from 0.91292 to 0.91367, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3460 - accuracy: 0.9159 - val_loss: 0.3602 - val_accuracy: 0.9137\n","Epoch 35/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3452 - accuracy: 0.9162\n","Epoch 00035: val_accuracy did not improve from 0.91367\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3450 - accuracy: 0.9162 - val_loss: 0.3591 - val_accuracy: 0.9130\n","Epoch 36/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3436 - accuracy: 0.9164\n","Epoch 00036: val_accuracy did not improve from 0.91367\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3438 - accuracy: 0.9163 - val_loss: 0.3578 - val_accuracy: 0.9126\n","Epoch 37/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3428 - accuracy: 0.9170\n","Epoch 00037: val_accuracy did not improve from 0.91367\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3428 - accuracy: 0.9170 - val_loss: 0.3572 - val_accuracy: 0.9128\n","Epoch 38/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3417 - accuracy: 0.9170\n","Epoch 00038: val_accuracy improved from 0.91367 to 0.91425, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3417 - accuracy: 0.9170 - val_loss: 0.3567 - val_accuracy: 0.9143\n","Epoch 39/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3407 - accuracy: 0.9166\n","Epoch 00039: val_accuracy did not improve from 0.91425\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3408 - accuracy: 0.9167 - val_loss: 0.3555 - val_accuracy: 0.9131\n","Epoch 40/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3396 - accuracy: 0.9169\n","Epoch 00040: val_accuracy improved from 0.91425 to 0.91450, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3398 - accuracy: 0.9169 - val_loss: 0.3544 - val_accuracy: 0.9145\n","Epoch 41/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3388 - accuracy: 0.9178\n","Epoch 00041: val_accuracy did not improve from 0.91450\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3388 - accuracy: 0.9178 - val_loss: 0.3538 - val_accuracy: 0.9138\n","Epoch 42/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3380 - accuracy: 0.9172\n","Epoch 00042: val_accuracy did not improve from 0.91450\n","188/188 [==============================] - 11s 59ms/step - loss: 0.3380 - accuracy: 0.9172 - val_loss: 0.3531 - val_accuracy: 0.9133\n","Epoch 43/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3370 - accuracy: 0.9177\n","Epoch 00043: val_accuracy improved from 0.91450 to 0.91458, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3370 - accuracy: 0.9177 - val_loss: 0.3523 - val_accuracy: 0.9146\n","Epoch 44/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3364 - accuracy: 0.9179\n","Epoch 00044: val_accuracy did not improve from 0.91458\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3362 - accuracy: 0.9179 - val_loss: 0.3514 - val_accuracy: 0.9145\n","Epoch 45/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.9179\n","Epoch 00045: val_accuracy did not improve from 0.91458\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3353 - accuracy: 0.9180 - val_loss: 0.3512 - val_accuracy: 0.9136\n","Epoch 46/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.9183\n","Epoch 00046: val_accuracy did not improve from 0.91458\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3345 - accuracy: 0.9182 - val_loss: 0.3508 - val_accuracy: 0.9146\n","Epoch 47/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3339 - accuracy: 0.9188\n","Epoch 00047: val_accuracy improved from 0.91458 to 0.91467, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3338 - accuracy: 0.9187 - val_loss: 0.3501 - val_accuracy: 0.9147\n","Epoch 48/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3331 - accuracy: 0.9183\n","Epoch 00048: val_accuracy improved from 0.91467 to 0.91525, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3331 - accuracy: 0.9184 - val_loss: 0.3489 - val_accuracy: 0.9153\n","Epoch 49/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3323 - accuracy: 0.9190\n","Epoch 00049: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3323 - accuracy: 0.9190 - val_loss: 0.3481 - val_accuracy: 0.9149\n","Epoch 50/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.9183\n","Epoch 00050: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3315 - accuracy: 0.9183 - val_loss: 0.3476 - val_accuracy: 0.9150\n","Epoch 51/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3309 - accuracy: 0.9190\n","Epoch 00051: val_accuracy did not improve from 0.91525\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3309 - accuracy: 0.9190 - val_loss: 0.3474 - val_accuracy: 0.9149\n","Epoch 52/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3304 - accuracy: 0.9192\n","Epoch 00052: val_accuracy improved from 0.91525 to 0.91550, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3302 - accuracy: 0.9193 - val_loss: 0.3466 - val_accuracy: 0.9155\n","Epoch 53/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3298 - accuracy: 0.9192\n","Epoch 00053: val_accuracy did not improve from 0.91550\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3296 - accuracy: 0.9193 - val_loss: 0.3461 - val_accuracy: 0.9155\n","Epoch 54/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3286 - accuracy: 0.9193\n","Epoch 00054: val_accuracy did not improve from 0.91550\n","188/188 [==============================] - 11s 61ms/step - loss: 0.3289 - accuracy: 0.9192 - val_loss: 0.3456 - val_accuracy: 0.9149\n","Epoch 55/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3281 - accuracy: 0.9192\n","Epoch 00055: val_accuracy improved from 0.91550 to 0.91600, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3282 - accuracy: 0.9191 - val_loss: 0.3453 - val_accuracy: 0.9160\n","Epoch 56/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3277 - accuracy: 0.9200\n","Epoch 00056: val_accuracy did not improve from 0.91600\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3276 - accuracy: 0.9200 - val_loss: 0.3448 - val_accuracy: 0.9147\n","Epoch 57/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3272 - accuracy: 0.9195\n","Epoch 00057: val_accuracy did not improve from 0.91600\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3270 - accuracy: 0.9196 - val_loss: 0.3440 - val_accuracy: 0.9157\n","Epoch 58/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3265 - accuracy: 0.9200\n","Epoch 00058: val_accuracy did not improve from 0.91600\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3265 - accuracy: 0.9200 - val_loss: 0.3434 - val_accuracy: 0.9156\n","Epoch 59/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.9201\n","Epoch 00059: val_accuracy did not improve from 0.91600\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3259 - accuracy: 0.9201 - val_loss: 0.3433 - val_accuracy: 0.9153\n","Epoch 60/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3254 - accuracy: 0.9200\n","Epoch 00060: val_accuracy did not improve from 0.91600\n","188/188 [==============================] - 11s 59ms/step - loss: 0.3253 - accuracy: 0.9200 - val_loss: 0.3431 - val_accuracy: 0.9157\n","Epoch 61/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.9206\n","Epoch 00061: val_accuracy did not improve from 0.91600\n","188/188 [==============================] - 11s 59ms/step - loss: 0.3248 - accuracy: 0.9206 - val_loss: 0.3423 - val_accuracy: 0.9156\n","Epoch 62/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.9209\n","Epoch 00062: val_accuracy improved from 0.91600 to 0.91692, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 59ms/step - loss: 0.3242 - accuracy: 0.9208 - val_loss: 0.3418 - val_accuracy: 0.9169\n","Epoch 63/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.9204\n","Epoch 00063: val_accuracy did not improve from 0.91692\n","188/188 [==============================] - 11s 59ms/step - loss: 0.3237 - accuracy: 0.9205 - val_loss: 0.3412 - val_accuracy: 0.9161\n","Epoch 64/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3230 - accuracy: 0.9207\n","Epoch 00064: val_accuracy improved from 0.91692 to 0.91700, saving model to mnist_hidden_kr__best.h5\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3231 - accuracy: 0.9207 - val_loss: 0.3411 - val_accuracy: 0.9170\n","Epoch 65/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.9211\n","Epoch 00065: val_accuracy did not improve from 0.91700\n","188/188 [==============================] - 11s 59ms/step - loss: 0.3226 - accuracy: 0.9210 - val_loss: 0.3413 - val_accuracy: 0.9165\n","Epoch 66/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.9205\n","Epoch 00066: val_accuracy did not improve from 0.91700\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3221 - accuracy: 0.9205 - val_loss: 0.3400 - val_accuracy: 0.9162\n","Epoch 67/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3216 - accuracy: 0.9211\n","Epoch 00067: val_accuracy did not improve from 0.91700\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3217 - accuracy: 0.9211 - val_loss: 0.3400 - val_accuracy: 0.9162\n","Epoch 68/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.9210\n","Epoch 00068: val_accuracy did not improve from 0.91700\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3212 - accuracy: 0.9209 - val_loss: 0.3393 - val_accuracy: 0.9158\n","Epoch 69/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.9212\n","Epoch 00069: val_accuracy did not improve from 0.91700\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3207 - accuracy: 0.9212 - val_loss: 0.3390 - val_accuracy: 0.9169\n","Epoch 70/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.9211\n","Epoch 00070: val_accuracy did not improve from 0.91700\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3202 - accuracy: 0.9211 - val_loss: 0.3386 - val_accuracy: 0.9158\n","Epoch 71/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.9214\n","Epoch 00071: val_accuracy did not improve from 0.91700\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3197 - accuracy: 0.9215 - val_loss: 0.3388 - val_accuracy: 0.9170\n","Epoch 72/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.9218\n","Epoch 00072: val_accuracy did not improve from 0.91700\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3194 - accuracy: 0.9218 - val_loss: 0.3382 - val_accuracy: 0.9161\n","Epoch 73/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3189 - accuracy: 0.9212\n","Epoch 00073: val_accuracy did not improve from 0.91700\n","188/188 [==============================] - 11s 59ms/step - loss: 0.3189 - accuracy: 0.9212 - val_loss: 0.3375 - val_accuracy: 0.9170\n","Epoch 74/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3184 - accuracy: 0.9216\n","Epoch 00074: val_accuracy did not improve from 0.91700\n","188/188 [==============================] - 11s 60ms/step - loss: 0.3184 - accuracy: 0.9216 - val_loss: 0.3374 - val_accuracy: 0.9168\n","Epoch 00074: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NYelDJDQi-3B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"fbb80813-ab27-44a3-8472-6b023c173666","executionInfo":{"status":"ok","timestamp":1590942287344,"user_tz":-60,"elapsed":2025,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["mnist_hidden_kr_model.load_weights('mnist_hidden_kr__best.h5')\n","loss, acc = mnist_hidden_kr_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 1s 4ms/step - loss: 0.3226 - accuracy: 0.9210\n","Accuracy: 0.9210000038146973\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hbgFsY1hM6UC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":444},"outputId":"6c24c6fc-fc85-4d05-87bd-d70ec3b3a859","executionInfo":{"status":"ok","timestamp":1590953927692,"user_tz":-60,"elapsed":1771,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_hidden_kr_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_hidden_kr_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_hidden_kr_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_hidden_kr_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":54,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Q2lT4pDOqCzN","colab_type":"text"},"source":["##### Dropout\n","\n","In this scenario, instead of applying regularization to the weights, we will use a different approach to regularization, namely, dropout. The idea behind dropout is to disable a percentage of randomly selected neurons during each step of the training phase, in order to avoid overfitting. In [Keras](https://www.tensorflow.org/api_docs/python/tf/keras), we can apply dropout directly to some layers by defining the corresponding parameters, or by using the [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layer and stating the percentage of neurons to disable."]},{"cell_type":"code","metadata":{"id":"ryzuICrvqEOV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":311},"outputId":"358113f9-ba0f-4eb1-c85b-04c72128b48d","executionInfo":{"status":"ok","timestamp":1590944642914,"user_tz":-60,"elapsed":1075,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["mnist_hidden_do_model = tf.keras.Sequential(name='mnist_hidden_do')\n","mnist_hidden_do_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_hidden_do_model.add(tf.keras.layers.Dense(32, activation='tanh', name='hidden'))\n","mnist_hidden_do_model.add(tf.keras.layers.Dropout(0.5, name='dropout'))\n","mnist_hidden_do_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_hidden_do_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_hidden_do_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_hidden_do_model.summary()"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Model: \"mnist_hidden_do\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","hidden (Dense)               (None, 28, 28, 32)        64        \n","_________________________________________________________________\n","dropout (Dropout)            (None, 28, 28, 32)        0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                250890    \n","=================================================================\n","Total params: 250,954\n","Trainable params: 250,954\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ph7kjbpPsQhu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d097afbd-456a-4090-bd12-03f905dce034","executionInfo":{"status":"ok","timestamp":1590947396993,"user_tz":-60,"elapsed":2588429,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_hidden_do__best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_hidden_do_model_train = mnist_hidden_do_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","188/188 [==============================] - ETA: 0s - loss: 1.3777 - accuracy: 0.7192\n","Epoch 00001: val_accuracy improved from -inf to 0.83275, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 1.3777 - accuracy: 0.7192 - val_loss: 0.7847 - val_accuracy: 0.8328\n","Epoch 2/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.6285 - accuracy: 0.8486\n","Epoch 00002: val_accuracy improved from 0.83275 to 0.86725, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.6285 - accuracy: 0.8486 - val_loss: 0.5225 - val_accuracy: 0.8673\n","Epoch 3/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.8709\n","Epoch 00003: val_accuracy improved from 0.86725 to 0.88083, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 121ms/step - loss: 0.4830 - accuracy: 0.8709 - val_loss: 0.4429 - val_accuracy: 0.8808\n","Epoch 4/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4271 - accuracy: 0.8806\n","Epoch 00004: val_accuracy improved from 0.88083 to 0.88808, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 121ms/step - loss: 0.4271 - accuracy: 0.8806 - val_loss: 0.4048 - val_accuracy: 0.8881\n","Epoch 5/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3972 - accuracy: 0.8870\n","Epoch 00005: val_accuracy improved from 0.88808 to 0.89242, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 121ms/step - loss: 0.3972 - accuracy: 0.8870 - val_loss: 0.3825 - val_accuracy: 0.8924\n","Epoch 6/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3778 - accuracy: 0.8924\n","Epoch 00006: val_accuracy improved from 0.89242 to 0.89600, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 121ms/step - loss: 0.3778 - accuracy: 0.8924 - val_loss: 0.3671 - val_accuracy: 0.8960\n","Epoch 7/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.8947\n","Epoch 00007: val_accuracy improved from 0.89600 to 0.89867, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 122ms/step - loss: 0.3648 - accuracy: 0.8947 - val_loss: 0.3562 - val_accuracy: 0.8987\n","Epoch 8/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3549 - accuracy: 0.8986\n","Epoch 00008: val_accuracy improved from 0.89867 to 0.90117, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.3549 - accuracy: 0.8986 - val_loss: 0.3477 - val_accuracy: 0.9012\n","Epoch 9/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3459 - accuracy: 0.8994\n","Epoch 00009: val_accuracy improved from 0.90117 to 0.90217, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 122ms/step - loss: 0.3459 - accuracy: 0.8994 - val_loss: 0.3410 - val_accuracy: 0.9022\n","Epoch 10/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3388 - accuracy: 0.9019\n","Epoch 00010: val_accuracy improved from 0.90217 to 0.90350, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 122ms/step - loss: 0.3388 - accuracy: 0.9019 - val_loss: 0.3352 - val_accuracy: 0.9035\n","Epoch 11/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3333 - accuracy: 0.9030\n","Epoch 00011: val_accuracy improved from 0.90350 to 0.90450, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 122ms/step - loss: 0.3333 - accuracy: 0.9030 - val_loss: 0.3307 - val_accuracy: 0.9045\n","Epoch 12/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3285 - accuracy: 0.9049\n","Epoch 00012: val_accuracy improved from 0.90450 to 0.90683, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 122ms/step - loss: 0.3285 - accuracy: 0.9049 - val_loss: 0.3267 - val_accuracy: 0.9068\n","Epoch 13/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3242 - accuracy: 0.9059\n","Epoch 00013: val_accuracy improved from 0.90683 to 0.90758, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 122ms/step - loss: 0.3242 - accuracy: 0.9059 - val_loss: 0.3232 - val_accuracy: 0.9076\n","Epoch 14/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3200 - accuracy: 0.9086\n","Epoch 00014: val_accuracy improved from 0.90758 to 0.90808, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.3200 - accuracy: 0.9086 - val_loss: 0.3204 - val_accuracy: 0.9081\n","Epoch 15/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3168 - accuracy: 0.9080\n","Epoch 00015: val_accuracy improved from 0.90808 to 0.91025, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.3168 - accuracy: 0.9080 - val_loss: 0.3176 - val_accuracy: 0.9103\n","Epoch 16/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3143 - accuracy: 0.9099\n","Epoch 00016: val_accuracy improved from 0.91025 to 0.91100, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 26s 140ms/step - loss: 0.3143 - accuracy: 0.9099 - val_loss: 0.3151 - val_accuracy: 0.9110\n","Epoch 17/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.9101\n","Epoch 00017: val_accuracy improved from 0.91100 to 0.91125, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.3111 - accuracy: 0.9101 - val_loss: 0.3138 - val_accuracy: 0.9112\n","Epoch 18/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.9099\n","Epoch 00018: val_accuracy improved from 0.91125 to 0.91200, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.3094 - accuracy: 0.9099 - val_loss: 0.3112 - val_accuracy: 0.9120\n","Epoch 19/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3073 - accuracy: 0.9109\n","Epoch 00019: val_accuracy improved from 0.91200 to 0.91333, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.3073 - accuracy: 0.9109 - val_loss: 0.3099 - val_accuracy: 0.9133\n","Epoch 20/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3048 - accuracy: 0.9125\n","Epoch 00020: val_accuracy did not improve from 0.91333\n","188/188 [==============================] - 23s 123ms/step - loss: 0.3048 - accuracy: 0.9125 - val_loss: 0.3079 - val_accuracy: 0.9133\n","Epoch 21/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3025 - accuracy: 0.9127\n","Epoch 00021: val_accuracy improved from 0.91333 to 0.91375, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.3025 - accuracy: 0.9127 - val_loss: 0.3069 - val_accuracy: 0.9137\n","Epoch 22/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.9127\n","Epoch 00022: val_accuracy improved from 0.91375 to 0.91450, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.3024 - accuracy: 0.9127 - val_loss: 0.3056 - val_accuracy: 0.9145\n","Epoch 23/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.9131\n","Epoch 00023: val_accuracy improved from 0.91450 to 0.91542, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.3009 - accuracy: 0.9131 - val_loss: 0.3048 - val_accuracy: 0.9154\n","Epoch 24/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.9143\n","Epoch 00024: val_accuracy did not improve from 0.91542\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2986 - accuracy: 0.9143 - val_loss: 0.3039 - val_accuracy: 0.9148\n","Epoch 25/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2978 - accuracy: 0.9140\n","Epoch 00025: val_accuracy improved from 0.91542 to 0.91583, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2978 - accuracy: 0.9140 - val_loss: 0.3026 - val_accuracy: 0.9158\n","Epoch 26/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.9156\n","Epoch 00026: val_accuracy did not improve from 0.91583\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2943 - accuracy: 0.9156 - val_loss: 0.3012 - val_accuracy: 0.9158\n","Epoch 27/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2949 - accuracy: 0.9162\n","Epoch 00027: val_accuracy did not improve from 0.91583\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2949 - accuracy: 0.9162 - val_loss: 0.3003 - val_accuracy: 0.9151\n","Epoch 28/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.9160\n","Epoch 00028: val_accuracy improved from 0.91583 to 0.91617, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2929 - accuracy: 0.9160 - val_loss: 0.2999 - val_accuracy: 0.9162\n","Epoch 29/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.9168\n","Epoch 00029: val_accuracy improved from 0.91617 to 0.91692, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2914 - accuracy: 0.9168 - val_loss: 0.2990 - val_accuracy: 0.9169\n","Epoch 30/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.9169\n","Epoch 00030: val_accuracy did not improve from 0.91692\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2908 - accuracy: 0.9169 - val_loss: 0.2987 - val_accuracy: 0.9167\n","Epoch 31/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2900 - accuracy: 0.9169\n","Epoch 00031: val_accuracy did not improve from 0.91692\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2900 - accuracy: 0.9169 - val_loss: 0.2975 - val_accuracy: 0.9168\n","Epoch 32/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2896 - accuracy: 0.9176\n","Epoch 00032: val_accuracy improved from 0.91692 to 0.91700, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2896 - accuracy: 0.9176 - val_loss: 0.2976 - val_accuracy: 0.9170\n","Epoch 33/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2880 - accuracy: 0.9184\n","Epoch 00033: val_accuracy improved from 0.91700 to 0.91742, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2880 - accuracy: 0.9184 - val_loss: 0.2963 - val_accuracy: 0.9174\n","Epoch 34/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.9182\n","Epoch 00034: val_accuracy improved from 0.91742 to 0.91767, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2877 - accuracy: 0.9182 - val_loss: 0.2958 - val_accuracy: 0.9177\n","Epoch 35/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.9180\n","Epoch 00035: val_accuracy improved from 0.91767 to 0.91792, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2857 - accuracy: 0.9180 - val_loss: 0.2955 - val_accuracy: 0.9179\n","Epoch 36/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2851 - accuracy: 0.9188\n","Epoch 00036: val_accuracy did not improve from 0.91792\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2851 - accuracy: 0.9188 - val_loss: 0.2948 - val_accuracy: 0.9177\n","Epoch 37/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.9195\n","Epoch 00037: val_accuracy improved from 0.91792 to 0.91833, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2844 - accuracy: 0.9195 - val_loss: 0.2941 - val_accuracy: 0.9183\n","Epoch 38/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.9185\n","Epoch 00038: val_accuracy improved from 0.91833 to 0.91858, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2844 - accuracy: 0.9185 - val_loss: 0.2942 - val_accuracy: 0.9186\n","Epoch 39/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2842 - accuracy: 0.9193\n","Epoch 00039: val_accuracy did not improve from 0.91858\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2842 - accuracy: 0.9193 - val_loss: 0.2933 - val_accuracy: 0.9183\n","Epoch 40/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.9194\n","Epoch 00040: val_accuracy did not improve from 0.91858\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2830 - accuracy: 0.9194 - val_loss: 0.2930 - val_accuracy: 0.9178\n","Epoch 41/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.9190\n","Epoch 00041: val_accuracy did not improve from 0.91858\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2819 - accuracy: 0.9190 - val_loss: 0.2928 - val_accuracy: 0.9180\n","Epoch 42/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2817 - accuracy: 0.9188\n","Epoch 00042: val_accuracy improved from 0.91858 to 0.91925, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2817 - accuracy: 0.9188 - val_loss: 0.2919 - val_accuracy: 0.9193\n","Epoch 43/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2807 - accuracy: 0.9198\n","Epoch 00043: val_accuracy did not improve from 0.91925\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2807 - accuracy: 0.9198 - val_loss: 0.2919 - val_accuracy: 0.9188\n","Epoch 44/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.9199\n","Epoch 00044: val_accuracy did not improve from 0.91925\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2791 - accuracy: 0.9199 - val_loss: 0.2924 - val_accuracy: 0.9186\n","Epoch 45/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.9211\n","Epoch 00045: val_accuracy did not improve from 0.91925\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2792 - accuracy: 0.9211 - val_loss: 0.2912 - val_accuracy: 0.9181\n","Epoch 46/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2781 - accuracy: 0.9206\n","Epoch 00046: val_accuracy did not improve from 0.91925\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2781 - accuracy: 0.9206 - val_loss: 0.2911 - val_accuracy: 0.9187\n","Epoch 47/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2787 - accuracy: 0.9215\n","Epoch 00047: val_accuracy did not improve from 0.91925\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2787 - accuracy: 0.9215 - val_loss: 0.2911 - val_accuracy: 0.9188\n","Epoch 48/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.9212\n","Epoch 00048: val_accuracy did not improve from 0.91925\n","188/188 [==============================] - 23s 125ms/step - loss: 0.2785 - accuracy: 0.9212 - val_loss: 0.2907 - val_accuracy: 0.9183\n","Epoch 49/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2766 - accuracy: 0.9214\n","Epoch 00049: val_accuracy did not improve from 0.91925\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2766 - accuracy: 0.9214 - val_loss: 0.2905 - val_accuracy: 0.9193\n","Epoch 50/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2777 - accuracy: 0.9209\n","Epoch 00050: val_accuracy improved from 0.91925 to 0.91992, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2777 - accuracy: 0.9209 - val_loss: 0.2897 - val_accuracy: 0.9199\n","Epoch 51/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.9220\n","Epoch 00051: val_accuracy improved from 0.91992 to 0.92017, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 125ms/step - loss: 0.2755 - accuracy: 0.9220 - val_loss: 0.2895 - val_accuracy: 0.9202\n","Epoch 52/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2762 - accuracy: 0.9217\n","Epoch 00052: val_accuracy did not improve from 0.92017\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2762 - accuracy: 0.9217 - val_loss: 0.2898 - val_accuracy: 0.9186\n","Epoch 53/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.9223\n","Epoch 00053: val_accuracy did not improve from 0.92017\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2750 - accuracy: 0.9223 - val_loss: 0.2887 - val_accuracy: 0.9191\n","Epoch 54/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2754 - accuracy: 0.9216\n","Epoch 00054: val_accuracy did not improve from 0.92017\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2754 - accuracy: 0.9216 - val_loss: 0.2889 - val_accuracy: 0.9188\n","Epoch 55/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2744 - accuracy: 0.9226\n","Epoch 00055: val_accuracy did not improve from 0.92017\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2744 - accuracy: 0.9226 - val_loss: 0.2886 - val_accuracy: 0.9194\n","Epoch 56/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2744 - accuracy: 0.9225\n","Epoch 00056: val_accuracy did not improve from 0.92017\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2744 - accuracy: 0.9225 - val_loss: 0.2897 - val_accuracy: 0.9183\n","Epoch 57/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2746 - accuracy: 0.9222\n","Epoch 00057: val_accuracy did not improve from 0.92017\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2746 - accuracy: 0.9222 - val_loss: 0.2879 - val_accuracy: 0.9193\n","Epoch 58/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2732 - accuracy: 0.9225\n","Epoch 00058: val_accuracy did not improve from 0.92017\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2732 - accuracy: 0.9225 - val_loss: 0.2882 - val_accuracy: 0.9202\n","Epoch 59/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.9229\n","Epoch 00059: val_accuracy did not improve from 0.92017\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2734 - accuracy: 0.9229 - val_loss: 0.2876 - val_accuracy: 0.9202\n","Epoch 60/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2722 - accuracy: 0.9235\n","Epoch 00060: val_accuracy did not improve from 0.92017\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2722 - accuracy: 0.9235 - val_loss: 0.2881 - val_accuracy: 0.9200\n","Epoch 61/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.9237\n","Epoch 00061: val_accuracy improved from 0.92017 to 0.92050, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2721 - accuracy: 0.9237 - val_loss: 0.2877 - val_accuracy: 0.9205\n","Epoch 62/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.9227\n","Epoch 00062: val_accuracy did not improve from 0.92050\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2721 - accuracy: 0.9227 - val_loss: 0.2876 - val_accuracy: 0.9200\n","Epoch 63/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.9227\n","Epoch 00063: val_accuracy did not improve from 0.92050\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2711 - accuracy: 0.9227 - val_loss: 0.2876 - val_accuracy: 0.9199\n","Epoch 64/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2716 - accuracy: 0.9232\n","Epoch 00064: val_accuracy did not improve from 0.92050\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2716 - accuracy: 0.9232 - val_loss: 0.2872 - val_accuracy: 0.9198\n","Epoch 65/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.9233\n","Epoch 00065: val_accuracy did not improve from 0.92050\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2710 - accuracy: 0.9233 - val_loss: 0.2873 - val_accuracy: 0.9196\n","Epoch 66/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.9228\n","Epoch 00066: val_accuracy improved from 0.92050 to 0.92058, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2696 - accuracy: 0.9228 - val_loss: 0.2872 - val_accuracy: 0.9206\n","Epoch 67/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2701 - accuracy: 0.9239\n","Epoch 00067: val_accuracy did not improve from 0.92058\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2701 - accuracy: 0.9239 - val_loss: 0.2870 - val_accuracy: 0.9191\n","Epoch 68/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.9244\n","Epoch 00068: val_accuracy did not improve from 0.92058\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2695 - accuracy: 0.9244 - val_loss: 0.2877 - val_accuracy: 0.9188\n","Epoch 69/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.9241\n","Epoch 00069: val_accuracy did not improve from 0.92058\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2692 - accuracy: 0.9241 - val_loss: 0.2866 - val_accuracy: 0.9204\n","Epoch 70/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2682 - accuracy: 0.9241\n","Epoch 00070: val_accuracy did not improve from 0.92058\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2682 - accuracy: 0.9241 - val_loss: 0.2877 - val_accuracy: 0.9199\n","Epoch 71/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.9239\n","Epoch 00071: val_accuracy did not improve from 0.92058\n","188/188 [==============================] - 26s 139ms/step - loss: 0.2685 - accuracy: 0.9239 - val_loss: 0.2873 - val_accuracy: 0.9196\n","Epoch 72/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2682 - accuracy: 0.9240\n","Epoch 00072: val_accuracy did not improve from 0.92058\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2682 - accuracy: 0.9240 - val_loss: 0.2868 - val_accuracy: 0.9193\n","Epoch 73/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2683 - accuracy: 0.9245\n","Epoch 00073: val_accuracy improved from 0.92058 to 0.92067, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2683 - accuracy: 0.9245 - val_loss: 0.2857 - val_accuracy: 0.9207\n","Epoch 74/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2672 - accuracy: 0.9244\n","Epoch 00074: val_accuracy improved from 0.92067 to 0.92108, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2672 - accuracy: 0.9244 - val_loss: 0.2862 - val_accuracy: 0.9211\n","Epoch 75/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.9242\n","Epoch 00075: val_accuracy did not improve from 0.92108\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2675 - accuracy: 0.9242 - val_loss: 0.2857 - val_accuracy: 0.9211\n","Epoch 76/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.9250\n","Epoch 00076: val_accuracy did not improve from 0.92108\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2665 - accuracy: 0.9250 - val_loss: 0.2857 - val_accuracy: 0.9207\n","Epoch 77/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.9251\n","Epoch 00077: val_accuracy did not improve from 0.92108\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2671 - accuracy: 0.9251 - val_loss: 0.2854 - val_accuracy: 0.9208\n","Epoch 78/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9250\n","Epoch 00078: val_accuracy improved from 0.92108 to 0.92125, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2662 - accuracy: 0.9250 - val_loss: 0.2859 - val_accuracy: 0.9212\n","Epoch 79/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.9253\n","Epoch 00079: val_accuracy did not improve from 0.92125\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2663 - accuracy: 0.9253 - val_loss: 0.2857 - val_accuracy: 0.9197\n","Epoch 80/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2660 - accuracy: 0.9253\n","Epoch 00080: val_accuracy did not improve from 0.92125\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2660 - accuracy: 0.9253 - val_loss: 0.2851 - val_accuracy: 0.9209\n","Epoch 81/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9245\n","Epoch 00081: val_accuracy did not improve from 0.92125\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2654 - accuracy: 0.9245 - val_loss: 0.2851 - val_accuracy: 0.9209\n","Epoch 82/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2643 - accuracy: 0.9251\n","Epoch 00082: val_accuracy did not improve from 0.92125\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2643 - accuracy: 0.9251 - val_loss: 0.2853 - val_accuracy: 0.9203\n","Epoch 83/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.9256\n","Epoch 00083: val_accuracy did not improve from 0.92125\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2645 - accuracy: 0.9256 - val_loss: 0.2848 - val_accuracy: 0.9208\n","Epoch 84/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9252\n","Epoch 00084: val_accuracy did not improve from 0.92125\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2648 - accuracy: 0.9252 - val_loss: 0.2848 - val_accuracy: 0.9212\n","Epoch 85/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2652 - accuracy: 0.9240\n","Epoch 00085: val_accuracy did not improve from 0.92125\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2652 - accuracy: 0.9240 - val_loss: 0.2856 - val_accuracy: 0.9208\n","Epoch 86/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.9250\n","Epoch 00086: val_accuracy improved from 0.92125 to 0.92133, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2649 - accuracy: 0.9250 - val_loss: 0.2855 - val_accuracy: 0.9213\n","Epoch 87/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9257\n","Epoch 00087: val_accuracy improved from 0.92133 to 0.92150, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2648 - accuracy: 0.9257 - val_loss: 0.2846 - val_accuracy: 0.9215\n","Epoch 88/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.9250\n","Epoch 00088: val_accuracy did not improve from 0.92150\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2635 - accuracy: 0.9250 - val_loss: 0.2854 - val_accuracy: 0.9213\n","Epoch 89/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.9248\n","Epoch 00089: val_accuracy improved from 0.92150 to 0.92158, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2638 - accuracy: 0.9248 - val_loss: 0.2847 - val_accuracy: 0.9216\n","Epoch 90/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.9251\n","Epoch 00090: val_accuracy did not improve from 0.92158\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2628 - accuracy: 0.9251 - val_loss: 0.2847 - val_accuracy: 0.9208\n","Epoch 91/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.9257\n","Epoch 00091: val_accuracy improved from 0.92158 to 0.92175, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2635 - accuracy: 0.9257 - val_loss: 0.2842 - val_accuracy: 0.9218\n","Epoch 92/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.9253\n","Epoch 00092: val_accuracy did not improve from 0.92175\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2621 - accuracy: 0.9253 - val_loss: 0.2842 - val_accuracy: 0.9218\n","Epoch 93/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9263\n","Epoch 00093: val_accuracy improved from 0.92175 to 0.92183, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2625 - accuracy: 0.9263 - val_loss: 0.2839 - val_accuracy: 0.9218\n","Epoch 94/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.9259\n","Epoch 00094: val_accuracy improved from 0.92183 to 0.92217, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2626 - accuracy: 0.9259 - val_loss: 0.2840 - val_accuracy: 0.9222\n","Epoch 95/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.9262\n","Epoch 00095: val_accuracy improved from 0.92217 to 0.92258, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2626 - accuracy: 0.9262 - val_loss: 0.2838 - val_accuracy: 0.9226\n","Epoch 96/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.9257\n","Epoch 00096: val_accuracy did not improve from 0.92258\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2614 - accuracy: 0.9257 - val_loss: 0.2842 - val_accuracy: 0.9217\n","Epoch 97/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.9260\n","Epoch 00097: val_accuracy did not improve from 0.92258\n","188/188 [==============================] - 26s 138ms/step - loss: 0.2617 - accuracy: 0.9260 - val_loss: 0.2837 - val_accuracy: 0.9221\n","Epoch 98/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2613 - accuracy: 0.9264\n","Epoch 00098: val_accuracy did not improve from 0.92258\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2613 - accuracy: 0.9264 - val_loss: 0.2839 - val_accuracy: 0.9219\n","Epoch 99/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.9253\n","Epoch 00099: val_accuracy improved from 0.92258 to 0.92300, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 121ms/step - loss: 0.2622 - accuracy: 0.9253 - val_loss: 0.2837 - val_accuracy: 0.9230\n","Epoch 100/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.9261\n","Epoch 00100: val_accuracy did not improve from 0.92300\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2614 - accuracy: 0.9261 - val_loss: 0.2849 - val_accuracy: 0.9211\n","Epoch 101/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.9257\n","Epoch 00101: val_accuracy improved from 0.92300 to 0.92308, saving model to mnist_hidden_do__best.h5\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2607 - accuracy: 0.9257 - val_loss: 0.2836 - val_accuracy: 0.9231\n","Epoch 102/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.9257\n","Epoch 00102: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2606 - accuracy: 0.9257 - val_loss: 0.2839 - val_accuracy: 0.9220\n","Epoch 103/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.9259\n","Epoch 00103: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2590 - accuracy: 0.9259 - val_loss: 0.2842 - val_accuracy: 0.9222\n","Epoch 104/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.9268\n","Epoch 00104: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2606 - accuracy: 0.9268 - val_loss: 0.2843 - val_accuracy: 0.9224\n","Epoch 105/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9264\n","Epoch 00105: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2615 - accuracy: 0.9264 - val_loss: 0.2843 - val_accuracy: 0.9226\n","Epoch 106/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.9258\n","Epoch 00106: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2606 - accuracy: 0.9258 - val_loss: 0.2836 - val_accuracy: 0.9221\n","Epoch 107/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9266\n","Epoch 00107: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2610 - accuracy: 0.9266 - val_loss: 0.2840 - val_accuracy: 0.9213\n","Epoch 108/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.9265\n","Epoch 00108: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2606 - accuracy: 0.9265 - val_loss: 0.2839 - val_accuracy: 0.9222\n","Epoch 109/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.9269\n","Epoch 00109: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 23s 123ms/step - loss: 0.2600 - accuracy: 0.9269 - val_loss: 0.2835 - val_accuracy: 0.9227\n","Epoch 110/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2583 - accuracy: 0.9259\n","Epoch 00110: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 23s 122ms/step - loss: 0.2583 - accuracy: 0.9259 - val_loss: 0.2837 - val_accuracy: 0.9219\n","Epoch 111/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.9264\n","Epoch 00111: val_accuracy did not improve from 0.92308\n","188/188 [==============================] - 23s 124ms/step - loss: 0.2592 - accuracy: 0.9264 - val_loss: 0.2840 - val_accuracy: 0.9222\n","Epoch 00111: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TYqDOl55s2Er","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"928db333-1c03-473e-d98c-22bb904ff1e6","executionInfo":{"status":"ok","timestamp":1590947526771,"user_tz":-60,"elapsed":2175,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["mnist_hidden_do_model.load_weights('mnist_hidden_do__best.h5')\n","loss, acc = mnist_hidden_do_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 1s 4ms/step - loss: 0.2658 - accuracy: 0.9253\n","Accuracy: 0.9253000020980835\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B6eEnizyNAkE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":444},"outputId":"fed786c8-c422-476e-c795-0b972f3834a0","executionInfo":{"status":"ok","timestamp":1590954113214,"user_tz":-60,"elapsed":1001,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_hidden_kr_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_hidden_kr_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_hidden_kr_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_hidden_kr_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":56,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"PbtnLAi1uP4U","colab_type":"text"},"source":["##### Combined L2 + dropout"]},{"cell_type":"code","metadata":{"id":"9qkI-LgzvWql","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":311},"outputId":"c3f36e3a-560d-4866-cb67-86c77459688b","executionInfo":{"status":"ok","timestamp":1590949394281,"user_tz":-60,"elapsed":601,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["mnist_hidden_dol2_model = tf.keras.Sequential(name='mnist_dol2_hidden')\n","mnist_hidden_dol2_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_hidden_dol2_model.add(tf.keras.layers.Dense(32, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.01), name='hidden'))\n","mnist_hidden_dol2_model.add(tf.keras.layers.Dropout(0.5, name='dropout'))\n","mnist_hidden_dol2_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_hidden_dol2_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_hidden_dol2_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_hidden_dol2_model.summary()"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Model: \"mnist_dol2_hidden\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","hidden (Dense)               (None, 28, 28, 32)        64        \n","_________________________________________________________________\n","dropout (Dropout)            (None, 28, 28, 32)        0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                250890    \n","=================================================================\n","Total params: 250,954\n","Trainable params: 250,954\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kM_AmjcSw4gk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"68e8190d-abb7-4be2-efc7-c217d3a8bd79","executionInfo":{"status":"ok","timestamp":1590950849752,"user_tz":-60,"elapsed":1453151,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_hidden_dol2_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_hidden_dol2_model_train = mnist_hidden_dol2_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","188/188 [==============================] - ETA: 0s - loss: 1.2997 - accuracy: 0.7371\n","Epoch 00001: val_accuracy improved from -inf to 0.84283, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 139ms/step - loss: 1.2997 - accuracy: 0.7371 - val_loss: 0.7723 - val_accuracy: 0.8428\n","Epoch 2/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.6492 - accuracy: 0.8530\n","Epoch 00002: val_accuracy improved from 0.84283 to 0.86867, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.6492 - accuracy: 0.8530 - val_loss: 0.5647 - val_accuracy: 0.8687\n","Epoch 3/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.5329 - accuracy: 0.8723\n","Epoch 00003: val_accuracy improved from 0.86867 to 0.88042, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.5329 - accuracy: 0.8723 - val_loss: 0.5002 - val_accuracy: 0.8804\n","Epoch 4/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4879 - accuracy: 0.8813\n","Epoch 00004: val_accuracy improved from 0.88042 to 0.88733, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 29s 153ms/step - loss: 0.4879 - accuracy: 0.8813 - val_loss: 0.4692 - val_accuracy: 0.8873\n","Epoch 5/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4628 - accuracy: 0.8863\n","Epoch 00005: val_accuracy improved from 0.88733 to 0.89242, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 25s 135ms/step - loss: 0.4628 - accuracy: 0.8863 - val_loss: 0.4506 - val_accuracy: 0.8924\n","Epoch 6/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4471 - accuracy: 0.8908\n","Epoch 00006: val_accuracy improved from 0.89242 to 0.89567, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.4471 - accuracy: 0.8908 - val_loss: 0.4374 - val_accuracy: 0.8957\n","Epoch 7/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4348 - accuracy: 0.8944\n","Epoch 00007: val_accuracy improved from 0.89567 to 0.89750, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.4348 - accuracy: 0.8944 - val_loss: 0.4281 - val_accuracy: 0.8975\n","Epoch 8/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4256 - accuracy: 0.8978\n","Epoch 00008: val_accuracy improved from 0.89750 to 0.89908, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.4256 - accuracy: 0.8978 - val_loss: 0.4204 - val_accuracy: 0.8991\n","Epoch 9/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4180 - accuracy: 0.8972\n","Epoch 00009: val_accuracy improved from 0.89908 to 0.90100, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 138ms/step - loss: 0.4180 - accuracy: 0.8972 - val_loss: 0.4136 - val_accuracy: 0.9010\n","Epoch 10/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.9000\n","Epoch 00010: val_accuracy did not improve from 0.90100\n","188/188 [==============================] - 26s 139ms/step - loss: 0.4128 - accuracy: 0.9000 - val_loss: 0.4085 - val_accuracy: 0.9010\n","Epoch 11/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4067 - accuracy: 0.9011\n","Epoch 00011: val_accuracy improved from 0.90100 to 0.90242, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.4067 - accuracy: 0.9011 - val_loss: 0.4038 - val_accuracy: 0.9024\n","Epoch 12/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4018 - accuracy: 0.9032\n","Epoch 00012: val_accuracy improved from 0.90242 to 0.90408, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.4018 - accuracy: 0.9032 - val_loss: 0.3999 - val_accuracy: 0.9041\n","Epoch 13/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3975 - accuracy: 0.9034\n","Epoch 00013: val_accuracy did not improve from 0.90408\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3975 - accuracy: 0.9034 - val_loss: 0.3962 - val_accuracy: 0.9038\n","Epoch 14/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.9042\n","Epoch 00014: val_accuracy improved from 0.90408 to 0.90583, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.3940 - accuracy: 0.9042 - val_loss: 0.3931 - val_accuracy: 0.9058\n","Epoch 15/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.9053\n","Epoch 00015: val_accuracy improved from 0.90583 to 0.90642, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 138ms/step - loss: 0.3896 - accuracy: 0.9053 - val_loss: 0.3899 - val_accuracy: 0.9064\n","Epoch 16/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.9055\n","Epoch 00016: val_accuracy improved from 0.90642 to 0.90725, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.3875 - accuracy: 0.9055 - val_loss: 0.3872 - val_accuracy: 0.9072\n","Epoch 17/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3834 - accuracy: 0.9072\n","Epoch 00017: val_accuracy improved from 0.90725 to 0.90750, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.3834 - accuracy: 0.9072 - val_loss: 0.3849 - val_accuracy: 0.9075\n","Epoch 18/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3821 - accuracy: 0.9067\n","Epoch 00018: val_accuracy improved from 0.90750 to 0.90800, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3821 - accuracy: 0.9067 - val_loss: 0.3825 - val_accuracy: 0.9080\n","Epoch 19/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.9078\n","Epoch 00019: val_accuracy improved from 0.90800 to 0.90858, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.3788 - accuracy: 0.9078 - val_loss: 0.3805 - val_accuracy: 0.9086\n","Epoch 20/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3762 - accuracy: 0.9079\n","Epoch 00020: val_accuracy did not improve from 0.90858\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3762 - accuracy: 0.9079 - val_loss: 0.3785 - val_accuracy: 0.9084\n","Epoch 21/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3734 - accuracy: 0.9088\n","Epoch 00021: val_accuracy improved from 0.90858 to 0.90958, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.3734 - accuracy: 0.9088 - val_loss: 0.3764 - val_accuracy: 0.9096\n","Epoch 22/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3721 - accuracy: 0.9101\n","Epoch 00022: val_accuracy improved from 0.90958 to 0.91025, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.3721 - accuracy: 0.9101 - val_loss: 0.3747 - val_accuracy: 0.9103\n","Epoch 23/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.9093\n","Epoch 00023: val_accuracy improved from 0.91025 to 0.91058, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3698 - accuracy: 0.9093 - val_loss: 0.3732 - val_accuracy: 0.9106\n","Epoch 24/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.9100\n","Epoch 00024: val_accuracy improved from 0.91058 to 0.91083, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 25s 135ms/step - loss: 0.3679 - accuracy: 0.9100 - val_loss: 0.3713 - val_accuracy: 0.9108\n","Epoch 25/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3669 - accuracy: 0.9107\n","Epoch 00025: val_accuracy did not improve from 0.91083\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3669 - accuracy: 0.9107 - val_loss: 0.3698 - val_accuracy: 0.9099\n","Epoch 26/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3643 - accuracy: 0.9105\n","Epoch 00026: val_accuracy improved from 0.91083 to 0.91100, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3643 - accuracy: 0.9105 - val_loss: 0.3684 - val_accuracy: 0.9110\n","Epoch 27/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3625 - accuracy: 0.9118\n","Epoch 00027: val_accuracy did not improve from 0.91100\n","188/188 [==============================] - 25s 135ms/step - loss: 0.3625 - accuracy: 0.9118 - val_loss: 0.3677 - val_accuracy: 0.9110\n","Epoch 28/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3614 - accuracy: 0.9106\n","Epoch 00028: val_accuracy improved from 0.91100 to 0.91125, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 28s 151ms/step - loss: 0.3614 - accuracy: 0.9106 - val_loss: 0.3660 - val_accuracy: 0.9112\n","Epoch 29/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3596 - accuracy: 0.9112\n","Epoch 00029: val_accuracy improved from 0.91125 to 0.91167, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 25s 136ms/step - loss: 0.3596 - accuracy: 0.9112 - val_loss: 0.3643 - val_accuracy: 0.9117\n","Epoch 30/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3583 - accuracy: 0.9126\n","Epoch 00030: val_accuracy did not improve from 0.91167\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3583 - accuracy: 0.9126 - val_loss: 0.3638 - val_accuracy: 0.9117\n","Epoch 31/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3576 - accuracy: 0.9129\n","Epoch 00031: val_accuracy did not improve from 0.91167\n","188/188 [==============================] - 26s 137ms/step - loss: 0.3576 - accuracy: 0.9129 - val_loss: 0.3624 - val_accuracy: 0.9117\n","Epoch 32/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.9124\n","Epoch 00032: val_accuracy improved from 0.91167 to 0.91317, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3563 - accuracy: 0.9124 - val_loss: 0.3614 - val_accuracy: 0.9132\n","Epoch 33/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3550 - accuracy: 0.9125\n","Epoch 00033: val_accuracy did not improve from 0.91317\n","188/188 [==============================] - 26s 137ms/step - loss: 0.3550 - accuracy: 0.9125 - val_loss: 0.3607 - val_accuracy: 0.9124\n","Epoch 34/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.9134\n","Epoch 00034: val_accuracy did not improve from 0.91317\n","188/188 [==============================] - 26s 138ms/step - loss: 0.3531 - accuracy: 0.9134 - val_loss: 0.3593 - val_accuracy: 0.9130\n","Epoch 35/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3515 - accuracy: 0.9143\n","Epoch 00035: val_accuracy did not improve from 0.91317\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3515 - accuracy: 0.9143 - val_loss: 0.3583 - val_accuracy: 0.9128\n","Epoch 36/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.9136\n","Epoch 00036: val_accuracy did not improve from 0.91317\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3509 - accuracy: 0.9136 - val_loss: 0.3573 - val_accuracy: 0.9132\n","Epoch 37/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3507 - accuracy: 0.9135\n","Epoch 00037: val_accuracy improved from 0.91317 to 0.91358, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3507 - accuracy: 0.9135 - val_loss: 0.3566 - val_accuracy: 0.9136\n","Epoch 38/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3488 - accuracy: 0.9138\n","Epoch 00038: val_accuracy did not improve from 0.91358\n","188/188 [==============================] - 25s 136ms/step - loss: 0.3488 - accuracy: 0.9138 - val_loss: 0.3558 - val_accuracy: 0.9134\n","Epoch 39/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3480 - accuracy: 0.9138\n","Epoch 00039: val_accuracy did not improve from 0.91358\n","188/188 [==============================] - 25s 135ms/step - loss: 0.3480 - accuracy: 0.9138 - val_loss: 0.3548 - val_accuracy: 0.9128\n","Epoch 40/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.9141\n","Epoch 00040: val_accuracy did not improve from 0.91358\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3478 - accuracy: 0.9141 - val_loss: 0.3540 - val_accuracy: 0.9133\n","Epoch 41/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.9142\n","Epoch 00041: val_accuracy improved from 0.91358 to 0.91433, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3468 - accuracy: 0.9142 - val_loss: 0.3531 - val_accuracy: 0.9143\n","Epoch 42/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3456 - accuracy: 0.9137\n","Epoch 00042: val_accuracy did not improve from 0.91433\n","188/188 [==============================] - 27s 142ms/step - loss: 0.3456 - accuracy: 0.9137 - val_loss: 0.3529 - val_accuracy: 0.9138\n","Epoch 43/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3446 - accuracy: 0.9144\n","Epoch 00043: val_accuracy did not improve from 0.91433\n","188/188 [==============================] - 25s 135ms/step - loss: 0.3446 - accuracy: 0.9144 - val_loss: 0.3517 - val_accuracy: 0.9142\n","Epoch 44/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3426 - accuracy: 0.9152\n","Epoch 00044: val_accuracy did not improve from 0.91433\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3426 - accuracy: 0.9152 - val_loss: 0.3517 - val_accuracy: 0.9143\n","Epoch 45/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3429 - accuracy: 0.9152\n","Epoch 00045: val_accuracy did not improve from 0.91433\n","188/188 [==============================] - 26s 137ms/step - loss: 0.3429 - accuracy: 0.9152 - val_loss: 0.3508 - val_accuracy: 0.9143\n","Epoch 46/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3424 - accuracy: 0.9152\n","Epoch 00046: val_accuracy improved from 0.91433 to 0.91567, saving model to mnist_hidden_dol2_best.h5\n","188/188 [==============================] - 26s 137ms/step - loss: 0.3424 - accuracy: 0.9152 - val_loss: 0.3498 - val_accuracy: 0.9157\n","Epoch 47/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3407 - accuracy: 0.9154\n","Epoch 00047: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3407 - accuracy: 0.9154 - val_loss: 0.3495 - val_accuracy: 0.9144\n","Epoch 48/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.9152\n","Epoch 00048: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3410 - accuracy: 0.9152 - val_loss: 0.3489 - val_accuracy: 0.9143\n","Epoch 49/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3394 - accuracy: 0.9155\n","Epoch 00049: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3394 - accuracy: 0.9155 - val_loss: 0.3478 - val_accuracy: 0.9151\n","Epoch 50/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 0.9157\n","Epoch 00050: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 26s 137ms/step - loss: 0.3384 - accuracy: 0.9157 - val_loss: 0.3476 - val_accuracy: 0.9142\n","Epoch 51/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3390 - accuracy: 0.9158\n","Epoch 00051: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 28s 151ms/step - loss: 0.3390 - accuracy: 0.9158 - val_loss: 0.3473 - val_accuracy: 0.9153\n","Epoch 52/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.9158\n","Epoch 00052: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 25s 135ms/step - loss: 0.3387 - accuracy: 0.9158 - val_loss: 0.3462 - val_accuracy: 0.9138\n","Epoch 53/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.9163\n","Epoch 00053: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3369 - accuracy: 0.9163 - val_loss: 0.3456 - val_accuracy: 0.9147\n","Epoch 54/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.9158\n","Epoch 00054: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3375 - accuracy: 0.9158 - val_loss: 0.3452 - val_accuracy: 0.9153\n","Epoch 55/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3356 - accuracy: 0.9162\n","Epoch 00055: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 26s 137ms/step - loss: 0.3356 - accuracy: 0.9162 - val_loss: 0.3447 - val_accuracy: 0.9152\n","Epoch 56/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.9167\n","Epoch 00056: val_accuracy did not improve from 0.91567\n","188/188 [==============================] - 26s 136ms/step - loss: 0.3348 - accuracy: 0.9167 - val_loss: 0.3440 - val_accuracy: 0.9156\n","Epoch 00056: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kPVw85TixMEa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"7413a75f-f1c0-433f-a6f4-576de3255c17","executionInfo":{"status":"ok","timestamp":1590950914323,"user_tz":-60,"elapsed":1996,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["mnist_hidden_dol2_model.load_weights('mnist_dol2_best.h5')\n","loss, acc = mnist_hidden_dol2_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":37,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 1s 4ms/step - loss: 0.3177 - accuracy: 0.9207\n","Accuracy: 0.9207000136375427\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_-Ib4s7zND79","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":444},"outputId":"ba47806a-b07b-44a7-8255-d9e1000eaaee","executionInfo":{"status":"ok","timestamp":1590954123080,"user_tz":-60,"elapsed":1427,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_hidden_do_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_hidden_do_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_hidden_do_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_hidden_do_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":57,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"13tVOFWonpZa","colab_type":"text"},"source":["## Convolutional Neural Networks\n","\n","Convolutional layers capture patterns corresponding to relevant features independently of where they occur in the input. To do so, they slide a window over the input and apply the convolution operation with a set of kernels or filters that represent the features. Although it is not their only field of application, convolutional neural networks are mainly praised for their performance on image processing tasks. "]},{"cell_type":"markdown","metadata":{"id":"mNIvLULvBopW","colab_type":"text"},"source":["### Models\n","\n","Now, let's create and train some networks to approach the problem posed by the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset."]},{"cell_type":"markdown","metadata":{"id":"SRphQLawFxAr","colab_type":"text"},"source":["#### Baseline\n","\n","To create our CNN, instead of feeding the flatenned output directly to the output layer, we will first pass it through a convolutional layer followed by a max pooling operation. Since, we are dealing with 2D data, we will use the [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers. \n","\n","In the convolutional layer, the *filters* parameter defines the number of kernels or filters used in the layer. The *kernel_size* parameter defines the size of the kernels. If only one number is provided, the kernel is assumed to be square. The stride values default to one, but can be changed using the *strides* parameter. Also, we can use same padding, by setting the *padding* parameter to 'same'.\n","\n","For the pooling operation, we define the size of the pooling window using the *pool_size* parameter. Similarly to the *kernel_size* parameter of the convolutional layer, if only one number is provided, the window is assumed to be square. Additionally, the *strides* parameter defaults to the size of the pooling window. That is, there is no overlap."]},{"cell_type":"code","metadata":{"id":"HsMkdg9bDvnu","colab_type":"code","outputId":"b1f0fedf-5f8b-4909-d98f-71c2188b99a7","executionInfo":{"status":"ok","timestamp":1590603069444,"user_tz":-60,"elapsed":186769,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["mnist_conv_model = tf.keras.Sequential(name='mnist_cnn')\n","mnist_conv_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution'))\n","mnist_conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling'))\n","mnist_conv_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_conv_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_conv_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_conv_model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"mnist_cnn\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","convolution (Conv2D)         (None, 28, 28, 16)        272       \n","_________________________________________________________________\n","pooling (MaxPooling2D)       (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 3136)              0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                31370     \n","=================================================================\n","Total params: 31,642\n","Trainable params: 31,642\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7W96_w63OL_i","colab_type":"text"},"source":["Let's train the model using the same approach as before:"]},{"cell_type":"code","metadata":{"id":"eVfO70uCE1hq","colab_type":"code","outputId":"084dbfd1-e239-45f6-8dc0-6f0f55beebc5","executionInfo":{"status":"ok","timestamp":1590606638346,"user_tz":-60,"elapsed":3755666,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_conv_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_conv_model_train = mnist_conv_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","187/188 [============================>.] - ETA: 0s - loss: 1.5820 - accuracy: 0.6042\n","Epoch 00001: val_accuracy improved from -inf to 0.81442, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 1.5801 - accuracy: 0.6048 - val_loss: 0.7909 - val_accuracy: 0.8144\n","Epoch 2/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.5848 - accuracy: 0.8475\n","Epoch 00002: val_accuracy improved from 0.81442 to 0.87042, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.5843 - accuracy: 0.8475 - val_loss: 0.4752 - val_accuracy: 0.8704\n","Epoch 3/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4354 - accuracy: 0.8767\n","Epoch 00003: val_accuracy improved from 0.87042 to 0.88367, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.4354 - accuracy: 0.8768 - val_loss: 0.4063 - val_accuracy: 0.8837\n","Epoch 4/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3892 - accuracy: 0.8877\n","Epoch 00004: val_accuracy improved from 0.88367 to 0.88958, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3894 - accuracy: 0.8876 - val_loss: 0.3770 - val_accuracy: 0.8896\n","Epoch 5/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3656 - accuracy: 0.8938\n","Epoch 00005: val_accuracy improved from 0.88958 to 0.89800, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3657 - accuracy: 0.8938 - val_loss: 0.3598 - val_accuracy: 0.8980\n","Epoch 6/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3510 - accuracy: 0.8978\n","Epoch 00006: val_accuracy improved from 0.89800 to 0.90000, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3507 - accuracy: 0.8979 - val_loss: 0.3474 - val_accuracy: 0.9000\n","Epoch 7/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3396 - accuracy: 0.9011\n","Epoch 00007: val_accuracy improved from 0.90000 to 0.90108, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.3396 - accuracy: 0.9010 - val_loss: 0.3400 - val_accuracy: 0.9011\n","Epoch 8/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3313 - accuracy: 0.9034\n","Epoch 00008: val_accuracy improved from 0.90108 to 0.90517, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3314 - accuracy: 0.9034 - val_loss: 0.3321 - val_accuracy: 0.9052\n","Epoch 9/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.9057\n","Epoch 00009: val_accuracy improved from 0.90517 to 0.90575, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3244 - accuracy: 0.9057 - val_loss: 0.3272 - val_accuracy: 0.9057\n","Epoch 10/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.9077\n","Epoch 00010: val_accuracy improved from 0.90575 to 0.90883, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.3184 - accuracy: 0.9076 - val_loss: 0.3212 - val_accuracy: 0.9088\n","Epoch 11/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3134 - accuracy: 0.9088\n","Epoch 00011: val_accuracy improved from 0.90883 to 0.90958, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3130 - accuracy: 0.9089 - val_loss: 0.3169 - val_accuracy: 0.9096\n","Epoch 12/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3083 - accuracy: 0.9107\n","Epoch 00012: val_accuracy improved from 0.90958 to 0.91067, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3084 - accuracy: 0.9107 - val_loss: 0.3139 - val_accuracy: 0.9107\n","Epoch 13/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3033 - accuracy: 0.9123\n","Epoch 00013: val_accuracy improved from 0.91067 to 0.91208, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3033 - accuracy: 0.9123 - val_loss: 0.3121 - val_accuracy: 0.9121\n","Epoch 14/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2996 - accuracy: 0.9133\n","Epoch 00014: val_accuracy did not improve from 0.91208\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2997 - accuracy: 0.9133 - val_loss: 0.3119 - val_accuracy: 0.9111\n","Epoch 15/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.9139\n","Epoch 00015: val_accuracy improved from 0.91208 to 0.91458, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2957 - accuracy: 0.9139 - val_loss: 0.3022 - val_accuracy: 0.9146\n","Epoch 16/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9155\n","Epoch 00016: val_accuracy improved from 0.91458 to 0.91633, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2915 - accuracy: 0.9154 - val_loss: 0.2987 - val_accuracy: 0.9163\n","Epoch 17/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9163\n","Epoch 00017: val_accuracy did not improve from 0.91633\n","188/188 [==============================] - 17s 91ms/step - loss: 0.2880 - accuracy: 0.9164 - val_loss: 0.2956 - val_accuracy: 0.9156\n","Epoch 18/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.9175\n","Epoch 00018: val_accuracy improved from 0.91633 to 0.91725, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2841 - accuracy: 0.9175 - val_loss: 0.2917 - val_accuracy: 0.9172\n","Epoch 19/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2802 - accuracy: 0.9186\n","Epoch 00019: val_accuracy improved from 0.91725 to 0.91817, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2806 - accuracy: 0.9184 - val_loss: 0.2893 - val_accuracy: 0.9182\n","Epoch 20/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2766 - accuracy: 0.9193\n","Epoch 00020: val_accuracy improved from 0.91817 to 0.92100, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2770 - accuracy: 0.9193 - val_loss: 0.2839 - val_accuracy: 0.9210\n","Epoch 21/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2737 - accuracy: 0.9205\n","Epoch 00021: val_accuracy improved from 0.92100 to 0.92167, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2736 - accuracy: 0.9204 - val_loss: 0.2840 - val_accuracy: 0.9217\n","Epoch 22/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2703 - accuracy: 0.9219\n","Epoch 00022: val_accuracy did not improve from 0.92167\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2701 - accuracy: 0.9219 - val_loss: 0.2801 - val_accuracy: 0.9210\n","Epoch 23/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2666 - accuracy: 0.9233\n","Epoch 00023: val_accuracy improved from 0.92167 to 0.92233, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2663 - accuracy: 0.9234 - val_loss: 0.2761 - val_accuracy: 0.9223\n","Epoch 24/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2630 - accuracy: 0.9238\n","Epoch 00024: val_accuracy improved from 0.92233 to 0.92483, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2629 - accuracy: 0.9238 - val_loss: 0.2711 - val_accuracy: 0.9248\n","Epoch 25/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2588 - accuracy: 0.9250\n","Epoch 00025: val_accuracy did not improve from 0.92483\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2589 - accuracy: 0.9249 - val_loss: 0.2698 - val_accuracy: 0.9243\n","Epoch 26/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2557 - accuracy: 0.9270\n","Epoch 00026: val_accuracy improved from 0.92483 to 0.92683, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2555 - accuracy: 0.9271 - val_loss: 0.2632 - val_accuracy: 0.9268\n","Epoch 27/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2516 - accuracy: 0.9276\n","Epoch 00027: val_accuracy improved from 0.92683 to 0.92717, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2516 - accuracy: 0.9276 - val_loss: 0.2619 - val_accuracy: 0.9272\n","Epoch 28/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2482 - accuracy: 0.9286\n","Epoch 00028: val_accuracy improved from 0.92717 to 0.92783, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.2481 - accuracy: 0.9286 - val_loss: 0.2572 - val_accuracy: 0.9278\n","Epoch 29/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.9300\n","Epoch 00029: val_accuracy improved from 0.92783 to 0.93000, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2443 - accuracy: 0.9300 - val_loss: 0.2532 - val_accuracy: 0.9300\n","Epoch 30/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2403 - accuracy: 0.9311\n","Epoch 00030: val_accuracy improved from 0.93000 to 0.93058, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2402 - accuracy: 0.9312 - val_loss: 0.2492 - val_accuracy: 0.9306\n","Epoch 31/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2363 - accuracy: 0.9317\n","Epoch 00031: val_accuracy improved from 0.93058 to 0.93200, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2363 - accuracy: 0.9317 - val_loss: 0.2456 - val_accuracy: 0.9320\n","Epoch 32/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2325 - accuracy: 0.9338\n","Epoch 00032: val_accuracy improved from 0.93200 to 0.93275, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.2324 - accuracy: 0.9338 - val_loss: 0.2421 - val_accuracy: 0.9327\n","Epoch 33/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2281 - accuracy: 0.9351\n","Epoch 00033: val_accuracy improved from 0.93275 to 0.93333, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2280 - accuracy: 0.9351 - val_loss: 0.2383 - val_accuracy: 0.9333\n","Epoch 34/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.9360\n","Epoch 00034: val_accuracy improved from 0.93333 to 0.93475, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2240 - accuracy: 0.9360 - val_loss: 0.2347 - val_accuracy: 0.9348\n","Epoch 35/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2195 - accuracy: 0.9378\n","Epoch 00035: val_accuracy improved from 0.93475 to 0.93692, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.2199 - accuracy: 0.9378 - val_loss: 0.2318 - val_accuracy: 0.9369\n","Epoch 36/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2162 - accuracy: 0.9385\n","Epoch 00036: val_accuracy improved from 0.93692 to 0.93783, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2161 - accuracy: 0.9385 - val_loss: 0.2264 - val_accuracy: 0.9378\n","Epoch 37/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2119 - accuracy: 0.9400\n","Epoch 00037: val_accuracy did not improve from 0.93783\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2118 - accuracy: 0.9400 - val_loss: 0.2259 - val_accuracy: 0.9369\n","Epoch 38/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2080 - accuracy: 0.9412\n","Epoch 00038: val_accuracy improved from 0.93783 to 0.94000, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2078 - accuracy: 0.9412 - val_loss: 0.2188 - val_accuracy: 0.9400\n","Epoch 39/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2038 - accuracy: 0.9421\n","Epoch 00039: val_accuracy improved from 0.94000 to 0.94200, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.2038 - accuracy: 0.9421 - val_loss: 0.2148 - val_accuracy: 0.9420\n","Epoch 40/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1999 - accuracy: 0.9428\n","Epoch 00040: val_accuracy improved from 0.94200 to 0.94292, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1998 - accuracy: 0.9429 - val_loss: 0.2098 - val_accuracy: 0.9429\n","Epoch 41/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1958 - accuracy: 0.9447\n","Epoch 00041: val_accuracy improved from 0.94292 to 0.94442, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1957 - accuracy: 0.9448 - val_loss: 0.2054 - val_accuracy: 0.9444\n","Epoch 42/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1920 - accuracy: 0.9454\n","Epoch 00042: val_accuracy improved from 0.94442 to 0.94475, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1919 - accuracy: 0.9454 - val_loss: 0.2040 - val_accuracy: 0.9448\n","Epoch 43/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1881 - accuracy: 0.9468\n","Epoch 00043: val_accuracy improved from 0.94475 to 0.94692, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1880 - accuracy: 0.9468 - val_loss: 0.1976 - val_accuracy: 0.9469\n","Epoch 44/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1844 - accuracy: 0.9477\n","Epoch 00044: val_accuracy improved from 0.94692 to 0.94750, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1842 - accuracy: 0.9477 - val_loss: 0.1945 - val_accuracy: 0.9475\n","Epoch 45/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1809 - accuracy: 0.9485\n","Epoch 00045: val_accuracy improved from 0.94750 to 0.94842, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1807 - accuracy: 0.9485 - val_loss: 0.1907 - val_accuracy: 0.9484\n","Epoch 46/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9503\n","Epoch 00046: val_accuracy improved from 0.94842 to 0.94917, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1769 - accuracy: 0.9504 - val_loss: 0.1870 - val_accuracy: 0.9492\n","Epoch 47/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.9513\n","Epoch 00047: val_accuracy improved from 0.94917 to 0.94925, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1733 - accuracy: 0.9513 - val_loss: 0.1865 - val_accuracy: 0.9492\n","Epoch 48/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1696 - accuracy: 0.9522\n","Epoch 00048: val_accuracy improved from 0.94925 to 0.95217, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1699 - accuracy: 0.9522 - val_loss: 0.1828 - val_accuracy: 0.9522\n","Epoch 49/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1667 - accuracy: 0.9530\n","Epoch 00049: val_accuracy did not improve from 0.95217\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1666 - accuracy: 0.9530 - val_loss: 0.1779 - val_accuracy: 0.9514\n","Epoch 50/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1637 - accuracy: 0.9540\n","Epoch 00050: val_accuracy improved from 0.95217 to 0.95442, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1638 - accuracy: 0.9539 - val_loss: 0.1734 - val_accuracy: 0.9544\n","Epoch 51/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1602 - accuracy: 0.9552\n","Epoch 00051: val_accuracy did not improve from 0.95442\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1602 - accuracy: 0.9553 - val_loss: 0.1720 - val_accuracy: 0.9543\n","Epoch 52/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1573 - accuracy: 0.9557\n","Epoch 00052: val_accuracy improved from 0.95442 to 0.95517, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1574 - accuracy: 0.9557 - val_loss: 0.1675 - val_accuracy: 0.9552\n","Epoch 53/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9565\n","Epoch 00053: val_accuracy did not improve from 0.95517\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1545 - accuracy: 0.9564 - val_loss: 0.1680 - val_accuracy: 0.9548\n","Epoch 54/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1517 - accuracy: 0.9573\n","Epoch 00054: val_accuracy improved from 0.95517 to 0.95667, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1517 - accuracy: 0.9573 - val_loss: 0.1632 - val_accuracy: 0.9567\n","Epoch 55/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9583\n","Epoch 00055: val_accuracy improved from 0.95667 to 0.95717, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1490 - accuracy: 0.9584 - val_loss: 0.1594 - val_accuracy: 0.9572\n","Epoch 56/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1464 - accuracy: 0.9588\n","Epoch 00056: val_accuracy improved from 0.95717 to 0.95817, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1464 - accuracy: 0.9588 - val_loss: 0.1574 - val_accuracy: 0.9582\n","Epoch 57/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1441 - accuracy: 0.9596\n","Epoch 00057: val_accuracy improved from 0.95817 to 0.95867, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1439 - accuracy: 0.9596 - val_loss: 0.1546 - val_accuracy: 0.9587\n","Epoch 58/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.9608\n","Epoch 00058: val_accuracy improved from 0.95867 to 0.95950, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1415 - accuracy: 0.9608 - val_loss: 0.1520 - val_accuracy: 0.9595\n","Epoch 59/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.9614\n","Epoch 00059: val_accuracy did not improve from 0.95950\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1389 - accuracy: 0.9615 - val_loss: 0.1497 - val_accuracy: 0.9592\n","Epoch 60/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.9618\n","Epoch 00060: val_accuracy improved from 0.95950 to 0.96058, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1368 - accuracy: 0.9617 - val_loss: 0.1474 - val_accuracy: 0.9606\n","Epoch 61/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1345 - accuracy: 0.9624\n","Epoch 00061: val_accuracy improved from 0.96058 to 0.96142, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1346 - accuracy: 0.9623 - val_loss: 0.1455 - val_accuracy: 0.9614\n","Epoch 62/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9632\n","Epoch 00062: val_accuracy improved from 0.96142 to 0.96150, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1324 - accuracy: 0.9632 - val_loss: 0.1447 - val_accuracy: 0.9615\n","Epoch 63/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.9640\n","Epoch 00063: val_accuracy improved from 0.96150 to 0.96217, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1305 - accuracy: 0.9641 - val_loss: 0.1410 - val_accuracy: 0.9622\n","Epoch 64/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9646\n","Epoch 00064: val_accuracy did not improve from 0.96217\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1284 - accuracy: 0.9646 - val_loss: 0.1437 - val_accuracy: 0.9613\n","Epoch 65/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9651\n","Epoch 00065: val_accuracy improved from 0.96217 to 0.96250, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1266 - accuracy: 0.9651 - val_loss: 0.1371 - val_accuracy: 0.9625\n","Epoch 66/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9654\n","Epoch 00066: val_accuracy improved from 0.96250 to 0.96367, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1247 - accuracy: 0.9654 - val_loss: 0.1358 - val_accuracy: 0.9637\n","Epoch 67/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9664\n","Epoch 00067: val_accuracy improved from 0.96367 to 0.96375, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1229 - accuracy: 0.9664 - val_loss: 0.1342 - val_accuracy: 0.9638\n","Epoch 68/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9665\n","Epoch 00068: val_accuracy improved from 0.96375 to 0.96442, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1212 - accuracy: 0.9666 - val_loss: 0.1326 - val_accuracy: 0.9644\n","Epoch 69/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9672\n","Epoch 00069: val_accuracy did not improve from 0.96442\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1195 - accuracy: 0.9672 - val_loss: 0.1305 - val_accuracy: 0.9643\n","Epoch 70/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9677\n","Epoch 00070: val_accuracy improved from 0.96442 to 0.96492, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1181 - accuracy: 0.9677 - val_loss: 0.1288 - val_accuracy: 0.9649\n","Epoch 71/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9681\n","Epoch 00071: val_accuracy did not improve from 0.96492\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1163 - accuracy: 0.9682 - val_loss: 0.1294 - val_accuracy: 0.9646\n","Epoch 72/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9685\n","Epoch 00072: val_accuracy did not improve from 0.96492\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1149 - accuracy: 0.9684 - val_loss: 0.1267 - val_accuracy: 0.9649\n","Epoch 73/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1135 - accuracy: 0.9694\n","Epoch 00073: val_accuracy improved from 0.96492 to 0.96700, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1135 - accuracy: 0.9694 - val_loss: 0.1251 - val_accuracy: 0.9670\n","Epoch 74/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9699\n","Epoch 00074: val_accuracy did not improve from 0.96700\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1119 - accuracy: 0.9699 - val_loss: 0.1231 - val_accuracy: 0.9662\n","Epoch 75/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1107 - accuracy: 0.9698\n","Epoch 00075: val_accuracy did not improve from 0.96700\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1106 - accuracy: 0.9698 - val_loss: 0.1225 - val_accuracy: 0.9663\n","Epoch 76/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1091 - accuracy: 0.9703\n","Epoch 00076: val_accuracy did not improve from 0.96700\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1093 - accuracy: 0.9702 - val_loss: 0.1212 - val_accuracy: 0.9663\n","Epoch 77/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1080 - accuracy: 0.9709\n","Epoch 00077: val_accuracy improved from 0.96700 to 0.96742, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1080 - accuracy: 0.9710 - val_loss: 0.1203 - val_accuracy: 0.9674\n","Epoch 78/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9708\n","Epoch 00078: val_accuracy improved from 0.96742 to 0.96767, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1068 - accuracy: 0.9709 - val_loss: 0.1184 - val_accuracy: 0.9677\n","Epoch 79/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1056 - accuracy: 0.9713\n","Epoch 00079: val_accuracy improved from 0.96767 to 0.96783, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1055 - accuracy: 0.9712 - val_loss: 0.1167 - val_accuracy: 0.9678\n","Epoch 80/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1043 - accuracy: 0.9716\n","Epoch 00080: val_accuracy did not improve from 0.96783\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1044 - accuracy: 0.9716 - val_loss: 0.1165 - val_accuracy: 0.9672\n","Epoch 81/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1033 - accuracy: 0.9724\n","Epoch 00081: val_accuracy improved from 0.96783 to 0.96833, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1033 - accuracy: 0.9724 - val_loss: 0.1144 - val_accuracy: 0.9683\n","Epoch 82/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9724\n","Epoch 00082: val_accuracy did not improve from 0.96833\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1022 - accuracy: 0.9725 - val_loss: 0.1139 - val_accuracy: 0.9683\n","Epoch 83/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9730\n","Epoch 00083: val_accuracy did not improve from 0.96833\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1010 - accuracy: 0.9730 - val_loss: 0.1135 - val_accuracy: 0.9682\n","Epoch 84/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0999 - accuracy: 0.9733\n","Epoch 00084: val_accuracy improved from 0.96833 to 0.96867, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1000 - accuracy: 0.9733 - val_loss: 0.1125 - val_accuracy: 0.9687\n","Epoch 85/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0989 - accuracy: 0.9732\n","Epoch 00085: val_accuracy improved from 0.96867 to 0.96883, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0990 - accuracy: 0.9732 - val_loss: 0.1120 - val_accuracy: 0.9688\n","Epoch 86/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0980 - accuracy: 0.9737\n","Epoch 00086: val_accuracy improved from 0.96883 to 0.96917, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0979 - accuracy: 0.9737 - val_loss: 0.1099 - val_accuracy: 0.9692\n","Epoch 87/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9738\n","Epoch 00087: val_accuracy improved from 0.96917 to 0.96958, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0970 - accuracy: 0.9738 - val_loss: 0.1087 - val_accuracy: 0.9696\n","Epoch 88/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9744\n","Epoch 00088: val_accuracy improved from 0.96958 to 0.96967, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0961 - accuracy: 0.9744 - val_loss: 0.1085 - val_accuracy: 0.9697\n","Epoch 89/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0951 - accuracy: 0.9745\n","Epoch 00089: val_accuracy did not improve from 0.96967\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0950 - accuracy: 0.9746 - val_loss: 0.1079 - val_accuracy: 0.9697\n","Epoch 90/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9744\n","Epoch 00090: val_accuracy did not improve from 0.96967\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0943 - accuracy: 0.9744 - val_loss: 0.1066 - val_accuracy: 0.9694\n","Epoch 91/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9749\n","Epoch 00091: val_accuracy improved from 0.96967 to 0.97033, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0933 - accuracy: 0.9749 - val_loss: 0.1057 - val_accuracy: 0.9703\n","Epoch 92/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0925 - accuracy: 0.9755\n","Epoch 00092: val_accuracy improved from 0.97033 to 0.97083, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0925 - accuracy: 0.9754 - val_loss: 0.1055 - val_accuracy: 0.9708\n","Epoch 93/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0918 - accuracy: 0.9753\n","Epoch 00093: val_accuracy did not improve from 0.97083\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0917 - accuracy: 0.9753 - val_loss: 0.1045 - val_accuracy: 0.9701\n","Epoch 94/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0908 - accuracy: 0.9758\n","Epoch 00094: val_accuracy improved from 0.97083 to 0.97142, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0908 - accuracy: 0.9758 - val_loss: 0.1032 - val_accuracy: 0.9714\n","Epoch 95/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0900 - accuracy: 0.9758\n","Epoch 00095: val_accuracy did not improve from 0.97142\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0901 - accuracy: 0.9758 - val_loss: 0.1023 - val_accuracy: 0.9710\n","Epoch 96/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0893 - accuracy: 0.9758\n","Epoch 00096: val_accuracy did not improve from 0.97142\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0893 - accuracy: 0.9758 - val_loss: 0.1014 - val_accuracy: 0.9712\n","Epoch 97/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9761\n","Epoch 00097: val_accuracy did not improve from 0.97142\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0885 - accuracy: 0.9762 - val_loss: 0.1008 - val_accuracy: 0.9713\n","Epoch 98/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9764\n","Epoch 00098: val_accuracy improved from 0.97142 to 0.97175, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0877 - accuracy: 0.9764 - val_loss: 0.1003 - val_accuracy: 0.9718\n","Epoch 99/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9766\n","Epoch 00099: val_accuracy did not improve from 0.97175\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0871 - accuracy: 0.9767 - val_loss: 0.0999 - val_accuracy: 0.9716\n","Epoch 100/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9767\n","Epoch 00100: val_accuracy improved from 0.97175 to 0.97200, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0864 - accuracy: 0.9768 - val_loss: 0.0990 - val_accuracy: 0.9720\n","Epoch 101/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0856 - accuracy: 0.9771\n","Epoch 00101: val_accuracy did not improve from 0.97200\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0856 - accuracy: 0.9771 - val_loss: 0.0985 - val_accuracy: 0.9720\n","Epoch 102/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0851 - accuracy: 0.9771\n","Epoch 00102: val_accuracy improved from 0.97200 to 0.97208, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0851 - accuracy: 0.9771 - val_loss: 0.0989 - val_accuracy: 0.9721\n","Epoch 103/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9775\n","Epoch 00103: val_accuracy improved from 0.97208 to 0.97225, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0844 - accuracy: 0.9775 - val_loss: 0.0969 - val_accuracy: 0.9722\n","Epoch 104/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0836 - accuracy: 0.9775\n","Epoch 00104: val_accuracy improved from 0.97225 to 0.97233, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0836 - accuracy: 0.9775 - val_loss: 0.0964 - val_accuracy: 0.9723\n","Epoch 105/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0831 - accuracy: 0.9776\n","Epoch 00105: val_accuracy did not improve from 0.97233\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0830 - accuracy: 0.9777 - val_loss: 0.0958 - val_accuracy: 0.9721\n","Epoch 106/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0824 - accuracy: 0.9780\n","Epoch 00106: val_accuracy improved from 0.97233 to 0.97267, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0825 - accuracy: 0.9780 - val_loss: 0.0952 - val_accuracy: 0.9727\n","Epoch 107/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9781\n","Epoch 00107: val_accuracy did not improve from 0.97267\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0817 - accuracy: 0.9781 - val_loss: 0.0948 - val_accuracy: 0.9726\n","Epoch 108/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0813 - accuracy: 0.9782\n","Epoch 00108: val_accuracy did not improve from 0.97267\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0812 - accuracy: 0.9783 - val_loss: 0.0945 - val_accuracy: 0.9722\n","Epoch 109/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9782\n","Epoch 00109: val_accuracy improved from 0.97267 to 0.97292, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0807 - accuracy: 0.9782 - val_loss: 0.0941 - val_accuracy: 0.9729\n","Epoch 110/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9785\n","Epoch 00110: val_accuracy improved from 0.97292 to 0.97317, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0800 - accuracy: 0.9785 - val_loss: 0.0939 - val_accuracy: 0.9732\n","Epoch 111/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9790\n","Epoch 00111: val_accuracy improved from 0.97317 to 0.97367, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0795 - accuracy: 0.9790 - val_loss: 0.0924 - val_accuracy: 0.9737\n","Epoch 112/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9788\n","Epoch 00112: val_accuracy did not improve from 0.97367\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0790 - accuracy: 0.9788 - val_loss: 0.0922 - val_accuracy: 0.9735\n","Epoch 113/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0784 - accuracy: 0.9787\n","Epoch 00113: val_accuracy did not improve from 0.97367\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0784 - accuracy: 0.9787 - val_loss: 0.0918 - val_accuracy: 0.9736\n","Epoch 114/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9788\n","Epoch 00114: val_accuracy improved from 0.97367 to 0.97425, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0779 - accuracy: 0.9787 - val_loss: 0.0909 - val_accuracy: 0.9743\n","Epoch 115/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 0.9795\n","Epoch 00115: val_accuracy improved from 0.97425 to 0.97458, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0774 - accuracy: 0.9795 - val_loss: 0.0910 - val_accuracy: 0.9746\n","Epoch 116/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9795\n","Epoch 00116: val_accuracy did not improve from 0.97458\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0769 - accuracy: 0.9795 - val_loss: 0.0902 - val_accuracy: 0.9743\n","Epoch 117/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9793\n","Epoch 00117: val_accuracy did not improve from 0.97458\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0763 - accuracy: 0.9794 - val_loss: 0.0899 - val_accuracy: 0.9744\n","Epoch 118/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9795\n","Epoch 00118: val_accuracy did not improve from 0.97458\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0759 - accuracy: 0.9795 - val_loss: 0.0892 - val_accuracy: 0.9743\n","Epoch 119/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9796\n","Epoch 00119: val_accuracy improved from 0.97458 to 0.97492, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0753 - accuracy: 0.9797 - val_loss: 0.0887 - val_accuracy: 0.9749\n","Epoch 120/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9800\n","Epoch 00120: val_accuracy did not improve from 0.97492\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0750 - accuracy: 0.9800 - val_loss: 0.0892 - val_accuracy: 0.9746\n","Epoch 121/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9800\n","Epoch 00121: val_accuracy did not improve from 0.97492\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0744 - accuracy: 0.9800 - val_loss: 0.0883 - val_accuracy: 0.9740\n","Epoch 122/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9803\n","Epoch 00122: val_accuracy did not improve from 0.97492\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0740 - accuracy: 0.9803 - val_loss: 0.0878 - val_accuracy: 0.9749\n","Epoch 123/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9803\n","Epoch 00123: val_accuracy did not improve from 0.97492\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0736 - accuracy: 0.9804 - val_loss: 0.0875 - val_accuracy: 0.9745\n","Epoch 124/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9802\n","Epoch 00124: val_accuracy improved from 0.97492 to 0.97508, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0732 - accuracy: 0.9802 - val_loss: 0.0873 - val_accuracy: 0.9751\n","Epoch 125/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9808\n","Epoch 00125: val_accuracy did not improve from 0.97508\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0726 - accuracy: 0.9807 - val_loss: 0.0872 - val_accuracy: 0.9743\n","Epoch 126/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9803\n","Epoch 00126: val_accuracy improved from 0.97508 to 0.97542, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0723 - accuracy: 0.9803 - val_loss: 0.0864 - val_accuracy: 0.9754\n","Epoch 127/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9810\n","Epoch 00127: val_accuracy did not improve from 0.97542\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0718 - accuracy: 0.9810 - val_loss: 0.0861 - val_accuracy: 0.9746\n","Epoch 128/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9807\n","Epoch 00128: val_accuracy did not improve from 0.97542\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0714 - accuracy: 0.9808 - val_loss: 0.0853 - val_accuracy: 0.9754\n","Epoch 129/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9813\n","Epoch 00129: val_accuracy improved from 0.97542 to 0.97558, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0710 - accuracy: 0.9813 - val_loss: 0.0845 - val_accuracy: 0.9756\n","Epoch 130/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9810\n","Epoch 00130: val_accuracy did not improve from 0.97558\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0705 - accuracy: 0.9810 - val_loss: 0.0848 - val_accuracy: 0.9748\n","Epoch 131/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0702 - accuracy: 0.9813\n","Epoch 00131: val_accuracy did not improve from 0.97558\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0702 - accuracy: 0.9813 - val_loss: 0.0847 - val_accuracy: 0.9752\n","Epoch 132/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0698 - accuracy: 0.9811\n","Epoch 00132: val_accuracy did not improve from 0.97558\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0698 - accuracy: 0.9811 - val_loss: 0.0844 - val_accuracy: 0.9756\n","Epoch 133/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0693 - accuracy: 0.9814\n","Epoch 00133: val_accuracy did not improve from 0.97558\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0694 - accuracy: 0.9814 - val_loss: 0.0833 - val_accuracy: 0.9753\n","Epoch 134/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0690 - accuracy: 0.9812\n","Epoch 00134: val_accuracy improved from 0.97558 to 0.97583, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0691 - accuracy: 0.9812 - val_loss: 0.0832 - val_accuracy: 0.9758\n","Epoch 135/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9813\n","Epoch 00135: val_accuracy did not improve from 0.97583\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0686 - accuracy: 0.9813 - val_loss: 0.0831 - val_accuracy: 0.9758\n","Epoch 136/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.9815\n","Epoch 00136: val_accuracy did not improve from 0.97583\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0682 - accuracy: 0.9814 - val_loss: 0.0822 - val_accuracy: 0.9758\n","Epoch 137/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9819\n","Epoch 00137: val_accuracy improved from 0.97583 to 0.97592, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0679 - accuracy: 0.9819 - val_loss: 0.0825 - val_accuracy: 0.9759\n","Epoch 138/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9818\n","Epoch 00138: val_accuracy improved from 0.97592 to 0.97600, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0675 - accuracy: 0.9818 - val_loss: 0.0821 - val_accuracy: 0.9760\n","Epoch 139/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9817\n","Epoch 00139: val_accuracy improved from 0.97600 to 0.97675, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0671 - accuracy: 0.9817 - val_loss: 0.0816 - val_accuracy: 0.9768\n","Epoch 140/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9816\n","Epoch 00140: val_accuracy did not improve from 0.97675\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0669 - accuracy: 0.9817 - val_loss: 0.0816 - val_accuracy: 0.9762\n","Epoch 141/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0665 - accuracy: 0.9825\n","Epoch 00141: val_accuracy did not improve from 0.97675\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0665 - accuracy: 0.9825 - val_loss: 0.0815 - val_accuracy: 0.9758\n","Epoch 142/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9823\n","Epoch 00142: val_accuracy did not improve from 0.97675\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0662 - accuracy: 0.9823 - val_loss: 0.0807 - val_accuracy: 0.9767\n","Epoch 143/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9827\n","Epoch 00143: val_accuracy did not improve from 0.97675\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0658 - accuracy: 0.9827 - val_loss: 0.0802 - val_accuracy: 0.9764\n","Epoch 144/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0655 - accuracy: 0.9827\n","Epoch 00144: val_accuracy improved from 0.97675 to 0.97708, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0654 - accuracy: 0.9827 - val_loss: 0.0808 - val_accuracy: 0.9771\n","Epoch 145/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9827\n","Epoch 00145: val_accuracy did not improve from 0.97708\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0652 - accuracy: 0.9827 - val_loss: 0.0795 - val_accuracy: 0.9768\n","Epoch 146/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9827\n","Epoch 00146: val_accuracy improved from 0.97708 to 0.97767, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0649 - accuracy: 0.9827 - val_loss: 0.0793 - val_accuracy: 0.9777\n","Epoch 147/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9826\n","Epoch 00147: val_accuracy did not improve from 0.97767\n","188/188 [==============================] - 21s 112ms/step - loss: 0.0645 - accuracy: 0.9826 - val_loss: 0.0799 - val_accuracy: 0.9758\n","Epoch 148/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0643 - accuracy: 0.9827\n","Epoch 00148: val_accuracy did not improve from 0.97767\n","188/188 [==============================] - 18s 93ms/step - loss: 0.0642 - accuracy: 0.9828 - val_loss: 0.0793 - val_accuracy: 0.9768\n","Epoch 149/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0639 - accuracy: 0.9831\n","Epoch 00149: val_accuracy did not improve from 0.97767\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0640 - accuracy: 0.9830 - val_loss: 0.0785 - val_accuracy: 0.9771\n","Epoch 150/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9831\n","Epoch 00150: val_accuracy did not improve from 0.97767\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0636 - accuracy: 0.9831 - val_loss: 0.0795 - val_accuracy: 0.9764\n","Epoch 151/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9827\n","Epoch 00151: val_accuracy did not improve from 0.97767\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0634 - accuracy: 0.9827 - val_loss: 0.0782 - val_accuracy: 0.9773\n","Epoch 152/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9829\n","Epoch 00152: val_accuracy did not improve from 0.97767\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0629 - accuracy: 0.9829 - val_loss: 0.0781 - val_accuracy: 0.9769\n","Epoch 153/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9831\n","Epoch 00153: val_accuracy did not improve from 0.97767\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0627 - accuracy: 0.9831 - val_loss: 0.0780 - val_accuracy: 0.9774\n","Epoch 154/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9830\n","Epoch 00154: val_accuracy improved from 0.97767 to 0.97808, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0624 - accuracy: 0.9830 - val_loss: 0.0777 - val_accuracy: 0.9781\n","Epoch 155/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0622 - accuracy: 0.9832\n","Epoch 00155: val_accuracy did not improve from 0.97808\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0622 - accuracy: 0.9832 - val_loss: 0.0771 - val_accuracy: 0.9778\n","Epoch 156/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9830\n","Epoch 00156: val_accuracy did not improve from 0.97808\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0619 - accuracy: 0.9830 - val_loss: 0.0773 - val_accuracy: 0.9776\n","Epoch 157/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9831\n","Epoch 00157: val_accuracy did not improve from 0.97808\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0616 - accuracy: 0.9831 - val_loss: 0.0770 - val_accuracy: 0.9776\n","Epoch 158/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0613 - accuracy: 0.9834\n","Epoch 00158: val_accuracy did not improve from 0.97808\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0614 - accuracy: 0.9833 - val_loss: 0.0769 - val_accuracy: 0.9772\n","Epoch 159/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0609 - accuracy: 0.9833\n","Epoch 00159: val_accuracy did not improve from 0.97808\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0610 - accuracy: 0.9833 - val_loss: 0.0770 - val_accuracy: 0.9775\n","Epoch 160/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9833\n","Epoch 00160: val_accuracy did not improve from 0.97808\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0608 - accuracy: 0.9833 - val_loss: 0.0768 - val_accuracy: 0.9775\n","Epoch 161/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9834\n","Epoch 00161: val_accuracy did not improve from 0.97808\n","188/188 [==============================] - 17s 92ms/step - loss: 0.0604 - accuracy: 0.9834 - val_loss: 0.0765 - val_accuracy: 0.9773\n","Epoch 162/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9834\n","Epoch 00162: val_accuracy did not improve from 0.97808\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0603 - accuracy: 0.9835 - val_loss: 0.0761 - val_accuracy: 0.9772\n","Epoch 163/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0599 - accuracy: 0.9837\n","Epoch 00163: val_accuracy did not improve from 0.97808\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0600 - accuracy: 0.9836 - val_loss: 0.0758 - val_accuracy: 0.9778\n","Epoch 164/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0597 - accuracy: 0.9838\n","Epoch 00164: val_accuracy improved from 0.97808 to 0.97833, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0597 - accuracy: 0.9837 - val_loss: 0.0755 - val_accuracy: 0.9783\n","Epoch 165/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9836\n","Epoch 00165: val_accuracy did not improve from 0.97833\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0595 - accuracy: 0.9836 - val_loss: 0.0749 - val_accuracy: 0.9778\n","Epoch 166/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9839\n","Epoch 00166: val_accuracy did not improve from 0.97833\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0593 - accuracy: 0.9839 - val_loss: 0.0750 - val_accuracy: 0.9782\n","Epoch 167/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0590 - accuracy: 0.9840\n","Epoch 00167: val_accuracy did not improve from 0.97833\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0590 - accuracy: 0.9839 - val_loss: 0.0755 - val_accuracy: 0.9780\n","Epoch 168/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0587 - accuracy: 0.9839\n","Epoch 00168: val_accuracy improved from 0.97833 to 0.97842, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0587 - accuracy: 0.9839 - val_loss: 0.0748 - val_accuracy: 0.9784\n","Epoch 169/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9838\n","Epoch 00169: val_accuracy did not improve from 0.97842\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0585 - accuracy: 0.9838 - val_loss: 0.0742 - val_accuracy: 0.9782\n","Epoch 170/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0581 - accuracy: 0.9842\n","Epoch 00170: val_accuracy did not improve from 0.97842\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0582 - accuracy: 0.9842 - val_loss: 0.0746 - val_accuracy: 0.9778\n","Epoch 171/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9842\n","Epoch 00171: val_accuracy improved from 0.97842 to 0.97867, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0580 - accuracy: 0.9842 - val_loss: 0.0739 - val_accuracy: 0.9787\n","Epoch 172/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9841\n","Epoch 00172: val_accuracy improved from 0.97867 to 0.97883, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0578 - accuracy: 0.9841 - val_loss: 0.0739 - val_accuracy: 0.9788\n","Epoch 173/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0575 - accuracy: 0.9842\n","Epoch 00173: val_accuracy did not improve from 0.97883\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0575 - accuracy: 0.9842 - val_loss: 0.0742 - val_accuracy: 0.9788\n","Epoch 174/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0574 - accuracy: 0.9844\n","Epoch 00174: val_accuracy did not improve from 0.97883\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0574 - accuracy: 0.9844 - val_loss: 0.0748 - val_accuracy: 0.9784\n","Epoch 175/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9844\n","Epoch 00175: val_accuracy did not improve from 0.97883\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0571 - accuracy: 0.9844 - val_loss: 0.0734 - val_accuracy: 0.9787\n","Epoch 176/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0569 - accuracy: 0.9846\n","Epoch 00176: val_accuracy improved from 0.97883 to 0.97908, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0568 - accuracy: 0.9846 - val_loss: 0.0728 - val_accuracy: 0.9791\n","Epoch 177/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0566 - accuracy: 0.9845\n","Epoch 00177: val_accuracy improved from 0.97908 to 0.97917, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0566 - accuracy: 0.9845 - val_loss: 0.0732 - val_accuracy: 0.9792\n","Epoch 178/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9848\n","Epoch 00178: val_accuracy did not improve from 0.97917\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0564 - accuracy: 0.9848 - val_loss: 0.0727 - val_accuracy: 0.9785\n","Epoch 179/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0562 - accuracy: 0.9846\n","Epoch 00179: val_accuracy did not improve from 0.97917\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0562 - accuracy: 0.9846 - val_loss: 0.0725 - val_accuracy: 0.9778\n","Epoch 180/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0561 - accuracy: 0.9844\n","Epoch 00180: val_accuracy did not improve from 0.97917\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0560 - accuracy: 0.9844 - val_loss: 0.0725 - val_accuracy: 0.9787\n","Epoch 181/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0559 - accuracy: 0.9845\n","Epoch 00181: val_accuracy did not improve from 0.97917\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0558 - accuracy: 0.9845 - val_loss: 0.0723 - val_accuracy: 0.9785\n","Epoch 182/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0556 - accuracy: 0.9849\n","Epoch 00182: val_accuracy did not improve from 0.97917\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0555 - accuracy: 0.9849 - val_loss: 0.0730 - val_accuracy: 0.9783\n","Epoch 183/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0553 - accuracy: 0.9847\n","Epoch 00183: val_accuracy improved from 0.97917 to 0.97950, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 22s 117ms/step - loss: 0.0553 - accuracy: 0.9847 - val_loss: 0.0721 - val_accuracy: 0.9795\n","Epoch 184/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0551 - accuracy: 0.9850\n","Epoch 00184: val_accuracy did not improve from 0.97950\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0552 - accuracy: 0.9849 - val_loss: 0.0715 - val_accuracy: 0.9795\n","Epoch 185/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0548 - accuracy: 0.9850\n","Epoch 00185: val_accuracy did not improve from 0.97950\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0548 - accuracy: 0.9851 - val_loss: 0.0712 - val_accuracy: 0.9794\n","Epoch 186/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0547 - accuracy: 0.9851\n","Epoch 00186: val_accuracy did not improve from 0.97950\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0547 - accuracy: 0.9851 - val_loss: 0.0719 - val_accuracy: 0.9793\n","Epoch 187/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0545 - accuracy: 0.9852\n","Epoch 00187: val_accuracy did not improve from 0.97950\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0545 - accuracy: 0.9852 - val_loss: 0.0713 - val_accuracy: 0.9784\n","Epoch 188/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0543 - accuracy: 0.9852\n","Epoch 00188: val_accuracy improved from 0.97950 to 0.97983, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0543 - accuracy: 0.9852 - val_loss: 0.0710 - val_accuracy: 0.9798\n","Epoch 189/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0541 - accuracy: 0.9852\n","Epoch 00189: val_accuracy did not improve from 0.97983\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0541 - accuracy: 0.9852 - val_loss: 0.0711 - val_accuracy: 0.9790\n","Epoch 190/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0539 - accuracy: 0.9851\n","Epoch 00190: val_accuracy improved from 0.97983 to 0.97992, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0539 - accuracy: 0.9851 - val_loss: 0.0704 - val_accuracy: 0.9799\n","Epoch 191/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0538 - accuracy: 0.9853\n","Epoch 00191: val_accuracy did not improve from 0.97992\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0537 - accuracy: 0.9853 - val_loss: 0.0710 - val_accuracy: 0.9796\n","Epoch 192/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0536 - accuracy: 0.9852\n","Epoch 00192: val_accuracy did not improve from 0.97992\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0535 - accuracy: 0.9852 - val_loss: 0.0701 - val_accuracy: 0.9793\n","Epoch 193/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0532 - accuracy: 0.9855\n","Epoch 00193: val_accuracy did not improve from 0.97992\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0532 - accuracy: 0.9854 - val_loss: 0.0708 - val_accuracy: 0.9787\n","Epoch 194/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0531 - accuracy: 0.9853\n","Epoch 00194: val_accuracy did not improve from 0.97992\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0531 - accuracy: 0.9853 - val_loss: 0.0701 - val_accuracy: 0.9797\n","Epoch 195/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0530 - accuracy: 0.9856\n","Epoch 00195: val_accuracy did not improve from 0.97992\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0529 - accuracy: 0.9856 - val_loss: 0.0707 - val_accuracy: 0.9788\n","Epoch 196/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0527 - accuracy: 0.9854\n","Epoch 00196: val_accuracy did not improve from 0.97992\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0527 - accuracy: 0.9854 - val_loss: 0.0700 - val_accuracy: 0.9794\n","Epoch 197/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0526 - accuracy: 0.9856\n","Epoch 00197: val_accuracy did not improve from 0.97992\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0525 - accuracy: 0.9856 - val_loss: 0.0698 - val_accuracy: 0.9799\n","Epoch 198/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0524 - accuracy: 0.9856\n","Epoch 00198: val_accuracy improved from 0.97992 to 0.98017, saving model to mnist_conv_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0523 - accuracy: 0.9857 - val_loss: 0.0696 - val_accuracy: 0.9802\n","Epoch 199/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9858\n","Epoch 00199: val_accuracy did not improve from 0.98017\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0522 - accuracy: 0.9858 - val_loss: 0.0698 - val_accuracy: 0.9787\n","Epoch 200/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0519 - accuracy: 0.9858\n","Epoch 00200: val_accuracy did not improve from 0.98017\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0520 - accuracy: 0.9858 - val_loss: 0.0695 - val_accuracy: 0.9797\n","Epoch 201/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0519 - accuracy: 0.9859\n","Epoch 00201: val_accuracy did not improve from 0.98017\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0518 - accuracy: 0.9859 - val_loss: 0.0691 - val_accuracy: 0.9793\n","Epoch 202/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0516 - accuracy: 0.9860\n","Epoch 00202: val_accuracy did not improve from 0.98017\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0516 - accuracy: 0.9860 - val_loss: 0.0692 - val_accuracy: 0.9790\n","Epoch 203/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0515 - accuracy: 0.9860\n","Epoch 00203: val_accuracy did not improve from 0.98017\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0514 - accuracy: 0.9860 - val_loss: 0.0686 - val_accuracy: 0.9797\n","Epoch 204/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0511 - accuracy: 0.9862\n","Epoch 00204: val_accuracy did not improve from 0.98017\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0513 - accuracy: 0.9862 - val_loss: 0.0688 - val_accuracy: 0.9793\n","Epoch 205/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0511 - accuracy: 0.9859\n","Epoch 00205: val_accuracy did not improve from 0.98017\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0511 - accuracy: 0.9860 - val_loss: 0.0691 - val_accuracy: 0.9787\n","Epoch 206/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0510 - accuracy: 0.9856\n","Epoch 00206: val_accuracy did not improve from 0.98017\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0509 - accuracy: 0.9857 - val_loss: 0.0688 - val_accuracy: 0.9795\n","Epoch 207/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0508 - accuracy: 0.9858\n","Epoch 00207: val_accuracy did not improve from 0.98017\n","188/188 [==============================] - 17s 91ms/step - loss: 0.0507 - accuracy: 0.9858 - val_loss: 0.0684 - val_accuracy: 0.9797\n","Epoch 208/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0504 - accuracy: 0.9860\n","Epoch 00208: val_accuracy did not improve from 0.98017\n","188/188 [==============================] - 17s 90ms/step - loss: 0.0505 - accuracy: 0.9860 - val_loss: 0.0690 - val_accuracy: 0.9802\n","Epoch 00208: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Yvx3T3u6OQJ1","colab_type":"text"},"source":["And visualize the evolution:"]},{"cell_type":"code","metadata":{"id":"GLpJC_HLF3ch","colab_type":"code","outputId":"7ccbd74c-5bf5-48c2-fd70-993640f80651","executionInfo":{"status":"error","timestamp":1590954132332,"user_tz":-60,"elapsed":1441,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}},"colab":{"base_uri":"https://localhost:8080/","height":673}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_conv_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_conv_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_conv_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_conv_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":58,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-93bb9af26c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloss_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_conv_model_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mloss_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_conv_model_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'mnist_conv_model_train' is not defined"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABIkAAAGrCAYAAABE7sfCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAYEElEQVR4nO3df6jv913Y8eerSX8wrTrMFTSJTcVUDW7M7q7rcMxu7UZaRvKHIgnr/EEx4BbZZpFFdNXF/bGuzIkQpxFL/YGN1T/cZUYyppWKmNLbdXZNSuQaq0nqSKw1ODpto+/9cU7H8Zr0fs0953u+yffxgC98f7x7zvvy5t68+jzf7+fMWisAAAAA9tsLTnsDAAAAAJw+kQgAAAAAkQgAAAAAkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIuAwz85GZed1p7wMAAIDLJxIBAAAAIBIBx2tmXjwzPzAzHz28/cDMvPjwtatm5r/OzB/OzB/MzK/OzAsOX/vXM/PYzPzRzDw0M6893T8JAADAfrnytDcAPO98V/Xq6m9Uq/ov1XdX/6Z6c/VodeZw7aurNTNfVt1e/a211kdn5rrqiu1uGwAAYL95JxFw3P5Jdeda6/G11hPVv63+6eFrn6q+sHrZWutTa61fXWut6k+rF1c3zMwL11ofWWv91qnsHgAAYE+JRMBx+6Lqd448/p3D56reVl2o/tvMPDwzd1SttS5U/7L63urxmblnZr4oAAAAtkYkAo7bR6uXHXn8xYfPtdb6o7XWm9daX1LdVH37p689tNb66bXW3z38367qrdvdNgAAwH4TiYDL9cKZecmnb9U7q++emTMzc1X1luqnqmbmH8/Ml87MVE928DGzP5uZL5uZf3B4ges/rv5v9Wen88cBAADYTyIRcLnu7SDqfPr2kup89cHqf1X/o/p3h2uvr/579X+qX69+aK317g6uR/Tvq9+v/nf1BdV3bu+PAAAAwBxcMxYAAACAfeadRAAAAABcOhLNzNtn5vGZ+dAzvD4z84Mzc2FmPjgzrzz+bQIA7BczGACwbZu8k+gd1Y2f4fXXd3Cdkeur26r/fPnbAgDYe+/IDAYAbNElI9Fa6z3VH3yGJTdXP7EO3F993sx84XFtEABgH5nBAIBtu/IYvsbV1SNHHj96+NzvXbxwZm7r4CddfdZnfdbf/PIv//Jj+PYAwC56//vf//trrTOnvY/nMTMYAPAXXM4MdhyRaGNrrburu6vOnj27zp8/v81vDwBs0cz8zmnvgQNmMADYH5czgx3Hbzd7rLr2yONrDp8DAODkmMEAgGN1HJHoXPUNh79h49XVk2utv/A2ZwAAjpUZDAA4Vpf8uNnMvLN6TXXVzDxafU/1wqq11g9X91ZvqC5Un6i++aQ2CwCwL8xgAMC2XTISrbVuvcTrq/rnx7YjAADMYADA1h3Hx80AAAAAeI4TiQAAAAAQiQAAAAAQiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAA2jASzcyNM/PQzFyYmTue5vUvnpl3z8wHZuaDM/OG498qAMB+MYMBANt0yUg0M1dUd1Wvr26obp2ZGy5a9t3Vu9ZaX1XdUv3QcW8UAGCfmMEAgG3b5J1Er6ourLUeXmt9srqnuvmiNav6nMP7n1t99Pi2CACwl8xgAMBWbRKJrq4eOfL40cPnjvre6o0z82h1b/VtT/eFZua2mTk/M+efeOKJZ7FdAIC9YQYDALbquC5cfWv1jrXWNdUbqp+cmb/wtddad6+1zq61zp45c+aYvjUAwN4ygwEAx2aTSPRYde2Rx9ccPnfUm6p3Va21fr16SXXVcWwQAGBPmcEAgK3aJBK9r7p+Zl4+My/q4KKI5y5a87vVa6tm5is6GFC8lxkA4NkzgwEAW3XJSLTWeqq6vbqv+nAHv0HjgZm5c2ZuOlz25upbZuY3qndW37TWWie1aQCA5zszGACwbVdusmitdW8HF0M8+txbjtx/sPrq490aAMB+M4MBANt0XBeuBgAAAOA5TCQCAAAAQCQCAAAAQCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAGjDSDQzN87MQzNzYWbueIY1Xz8zD87MAzPz08e7TQCA/WMGAwC26cpLLZiZK6q7qn9YPVq9b2bOrbUePLLm+uo7q69ea318Zr7gpDYMALAPzGAAwLZt8k6iV1UX1loPr7U+Wd1T3XzRmm+p7lprfbxqrfX48W4TAGDvmMEAgK3aJBJdXT1y5PGjh88d9YrqFTPzazNz/8zc+HRfaGZum5nzM3P+iSeeeHY7BgDYD2YwAGCrjuvC1VdW11evqW6tfnRmPu/iRWutu9daZ9daZ8+cOXNM3xoAYG+ZwQCAY7NJJHqsuvbI42sOnzvq0ercWutTa63frn6zg4EFAIBnxwwGAGzVJpHofdX1M/PymXlRdUt17qI1P9/BT7Camas6eOvzw8e4TwCAfWMGAwC26pKRaK31VHV7dV/14epda60HZubOmbnpcNl91cdm5sHq3dV3rLU+dlKbBgB4vjODAQDbNmutU/nGZ8+eXefPnz+V7w0AnLyZef9a6+xp74M/zwwGAM9vlzODHdeFqwEAAAB4DhOJAAAAABCJAAAAABCJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABow0g0MzfOzEMzc2Fm7vgM6752ZtbMnD2+LQIA7CczGACwTZeMRDNzRXVX9frqhurWmbnhada9tPoX1XuPe5MAAPvGDAYAbNsm7yR6VXVhrfXwWuuT1T3VzU+z7vuqt1Z/fIz7AwDYV2YwAGCrNolEV1ePHHn86OFz/9/MvLK6dq31C5/pC83MbTNzfmbOP/HEE3/pzQIA7BEzGACwVZd94eqZeUH1/dWbL7V2rXX3WuvsWuvsmTNnLvdbAwDsLTMYAHDcNolEj1XXHnl8zeFzn/bS6iurX5mZj1Svrs65cCIAwGUxgwEAW7VJJHpfdf3MvHxmXlTdUp379ItrrSfXWletta5ba11X3V/dtNY6fyI7BgDYD2YwAGCrLhmJ1lpPVbdX91Ufrt611npgZu6cmZtOeoMAAPvIDAYAbNuVmyxaa91b3XvRc295hrWvufxtAQBgBgMAtumyL1wNAAAAwHOfSAQAAACASAQAAACASAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAA0IaRaGZunJmHZubCzNzxNK9/+8w8ODMfnJlfmpmXHf9WAQD2ixkMANimS0aimbmiuqt6fXVDdevM3HDRsg9UZ9daf736ueo/HPdGAQD2iRkMANi2Td5J9Krqwlrr4bXWJ6t7qpuPLlhrvXut9YnDh/dX1xzvNgEA9o4ZDADYqk0i0dXVI0ceP3r43DN5U/WLT/fCzNw2M+dn5vwTTzyx+S4BAPaPGQwA2KpjvXD1zLyxOlu97eleX2vdvdY6u9Y6e+bMmeP81gAAe8sMBgAchys3WPNYde2Rx9ccPvfnzMzrqu+qvmat9SfHsz0AgL1lBgMAtmqTdxK9r7p+Zl4+My+qbqnOHV0wM19V/Uh101rr8ePfJgDA3jGDAQBbdclItNZ6qrq9uq/6cPWutdYDM3PnzNx0uOxt1WdXPzsz/3Nmzj3DlwMAYANmMABg2zb5uFlrrXurey967i1H7r/umPcFALD3zGAAwDYd64WrAQAAAHhuEokAAAAAEIkAAAAAEIkAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAANowEs3MjTPz0MxcmJk7nub1F8/Mzxy+/t6Zue64NwoAsG/MYADANl0yEs3MFdVd1eurG6pbZ+aGi5a9qfr4WutLq/9UvfW4NwoAsE/MYADAtm3yTqJXVRfWWg+vtT5Z3VPdfNGam6sfP7z/c9VrZ2aOb5sAAHvHDAYAbNWVG6y5unrkyONHq7/9TGvWWk/NzJPV51e/f3TRzNxW3Xb48E9m5kPPZtOcqKu66Nw4dc5k9ziT3eRcds+XnfYGnuPMYPvDv1+7ybnsHmeym5zL7nnWM9gmkejYrLXuru6umpnza62z2/z+XJpz2T3OZPc4k93kXHbPzJw/7T1wwAy225zJbnIuu8eZ7CbnsnsuZwbb5ONmj1XXHnl8zeFzT7tmZq6sPrf62LPdFAAAZjAAYLs2iUTvq66fmZfPzIuqW6pzF605V33j4f2vq355rbWOb5sAAHvHDAYAbNUlP252+Pn226v7qiuqt6+1HpiZO6vza61z1Y9VPzkzF6o/6GCIuZS7L2PfnBznsnucye5xJrvJueweZ3IZzGB7xZnsJueye5zJbnIuu+dZn8n4YRMAAAAAm3zcDAAAAIDnOZEIAAAAgJOPRDNz48w8NDMXZuaOp3n9xTPzM4evv3dmrjvpPe27Dc7k22fmwZn54Mz80sy87DT2uW8udS5H1n3tzKyZ8WsmT9gmZzIzX3/49+WBmfnpbe9xH23wb9gXz8y7Z+YDh/+OveE09rlPZubtM/P4zHzoGV6fmfnBwzP74My8ctt73EdmsN1jBts95q/dZAbbPeav3XNi89da68RuHVxk8beqL6leVP1GdcNFa/5Z9cOH92+pfuYk97Tvtw3P5O9Xf+Xw/rc6k904l8N1L63eU91fnT3tfT+fbxv+Xbm++kD1Vw8ff8Fp7/v5ftvwXO6uvvXw/g3VR05738/3W/X3qldWH3qG199Q/WI11aur9572np/vNzPY7t3MYLt3M3/t5s0Mtns389du3k5q/jrpdxK9qrqw1np4rfXJ6p7q5ovW3Fz9+OH9n6teOzNzwvvaZ5c8k7XWu9danzh8eH91zZb3uI82+btS9X3VW6s/3ubm9tQmZ/It1V1rrY9XrbUe3/Ie99Em57Kqzzm8/7nVR7e4v7201npPB79Z65ncXP3EOnB/9Xkz84Xb2d3eMoPtHjPY7jF/7SYz2O4xf+2gk5q/TjoSXV09cuTxo4fPPe2atdZT1ZPV55/wvvbZJmdy1Js6qI+crEuey+HbA69da/3CNje2xzb5u/KK6hUz82szc//M3Li13e2vTc7le6s3zsyj1b3Vt21na3wGf9n/9nD5zGC7xwy2e8xfu8kMtnvMX89Nz2r+uvLEtsNz3sy8sTpbfc1p72XfzcwLqu+vvumUt8Kfd2UHb3d+TQc/7X3PzPy1tdYfnuquuLV6x1rrP87M36l+cma+cq31Z6e9MYBNmMF2g/lrp5nBdo/563nipN9J9Fh17ZHH1xw+97RrZubKDt6a9rET3tc+2+RMmpnXVd9V3bTW+pMt7W2fXepcXlp9ZfUrM/ORDj5Tes7FE0/UJn9XHq3OrbU+tdb67eo3OxhYODmbnMubqndVrbV+vXpJddVWdscz2ei/PRwrM9juMYPtHvPXbjKD7R7z13PTs5q/TjoSva+6fmZePjMv6uCiiOcuWnOu+sbD+19X/fI6vMoSJ+KSZzIzX1X9SAfDic/3bsdnPJe11pNrravWWtetta7r4DoFN621zp/OdvfCJv9+/XwHP8FqZq7q4K3PD29zk3tok3P53eq1VTPzFR0MKU9sdZdc7Fz1DYe/ZePV1ZNrrd877U09z5nBdo8ZbPeYv3aTGWz3mL+em57V/HWiHzdbaz01M7dX93VwRfS3r7UemJk7q/NrrXPVj3XwVrQLHVx06ZaT3NO+2/BM3lZ9dvWzh9ev/N211k2ntuk9sOG5sEUbnsl91T+amQerP62+Y63lp/AnaMNzeXP1ozPzrzq4iOI3+T++J2tm3tnBsH7V4bUIvqd6YdVa64c7uDbBG6oL1Seqbz6dne4PM9juMYPtHvPXbjKD7R7z1246qflrnBsAAAAAJ/1xMwAAAACeA0QiAAAAAEQiAAAAAEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIDq/wHl6NCeGuE2JgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 1440x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"VE5RpeG6OUBt","colab_type":"text"},"source":["*Finally*, we can evaluate the model on the test set, and verify that the performance is higher than without the convolutional layers."]},{"cell_type":"code","metadata":{"id":"ejq-Nb9HGFMC","colab_type":"code","outputId":"963a9ca7-5b03-4701-8b95-42bc9464e28e","executionInfo":{"status":"ok","timestamp":1590606640577,"user_tz":-60,"elapsed":3757864,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["mnist_conv_model.load_weights('mnist_conv_best.h5')\n","loss, acc = mnist_conv_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 2s 6ms/step - loss: 0.0587 - accuracy: 0.9817\n","Accuracy: 0.9817000031471252\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t-kSVx5wEmqy","colab_type":"text"},"source":["#### Hidden Layers\n"]},{"cell_type":"markdown","metadata":{"id":"7KmObkijEsXc","colab_type":"text"},"source":["We will vary the number of hidden layers in order to see if the performance on the model is increased. We will do this by adding some more convolutional layers to the baseline model:"]},{"cell_type":"markdown","metadata":{"id":"LpbF4z5-E2n3","colab_type":"text"},"source":["##### 2 Hidden Layers"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"9cd0d031-2c85-4684-aec5-de05431a0a0e","executionInfo":{"status":"ok","timestamp":1590750174304,"user_tz":-60,"elapsed":1012,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"proCzjiTFmlf","colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["mnist_2conv_model = tf.keras.Sequential(name='mnist_2cnn')\n","mnist_2conv_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_2conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution'))\n","mnist_2conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling'))\n","mnist_2conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution2'))\n","mnist_2conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling2'))\n","mnist_2conv_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_2conv_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_2conv_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_2conv_model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"mnist_2cnn\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","convolution (Conv2D)         (None, 28, 28, 16)        272       \n","_________________________________________________________________\n","pooling (MaxPooling2D)       (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","convolution2 (Conv2D)        (None, 14, 14, 16)        4112      \n","_________________________________________________________________\n","pooling2 (MaxPooling2D)      (None, 7, 7, 16)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 784)               0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                7850      \n","=================================================================\n","Total params: 12,234\n","Trainable params: 12,234\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"8f687c72-7bb1-4998-8d74-829b090adc73","executionInfo":{"status":"ok","timestamp":1590752792490,"user_tz":-60,"elapsed":2317308,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"7HRsXxZYHXIW","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_2conv_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_2conv_model_train = mnist_2conv_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","188/188 [==============================] - ETA: 0s - loss: 2.0604 - accuracy: 0.4080\n","Epoch 00001: val_accuracy improved from -inf to 0.69150, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 150ms/step - loss: 2.0604 - accuracy: 0.4080 - val_loss: 1.3756 - val_accuracy: 0.6915\n","Epoch 2/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.7256 - accuracy: 0.8135\n","Epoch 00002: val_accuracy improved from 0.69150 to 0.86733, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.7256 - accuracy: 0.8135 - val_loss: 0.4708 - val_accuracy: 0.8673\n","Epoch 3/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3956 - accuracy: 0.8865\n","Epoch 00003: val_accuracy improved from 0.86733 to 0.90083, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.3956 - accuracy: 0.8865 - val_loss: 0.3493 - val_accuracy: 0.9008\n","Epoch 4/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3214 - accuracy: 0.9080\n","Epoch 00004: val_accuracy did not improve from 0.90083\n","188/188 [==============================] - 28s 148ms/step - loss: 0.3214 - accuracy: 0.9080 - val_loss: 0.3190 - val_accuracy: 0.8994\n","Epoch 5/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.9187\n","Epoch 00005: val_accuracy improved from 0.90083 to 0.92117, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.2815 - accuracy: 0.9187 - val_loss: 0.2780 - val_accuracy: 0.9212\n","Epoch 6/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.9257\n","Epoch 00006: val_accuracy improved from 0.92117 to 0.92550, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.2525 - accuracy: 0.9257 - val_loss: 0.2559 - val_accuracy: 0.9255\n","Epoch 7/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2298 - accuracy: 0.9321\n","Epoch 00007: val_accuracy did not improve from 0.92550\n","188/188 [==============================] - 28s 148ms/step - loss: 0.2298 - accuracy: 0.9321 - val_loss: 0.2480 - val_accuracy: 0.9250\n","Epoch 8/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2116 - accuracy: 0.9381\n","Epoch 00008: val_accuracy improved from 0.92550 to 0.93733, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.2116 - accuracy: 0.9381 - val_loss: 0.2160 - val_accuracy: 0.9373\n","Epoch 9/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.9434\n","Epoch 00009: val_accuracy improved from 0.93733 to 0.94100, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.1948 - accuracy: 0.9434 - val_loss: 0.2083 - val_accuracy: 0.9410\n","Epoch 10/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9468\n","Epoch 00010: val_accuracy improved from 0.94100 to 0.94600, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 147ms/step - loss: 0.1810 - accuracy: 0.9468 - val_loss: 0.1890 - val_accuracy: 0.9460\n","Epoch 11/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9503\n","Epoch 00011: val_accuracy improved from 0.94600 to 0.94717, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.1702 - accuracy: 0.9503 - val_loss: 0.1840 - val_accuracy: 0.9472\n","Epoch 12/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1597 - accuracy: 0.9532\n","Epoch 00012: val_accuracy improved from 0.94717 to 0.95267, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.1597 - accuracy: 0.9532 - val_loss: 0.1657 - val_accuracy: 0.9527\n","Epoch 13/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.9562\n","Epoch 00013: val_accuracy improved from 0.95267 to 0.95467, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.1509 - accuracy: 0.9562 - val_loss: 0.1556 - val_accuracy: 0.9547\n","Epoch 14/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1436 - accuracy: 0.9579\n","Epoch 00014: val_accuracy improved from 0.95467 to 0.95725, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.1436 - accuracy: 0.9579 - val_loss: 0.1505 - val_accuracy: 0.9572\n","Epoch 15/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1368 - accuracy: 0.9599\n","Epoch 00015: val_accuracy improved from 0.95725 to 0.95875, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.1368 - accuracy: 0.9599 - val_loss: 0.1447 - val_accuracy: 0.9588\n","Epoch 16/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1313 - accuracy: 0.9614\n","Epoch 00016: val_accuracy did not improve from 0.95875\n","188/188 [==============================] - 28s 148ms/step - loss: 0.1313 - accuracy: 0.9614 - val_loss: 0.1431 - val_accuracy: 0.9580\n","Epoch 17/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9626\n","Epoch 00017: val_accuracy improved from 0.95875 to 0.96133, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.1255 - accuracy: 0.9626 - val_loss: 0.1327 - val_accuracy: 0.9613\n","Epoch 18/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1206 - accuracy: 0.9643\n","Epoch 00018: val_accuracy did not improve from 0.96133\n","188/188 [==============================] - 28s 147ms/step - loss: 0.1206 - accuracy: 0.9643 - val_loss: 0.1334 - val_accuracy: 0.9602\n","Epoch 19/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1165 - accuracy: 0.9653\n","Epoch 00019: val_accuracy did not improve from 0.96133\n","188/188 [==============================] - 28s 148ms/step - loss: 0.1165 - accuracy: 0.9653 - val_loss: 0.1349 - val_accuracy: 0.9589\n","Epoch 20/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.9662\n","Epoch 00020: val_accuracy improved from 0.96133 to 0.96175, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.1131 - accuracy: 0.9662 - val_loss: 0.1251 - val_accuracy: 0.9617\n","Epoch 21/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9686\n","Epoch 00021: val_accuracy improved from 0.96175 to 0.96492, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.1093 - accuracy: 0.9686 - val_loss: 0.1157 - val_accuracy: 0.9649\n","Epoch 22/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9686\n","Epoch 00022: val_accuracy improved from 0.96492 to 0.96633, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.1057 - accuracy: 0.9686 - val_loss: 0.1141 - val_accuracy: 0.9663\n","Epoch 23/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9700\n","Epoch 00023: val_accuracy did not improve from 0.96633\n","188/188 [==============================] - 28s 147ms/step - loss: 0.1029 - accuracy: 0.9700 - val_loss: 0.1206 - val_accuracy: 0.9648\n","Epoch 24/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9708\n","Epoch 00024: val_accuracy did not improve from 0.96633\n","188/188 [==============================] - 28s 147ms/step - loss: 0.1001 - accuracy: 0.9708 - val_loss: 0.1177 - val_accuracy: 0.9644\n","Epoch 25/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9715\n","Epoch 00025: val_accuracy improved from 0.96633 to 0.96908, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0979 - accuracy: 0.9715 - val_loss: 0.1060 - val_accuracy: 0.9691\n","Epoch 26/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9716\n","Epoch 00026: val_accuracy did not improve from 0.96908\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0951 - accuracy: 0.9716 - val_loss: 0.1052 - val_accuracy: 0.9684\n","Epoch 27/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9718\n","Epoch 00027: val_accuracy improved from 0.96908 to 0.97075, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0931 - accuracy: 0.9718 - val_loss: 0.1006 - val_accuracy: 0.9707\n","Epoch 28/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9735\n","Epoch 00028: val_accuracy did not improve from 0.97075\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0909 - accuracy: 0.9735 - val_loss: 0.1000 - val_accuracy: 0.9697\n","Epoch 29/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9737\n","Epoch 00029: val_accuracy improved from 0.97075 to 0.97217, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0887 - accuracy: 0.9737 - val_loss: 0.0969 - val_accuracy: 0.9722\n","Epoch 30/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9740\n","Epoch 00030: val_accuracy did not improve from 0.97217\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0874 - accuracy: 0.9740 - val_loss: 0.0957 - val_accuracy: 0.9717\n","Epoch 31/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9748\n","Epoch 00031: val_accuracy did not improve from 0.97217\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0850 - accuracy: 0.9748 - val_loss: 0.1007 - val_accuracy: 0.9697\n","Epoch 32/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9751\n","Epoch 00032: val_accuracy did not improve from 0.97217\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0837 - accuracy: 0.9751 - val_loss: 0.0932 - val_accuracy: 0.9718\n","Epoch 33/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9757\n","Epoch 00033: val_accuracy did not improve from 0.97217\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0822 - accuracy: 0.9757 - val_loss: 0.0966 - val_accuracy: 0.9712\n","Epoch 34/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9762\n","Epoch 00034: val_accuracy improved from 0.97217 to 0.97292, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0807 - accuracy: 0.9762 - val_loss: 0.0904 - val_accuracy: 0.9729\n","Epoch 35/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9765\n","Epoch 00035: val_accuracy improved from 0.97292 to 0.97333, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0795 - accuracy: 0.9765 - val_loss: 0.0886 - val_accuracy: 0.9733\n","Epoch 36/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9769\n","Epoch 00036: val_accuracy did not improve from 0.97333\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0781 - accuracy: 0.9769 - val_loss: 0.0902 - val_accuracy: 0.9728\n","Epoch 37/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9780\n","Epoch 00037: val_accuracy improved from 0.97333 to 0.97375, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0763 - accuracy: 0.9780 - val_loss: 0.0861 - val_accuracy: 0.9737\n","Epoch 38/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9775\n","Epoch 00038: val_accuracy improved from 0.97375 to 0.97517, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0753 - accuracy: 0.9775 - val_loss: 0.0853 - val_accuracy: 0.9752\n","Epoch 39/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9780\n","Epoch 00039: val_accuracy did not improve from 0.97517\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0744 - accuracy: 0.9780 - val_loss: 0.0856 - val_accuracy: 0.9738\n","Epoch 40/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9787\n","Epoch 00040: val_accuracy did not improve from 0.97517\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0733 - accuracy: 0.9787 - val_loss: 0.0819 - val_accuracy: 0.9752\n","Epoch 41/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9787\n","Epoch 00041: val_accuracy did not improve from 0.97517\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0724 - accuracy: 0.9787 - val_loss: 0.0865 - val_accuracy: 0.9749\n","Epoch 42/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9788\n","Epoch 00042: val_accuracy improved from 0.97517 to 0.97525, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0713 - accuracy: 0.9788 - val_loss: 0.0828 - val_accuracy: 0.9753\n","Epoch 43/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9796\n","Epoch 00043: val_accuracy did not improve from 0.97525\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0702 - accuracy: 0.9796 - val_loss: 0.0810 - val_accuracy: 0.9753\n","Epoch 44/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9789\n","Epoch 00044: val_accuracy improved from 0.97525 to 0.97550, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0700 - accuracy: 0.9789 - val_loss: 0.0821 - val_accuracy: 0.9755\n","Epoch 45/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9801\n","Epoch 00045: val_accuracy improved from 0.97550 to 0.97608, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0684 - accuracy: 0.9801 - val_loss: 0.0802 - val_accuracy: 0.9761\n","Epoch 46/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9797\n","Epoch 00046: val_accuracy did not improve from 0.97608\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0675 - accuracy: 0.9797 - val_loss: 0.0776 - val_accuracy: 0.9761\n","Epoch 47/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9807\n","Epoch 00047: val_accuracy did not improve from 0.97608\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0669 - accuracy: 0.9807 - val_loss: 0.0814 - val_accuracy: 0.9749\n","Epoch 48/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9804\n","Epoch 00048: val_accuracy did not improve from 0.97608\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0660 - accuracy: 0.9804 - val_loss: 0.0816 - val_accuracy: 0.9742\n","Epoch 49/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9806\n","Epoch 00049: val_accuracy improved from 0.97608 to 0.97617, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0652 - accuracy: 0.9806 - val_loss: 0.0780 - val_accuracy: 0.9762\n","Epoch 50/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9813\n","Epoch 00050: val_accuracy improved from 0.97617 to 0.97692, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0642 - accuracy: 0.9813 - val_loss: 0.0750 - val_accuracy: 0.9769\n","Epoch 51/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9807\n","Epoch 00051: val_accuracy did not improve from 0.97692\n","188/188 [==============================] - 28s 149ms/step - loss: 0.0636 - accuracy: 0.9807 - val_loss: 0.0751 - val_accuracy: 0.9761\n","Epoch 52/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9813\n","Epoch 00052: val_accuracy improved from 0.97692 to 0.97717, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 149ms/step - loss: 0.0631 - accuracy: 0.9813 - val_loss: 0.0753 - val_accuracy: 0.9772\n","Epoch 53/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9816\n","Epoch 00053: val_accuracy did not improve from 0.97717\n","188/188 [==============================] - 28s 149ms/step - loss: 0.0623 - accuracy: 0.9816 - val_loss: 0.0758 - val_accuracy: 0.9761\n","Epoch 54/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9820\n","Epoch 00054: val_accuracy did not improve from 0.97717\n","188/188 [==============================] - 28s 149ms/step - loss: 0.0617 - accuracy: 0.9820 - val_loss: 0.0731 - val_accuracy: 0.9768\n","Epoch 55/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9816\n","Epoch 00055: val_accuracy did not improve from 0.97717\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0611 - accuracy: 0.9816 - val_loss: 0.0779 - val_accuracy: 0.9758\n","Epoch 56/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9815\n","Epoch 00056: val_accuracy did not improve from 0.97717\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0605 - accuracy: 0.9815 - val_loss: 0.0744 - val_accuracy: 0.9772\n","Epoch 57/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9823\n","Epoch 00057: val_accuracy did not improve from 0.97717\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0601 - accuracy: 0.9823 - val_loss: 0.0802 - val_accuracy: 0.9747\n","Epoch 58/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9826\n","Epoch 00058: val_accuracy did not improve from 0.97717\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0594 - accuracy: 0.9826 - val_loss: 0.0798 - val_accuracy: 0.9751\n","Epoch 59/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9824\n","Epoch 00059: val_accuracy improved from 0.97717 to 0.97842, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0589 - accuracy: 0.9824 - val_loss: 0.0709 - val_accuracy: 0.9784\n","Epoch 60/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9831\n","Epoch 00060: val_accuracy improved from 0.97842 to 0.97917, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0579 - accuracy: 0.9831 - val_loss: 0.0690 - val_accuracy: 0.9792\n","Epoch 61/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9831\n","Epoch 00061: val_accuracy did not improve from 0.97917\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0577 - accuracy: 0.9831 - val_loss: 0.0718 - val_accuracy: 0.9772\n","Epoch 62/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9834\n","Epoch 00062: val_accuracy did not improve from 0.97917\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0568 - accuracy: 0.9834 - val_loss: 0.0732 - val_accuracy: 0.9778\n","Epoch 63/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9828\n","Epoch 00063: val_accuracy did not improve from 0.97917\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0566 - accuracy: 0.9828 - val_loss: 0.0721 - val_accuracy: 0.9779\n","Epoch 64/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9835\n","Epoch 00064: val_accuracy did not improve from 0.97917\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0561 - accuracy: 0.9835 - val_loss: 0.0702 - val_accuracy: 0.9787\n","Epoch 65/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9830\n","Epoch 00065: val_accuracy did not improve from 0.97917\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0558 - accuracy: 0.9830 - val_loss: 0.0703 - val_accuracy: 0.9778\n","Epoch 66/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9835\n","Epoch 00066: val_accuracy did not improve from 0.97917\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0548 - accuracy: 0.9835 - val_loss: 0.0715 - val_accuracy: 0.9771\n","Epoch 67/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9845\n","Epoch 00067: val_accuracy improved from 0.97917 to 0.97967, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0544 - accuracy: 0.9845 - val_loss: 0.0678 - val_accuracy: 0.9797\n","Epoch 68/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9836\n","Epoch 00068: val_accuracy did not improve from 0.97967\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0545 - accuracy: 0.9836 - val_loss: 0.0684 - val_accuracy: 0.9793\n","Epoch 69/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9843\n","Epoch 00069: val_accuracy did not improve from 0.97967\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0537 - accuracy: 0.9843 - val_loss: 0.0672 - val_accuracy: 0.9793\n","Epoch 70/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9845\n","Epoch 00070: val_accuracy did not improve from 0.97967\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0530 - accuracy: 0.9845 - val_loss: 0.0680 - val_accuracy: 0.9787\n","Epoch 71/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9844\n","Epoch 00071: val_accuracy did not improve from 0.97967\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0530 - accuracy: 0.9844 - val_loss: 0.0693 - val_accuracy: 0.9788\n","Epoch 72/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9846\n","Epoch 00072: val_accuracy did not improve from 0.97967\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0523 - accuracy: 0.9846 - val_loss: 0.0689 - val_accuracy: 0.9781\n","Epoch 73/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9841\n","Epoch 00073: val_accuracy improved from 0.97967 to 0.98025, saving model to mnist_2conv_best.h5\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0521 - accuracy: 0.9841 - val_loss: 0.0655 - val_accuracy: 0.9803\n","Epoch 74/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9843\n","Epoch 00074: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0513 - accuracy: 0.9843 - val_loss: 0.0673 - val_accuracy: 0.9787\n","Epoch 75/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9846\n","Epoch 00075: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0517 - accuracy: 0.9846 - val_loss: 0.0747 - val_accuracy: 0.9765\n","Epoch 76/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9851\n","Epoch 00076: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0510 - accuracy: 0.9851 - val_loss: 0.0665 - val_accuracy: 0.9803\n","Epoch 77/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9849\n","Epoch 00077: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0508 - accuracy: 0.9849 - val_loss: 0.0657 - val_accuracy: 0.9792\n","Epoch 78/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9848\n","Epoch 00078: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0501 - accuracy: 0.9848 - val_loss: 0.0767 - val_accuracy: 0.9773\n","Epoch 79/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9851\n","Epoch 00079: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0496 - accuracy: 0.9851 - val_loss: 0.0660 - val_accuracy: 0.9787\n","Epoch 80/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9850\n","Epoch 00080: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0495 - accuracy: 0.9850 - val_loss: 0.0632 - val_accuracy: 0.9801\n","Epoch 81/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9849\n","Epoch 00081: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 28s 147ms/step - loss: 0.0492 - accuracy: 0.9849 - val_loss: 0.0630 - val_accuracy: 0.9800\n","Epoch 82/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9853\n","Epoch 00082: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0485 - accuracy: 0.9853 - val_loss: 0.0632 - val_accuracy: 0.9798\n","Epoch 83/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9859\n","Epoch 00083: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 28s 148ms/step - loss: 0.0484 - accuracy: 0.9859 - val_loss: 0.0647 - val_accuracy: 0.9798\n","Epoch 00083: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"9fd830e4-e891-4145-c62a-3d78ef4cffdf","executionInfo":{"status":"ok","timestamp":1590762432737,"user_tz":-60,"elapsed":2834,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"kOvb99Ap1DMU","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["mnist_2conv_model.load_weights('mnist_2conv_best.h5')\n","loss, acc = mnist_2conv_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 2s 7ms/step - loss: 0.0525 - accuracy: 0.9834\n","Accuracy: 0.9833999872207642\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q_8hdJhRNpaS","colab_type":"code","colab":{}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_2conv_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_2conv_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_2conv_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_2conv_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0ui1wP1YGwIZ"},"source":["##### 3 Hidden Layers"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"2cd1ef48-ae81-43b5-fe94-f445cebd4ff3","executionInfo":{"status":"ok","timestamp":1590750324647,"user_tz":-60,"elapsed":1100,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"CjLCe5gPGwIb","colab":{"base_uri":"https://localhost:8080/","height":425}},"source":["mnist_3conv_model = tf.keras.Sequential(name='mnist_3cnn')\n","mnist_3conv_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_3conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution'))\n","mnist_3conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling'))\n","mnist_3conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution2'))\n","mnist_3conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling2'))\n","mnist_3conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution3'))\n","mnist_3conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling3'))\n","mnist_3conv_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_3conv_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_3conv_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_3conv_model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"mnist_3cnn\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","convolution (Conv2D)         (None, 28, 28, 16)        272       \n","_________________________________________________________________\n","pooling (MaxPooling2D)       (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","convolution2 (Conv2D)        (None, 14, 14, 16)        4112      \n","_________________________________________________________________\n","pooling2 (MaxPooling2D)      (None, 7, 7, 16)          0         \n","_________________________________________________________________\n","convolution3 (Conv2D)        (None, 7, 7, 16)          4112      \n","_________________________________________________________________\n","pooling3 (MaxPooling2D)      (None, 3, 3, 16)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 144)               0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                1450      \n","=================================================================\n","Total params: 9,946\n","Trainable params: 9,946\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"7acad29d-81e7-444c-d87e-980c1b567ef4","executionInfo":{"status":"ok","timestamp":1590755738985,"user_tz":-60,"elapsed":2946472,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"Bd7G9IlOHg_I","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_3conv_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_3conv_model_train = mnist_3conv_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","188/188 [==============================] - ETA: 0s - loss: 2.2548 - accuracy: 0.1564\n","Epoch 00001: val_accuracy improved from -inf to 0.28933, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 2.2548 - accuracy: 0.1564 - val_loss: 2.1513 - val_accuracy: 0.2893\n","Epoch 2/10000\n","188/188 [==============================] - ETA: 0s - loss: 1.5277 - accuracy: 0.5924\n","Epoch 00002: val_accuracy improved from 0.28933 to 0.75683, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 1.5277 - accuracy: 0.5924 - val_loss: 0.7861 - val_accuracy: 0.7568\n","Epoch 3/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.8266\n","Epoch 00003: val_accuracy improved from 0.75683 to 0.84933, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.5739 - accuracy: 0.8266 - val_loss: 0.4786 - val_accuracy: 0.8493\n","Epoch 4/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3880 - accuracy: 0.8815\n","Epoch 00004: val_accuracy improved from 0.84933 to 0.87467, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.3880 - accuracy: 0.8815 - val_loss: 0.3890 - val_accuracy: 0.8747\n","Epoch 5/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3109 - accuracy: 0.9060\n","Epoch 00005: val_accuracy improved from 0.87467 to 0.91317, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.3109 - accuracy: 0.9060 - val_loss: 0.2883 - val_accuracy: 0.9132\n","Epoch 6/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.9200\n","Epoch 00006: val_accuracy improved from 0.91317 to 0.92192, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.2658 - accuracy: 0.9200 - val_loss: 0.2662 - val_accuracy: 0.9219\n","Epoch 7/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2376 - accuracy: 0.9287\n","Epoch 00007: val_accuracy did not improve from 0.92192\n","188/188 [==============================] - 31s 166ms/step - loss: 0.2376 - accuracy: 0.9287 - val_loss: 0.2590 - val_accuracy: 0.9212\n","Epoch 8/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 0.9347\n","Epoch 00008: val_accuracy improved from 0.92192 to 0.92875, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.2164 - accuracy: 0.9347 - val_loss: 0.2285 - val_accuracy: 0.9287\n","Epoch 9/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9393\n","Epoch 00009: val_accuracy improved from 0.92875 to 0.93658, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.2003 - accuracy: 0.9393 - val_loss: 0.2092 - val_accuracy: 0.9366\n","Epoch 10/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9434\n","Epoch 00010: val_accuracy improved from 0.93658 to 0.94333, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1874 - accuracy: 0.9434 - val_loss: 0.1928 - val_accuracy: 0.9433\n","Epoch 11/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9458\n","Epoch 00011: val_accuracy improved from 0.94333 to 0.94442, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.1777 - accuracy: 0.9458 - val_loss: 0.1883 - val_accuracy: 0.9444\n","Epoch 12/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9492\n","Epoch 00012: val_accuracy improved from 0.94442 to 0.95017, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.1677 - accuracy: 0.9492 - val_loss: 0.1723 - val_accuracy: 0.9502\n","Epoch 13/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1595 - accuracy: 0.9517\n","Epoch 00013: val_accuracy did not improve from 0.95017\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1595 - accuracy: 0.9517 - val_loss: 0.1676 - val_accuracy: 0.9495\n","Epoch 14/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1501 - accuracy: 0.9546\n","Epoch 00014: val_accuracy did not improve from 0.95017\n","188/188 [==============================] - 31s 166ms/step - loss: 0.1501 - accuracy: 0.9546 - val_loss: 0.1675 - val_accuracy: 0.9498\n","Epoch 15/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1449 - accuracy: 0.9563\n","Epoch 00015: val_accuracy did not improve from 0.95017\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1449 - accuracy: 0.9563 - val_loss: 0.1644 - val_accuracy: 0.9489\n","Epoch 16/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9573\n","Epoch 00016: val_accuracy improved from 0.95017 to 0.95200, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1403 - accuracy: 0.9573 - val_loss: 0.1605 - val_accuracy: 0.9520\n","Epoch 17/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.9593\n","Epoch 00017: val_accuracy improved from 0.95200 to 0.95392, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1349 - accuracy: 0.9593 - val_loss: 0.1550 - val_accuracy: 0.9539\n","Epoch 18/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9605\n","Epoch 00018: val_accuracy improved from 0.95392 to 0.95608, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1301 - accuracy: 0.9605 - val_loss: 0.1453 - val_accuracy: 0.9561\n","Epoch 19/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9617\n","Epoch 00019: val_accuracy did not improve from 0.95608\n","188/188 [==============================] - 35s 186ms/step - loss: 0.1263 - accuracy: 0.9617 - val_loss: 0.1540 - val_accuracy: 0.9517\n","Epoch 20/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9631\n","Epoch 00020: val_accuracy improved from 0.95608 to 0.96050, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1225 - accuracy: 0.9631 - val_loss: 0.1351 - val_accuracy: 0.9605\n","Epoch 21/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1189 - accuracy: 0.9637\n","Epoch 00021: val_accuracy did not improve from 0.96050\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1189 - accuracy: 0.9637 - val_loss: 0.1318 - val_accuracy: 0.9592\n","Epoch 22/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9649\n","Epoch 00022: val_accuracy did not improve from 0.96050\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1154 - accuracy: 0.9649 - val_loss: 0.1377 - val_accuracy: 0.9605\n","Epoch 23/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.9653\n","Epoch 00023: val_accuracy improved from 0.96050 to 0.96058, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1131 - accuracy: 0.9653 - val_loss: 0.1267 - val_accuracy: 0.9606\n","Epoch 24/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9672\n","Epoch 00024: val_accuracy did not improve from 0.96058\n","188/188 [==============================] - 31s 166ms/step - loss: 0.1099 - accuracy: 0.9672 - val_loss: 0.1262 - val_accuracy: 0.9606\n","Epoch 25/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9668\n","Epoch 00025: val_accuracy improved from 0.96058 to 0.96383, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1074 - accuracy: 0.9668 - val_loss: 0.1247 - val_accuracy: 0.9638\n","Epoch 26/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9680\n","Epoch 00026: val_accuracy improved from 0.96383 to 0.96642, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1051 - accuracy: 0.9680 - val_loss: 0.1132 - val_accuracy: 0.9664\n","Epoch 27/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9683\n","Epoch 00027: val_accuracy did not improve from 0.96642\n","188/188 [==============================] - 31s 167ms/step - loss: 0.1016 - accuracy: 0.9683 - val_loss: 0.1565 - val_accuracy: 0.9521\n","Epoch 28/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9690\n","Epoch 00028: val_accuracy improved from 0.96642 to 0.96708, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.1003 - accuracy: 0.9690 - val_loss: 0.1094 - val_accuracy: 0.9671\n","Epoch 29/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9693\n","Epoch 00029: val_accuracy did not improve from 0.96708\n","188/188 [==============================] - 31s 167ms/step - loss: 0.0973 - accuracy: 0.9693 - val_loss: 0.1195 - val_accuracy: 0.9645\n","Epoch 30/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0954 - accuracy: 0.9703\n","Epoch 00030: val_accuracy did not improve from 0.96708\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0954 - accuracy: 0.9703 - val_loss: 0.1131 - val_accuracy: 0.9670\n","Epoch 31/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9707\n","Epoch 00031: val_accuracy improved from 0.96708 to 0.96783, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0936 - accuracy: 0.9707 - val_loss: 0.1077 - val_accuracy: 0.9678\n","Epoch 32/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9717\n","Epoch 00032: val_accuracy improved from 0.96783 to 0.96942, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0919 - accuracy: 0.9717 - val_loss: 0.1032 - val_accuracy: 0.9694\n","Epoch 33/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9718\n","Epoch 00033: val_accuracy did not improve from 0.96942\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0903 - accuracy: 0.9718 - val_loss: 0.1060 - val_accuracy: 0.9679\n","Epoch 34/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9726\n","Epoch 00034: val_accuracy did not improve from 0.96942\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0877 - accuracy: 0.9726 - val_loss: 0.1033 - val_accuracy: 0.9682\n","Epoch 35/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9731\n","Epoch 00035: val_accuracy did not improve from 0.96942\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0863 - accuracy: 0.9731 - val_loss: 0.0989 - val_accuracy: 0.9693\n","Epoch 36/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0848 - accuracy: 0.9735\n","Epoch 00036: val_accuracy improved from 0.96942 to 0.97033, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0848 - accuracy: 0.9735 - val_loss: 0.0979 - val_accuracy: 0.9703\n","Epoch 37/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9741\n","Epoch 00037: val_accuracy did not improve from 0.97033\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0834 - accuracy: 0.9741 - val_loss: 0.1176 - val_accuracy: 0.9630\n","Epoch 38/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9743\n","Epoch 00038: val_accuracy did not improve from 0.97033\n","188/188 [==============================] - 35s 187ms/step - loss: 0.0814 - accuracy: 0.9743 - val_loss: 0.1095 - val_accuracy: 0.9661\n","Epoch 39/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9747\n","Epoch 00039: val_accuracy did not improve from 0.97033\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0807 - accuracy: 0.9747 - val_loss: 0.1016 - val_accuracy: 0.9693\n","Epoch 40/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9754\n","Epoch 00040: val_accuracy improved from 0.97033 to 0.97183, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0788 - accuracy: 0.9754 - val_loss: 0.0957 - val_accuracy: 0.9718\n","Epoch 41/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9760\n","Epoch 00041: val_accuracy did not improve from 0.97183\n","188/188 [==============================] - 31s 165ms/step - loss: 0.0778 - accuracy: 0.9760 - val_loss: 0.1108 - val_accuracy: 0.9654\n","Epoch 42/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9762\n","Epoch 00042: val_accuracy did not improve from 0.97183\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0763 - accuracy: 0.9762 - val_loss: 0.1064 - val_accuracy: 0.9675\n","Epoch 43/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9764\n","Epoch 00043: val_accuracy improved from 0.97183 to 0.97333, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.0759 - accuracy: 0.9764 - val_loss: 0.0904 - val_accuracy: 0.9733\n","Epoch 44/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9762\n","Epoch 00044: val_accuracy did not improve from 0.97333\n","188/188 [==============================] - 31s 167ms/step - loss: 0.0742 - accuracy: 0.9762 - val_loss: 0.0914 - val_accuracy: 0.9722\n","Epoch 45/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9767\n","Epoch 00045: val_accuracy did not improve from 0.97333\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0738 - accuracy: 0.9767 - val_loss: 0.1155 - val_accuracy: 0.9651\n","Epoch 46/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9775\n","Epoch 00046: val_accuracy improved from 0.97333 to 0.97383, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0719 - accuracy: 0.9775 - val_loss: 0.0867 - val_accuracy: 0.9738\n","Epoch 47/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9779\n","Epoch 00047: val_accuracy improved from 0.97383 to 0.97442, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 166ms/step - loss: 0.0705 - accuracy: 0.9779 - val_loss: 0.0844 - val_accuracy: 0.9744\n","Epoch 48/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9781\n","Epoch 00048: val_accuracy did not improve from 0.97442\n","188/188 [==============================] - 31s 167ms/step - loss: 0.0696 - accuracy: 0.9781 - val_loss: 0.0843 - val_accuracy: 0.9743\n","Epoch 49/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9777\n","Epoch 00049: val_accuracy did not improve from 0.97442\n","188/188 [==============================] - 32s 169ms/step - loss: 0.0699 - accuracy: 0.9777 - val_loss: 0.0900 - val_accuracy: 0.9725\n","Epoch 50/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9785\n","Epoch 00050: val_accuracy did not improve from 0.97442\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0683 - accuracy: 0.9785 - val_loss: 0.0832 - val_accuracy: 0.9737\n","Epoch 51/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9790\n","Epoch 00051: val_accuracy did not improve from 0.97442\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0672 - accuracy: 0.9790 - val_loss: 0.0854 - val_accuracy: 0.9732\n","Epoch 52/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9790\n","Epoch 00052: val_accuracy improved from 0.97442 to 0.97583, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0665 - accuracy: 0.9790 - val_loss: 0.0832 - val_accuracy: 0.9758\n","Epoch 53/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9795\n","Epoch 00053: val_accuracy did not improve from 0.97583\n","188/188 [==============================] - 31s 167ms/step - loss: 0.0655 - accuracy: 0.9795 - val_loss: 0.0937 - val_accuracy: 0.9727\n","Epoch 54/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9796\n","Epoch 00054: val_accuracy did not improve from 0.97583\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0648 - accuracy: 0.9796 - val_loss: 0.0782 - val_accuracy: 0.9758\n","Epoch 55/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9800\n","Epoch 00055: val_accuracy did not improve from 0.97583\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0640 - accuracy: 0.9800 - val_loss: 0.0837 - val_accuracy: 0.9745\n","Epoch 56/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9801\n","Epoch 00056: val_accuracy did not improve from 0.97583\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0637 - accuracy: 0.9801 - val_loss: 0.1040 - val_accuracy: 0.9691\n","Epoch 57/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9804\n","Epoch 00057: val_accuracy did not improve from 0.97583\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0627 - accuracy: 0.9804 - val_loss: 0.0860 - val_accuracy: 0.9731\n","Epoch 58/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9809\n","Epoch 00058: val_accuracy did not improve from 0.97583\n","188/188 [==============================] - 32s 169ms/step - loss: 0.0616 - accuracy: 0.9809 - val_loss: 0.1050 - val_accuracy: 0.9689\n","Epoch 59/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9808\n","Epoch 00059: val_accuracy improved from 0.97583 to 0.97658, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0612 - accuracy: 0.9808 - val_loss: 0.0763 - val_accuracy: 0.9766\n","Epoch 60/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9809\n","Epoch 00060: val_accuracy did not improve from 0.97658\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0604 - accuracy: 0.9809 - val_loss: 0.0784 - val_accuracy: 0.9758\n","Epoch 61/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9810\n","Epoch 00061: val_accuracy did not improve from 0.97658\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0599 - accuracy: 0.9810 - val_loss: 0.0775 - val_accuracy: 0.9761\n","Epoch 62/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9819\n","Epoch 00062: val_accuracy did not improve from 0.97658\n","188/188 [==============================] - 31s 167ms/step - loss: 0.0586 - accuracy: 0.9819 - val_loss: 0.0888 - val_accuracy: 0.9725\n","Epoch 63/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9820\n","Epoch 00063: val_accuracy improved from 0.97658 to 0.97792, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.0581 - accuracy: 0.9820 - val_loss: 0.0773 - val_accuracy: 0.9779\n","Epoch 64/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9819\n","Epoch 00064: val_accuracy did not improve from 0.97792\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0582 - accuracy: 0.9819 - val_loss: 0.0747 - val_accuracy: 0.9768\n","Epoch 65/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9823\n","Epoch 00065: val_accuracy improved from 0.97792 to 0.97858, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0576 - accuracy: 0.9823 - val_loss: 0.0713 - val_accuracy: 0.9786\n","Epoch 66/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9818\n","Epoch 00066: val_accuracy did not improve from 0.97858\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0573 - accuracy: 0.9818 - val_loss: 0.0715 - val_accuracy: 0.9782\n","Epoch 67/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9828\n","Epoch 00067: val_accuracy did not improve from 0.97858\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0563 - accuracy: 0.9828 - val_loss: 0.0741 - val_accuracy: 0.9780\n","Epoch 68/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9820\n","Epoch 00068: val_accuracy improved from 0.97858 to 0.97933, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 32s 169ms/step - loss: 0.0557 - accuracy: 0.9820 - val_loss: 0.0725 - val_accuracy: 0.9793\n","Epoch 69/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9823\n","Epoch 00069: val_accuracy did not improve from 0.97933\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0554 - accuracy: 0.9823 - val_loss: 0.0700 - val_accuracy: 0.9789\n","Epoch 70/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9835\n","Epoch 00070: val_accuracy did not improve from 0.97933\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0543 - accuracy: 0.9835 - val_loss: 0.0748 - val_accuracy: 0.9768\n","Epoch 71/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9829\n","Epoch 00071: val_accuracy did not improve from 0.97933\n","188/188 [==============================] - 31s 168ms/step - loss: 0.0543 - accuracy: 0.9829 - val_loss: 0.0715 - val_accuracy: 0.9790\n","Epoch 72/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9830\n","Epoch 00072: val_accuracy improved from 0.97933 to 0.97975, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0537 - accuracy: 0.9830 - val_loss: 0.0697 - val_accuracy: 0.9797\n","Epoch 73/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9833\n","Epoch 00073: val_accuracy did not improve from 0.97975\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0531 - accuracy: 0.9833 - val_loss: 0.0708 - val_accuracy: 0.9791\n","Epoch 74/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9836\n","Epoch 00074: val_accuracy did not improve from 0.97975\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0527 - accuracy: 0.9836 - val_loss: 0.0700 - val_accuracy: 0.9794\n","Epoch 75/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9838\n","Epoch 00075: val_accuracy did not improve from 0.97975\n","188/188 [==============================] - 31s 167ms/step - loss: 0.0521 - accuracy: 0.9838 - val_loss: 0.0711 - val_accuracy: 0.9784\n","Epoch 76/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9837\n","Epoch 00076: val_accuracy did not improve from 0.97975\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0520 - accuracy: 0.9837 - val_loss: 0.0711 - val_accuracy: 0.9784\n","Epoch 77/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9839\n","Epoch 00077: val_accuracy did not improve from 0.97975\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0517 - accuracy: 0.9839 - val_loss: 0.0711 - val_accuracy: 0.9784\n","Epoch 78/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9840\n","Epoch 00078: val_accuracy did not improve from 0.97975\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0510 - accuracy: 0.9840 - val_loss: 0.0757 - val_accuracy: 0.9760\n","Epoch 79/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9846\n","Epoch 00079: val_accuracy did not improve from 0.97975\n","188/188 [==============================] - 31s 167ms/step - loss: 0.0505 - accuracy: 0.9846 - val_loss: 0.0730 - val_accuracy: 0.9772\n","Epoch 80/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.9840\n","Epoch 00080: val_accuracy improved from 0.97975 to 0.97992, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0499 - accuracy: 0.9840 - val_loss: 0.0664 - val_accuracy: 0.9799\n","Epoch 81/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9851\n","Epoch 00081: val_accuracy did not improve from 0.97992\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0492 - accuracy: 0.9851 - val_loss: 0.0831 - val_accuracy: 0.9747\n","Epoch 82/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9850\n","Epoch 00082: val_accuracy improved from 0.97992 to 0.98000, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 31s 167ms/step - loss: 0.0493 - accuracy: 0.9850 - val_loss: 0.0689 - val_accuracy: 0.9800\n","Epoch 83/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9849\n","Epoch 00083: val_accuracy improved from 0.98000 to 0.98150, saving model to mnist_3conv_best.h5\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0492 - accuracy: 0.9849 - val_loss: 0.0639 - val_accuracy: 0.9815\n","Epoch 84/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9847\n","Epoch 00084: val_accuracy did not improve from 0.98150\n","188/188 [==============================] - 31s 167ms/step - loss: 0.0487 - accuracy: 0.9847 - val_loss: 0.0641 - val_accuracy: 0.9814\n","Epoch 85/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9851\n","Epoch 00085: val_accuracy did not improve from 0.98150\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0478 - accuracy: 0.9851 - val_loss: 0.0702 - val_accuracy: 0.9785\n","Epoch 86/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9850\n","Epoch 00086: val_accuracy did not improve from 0.98150\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0478 - accuracy: 0.9850 - val_loss: 0.0700 - val_accuracy: 0.9792\n","Epoch 87/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9858\n","Epoch 00087: val_accuracy did not improve from 0.98150\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0471 - accuracy: 0.9858 - val_loss: 0.0739 - val_accuracy: 0.9777\n","Epoch 88/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9854\n","Epoch 00088: val_accuracy did not improve from 0.98150\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0474 - accuracy: 0.9854 - val_loss: 0.0686 - val_accuracy: 0.9797\n","Epoch 89/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9858\n","Epoch 00089: val_accuracy did not improve from 0.98150\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0467 - accuracy: 0.9858 - val_loss: 0.0653 - val_accuracy: 0.9811\n","Epoch 90/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9855\n","Epoch 00090: val_accuracy did not improve from 0.98150\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0466 - accuracy: 0.9855 - val_loss: 0.0646 - val_accuracy: 0.9803\n","Epoch 91/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9861\n","Epoch 00091: val_accuracy did not improve from 0.98150\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0461 - accuracy: 0.9861 - val_loss: 0.0637 - val_accuracy: 0.9812\n","Epoch 92/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9857\n","Epoch 00092: val_accuracy did not improve from 0.98150\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0458 - accuracy: 0.9857 - val_loss: 0.0672 - val_accuracy: 0.9792\n","Epoch 93/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9858\n","Epoch 00093: val_accuracy did not improve from 0.98150\n","188/188 [==============================] - 32s 168ms/step - loss: 0.0453 - accuracy: 0.9858 - val_loss: 0.0650 - val_accuracy: 0.9801\n","Epoch 00093: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"055207c3-cc60-4966-ea21-62fd1965aa1c","executionInfo":{"status":"ok","timestamp":1590762375242,"user_tz":-60,"elapsed":3185,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"_fiwHvaK0t2C","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["mnist_3conv_model.load_weights('mnist_3conv_best.h5')\n","loss, acc = mnist_3conv_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 2s 8ms/step - loss: 0.0503 - accuracy: 0.9842\n","Accuracy: 0.9842000007629395\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"isn3AvjvOD--","colab_type":"code","colab":{}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_3conv_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_3conv_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_3conv_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_3conv_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zzaGgbND25iP"},"source":["##### 4 Hidden Layers"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"2283ae85-fb06-4d69-e209-ef3196e7c8ce","executionInfo":{"status":"ok","timestamp":1590774155126,"user_tz":-60,"elapsed":928,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"-ALiDLvw25iQ","colab":{"base_uri":"https://localhost:8080/","height":493}},"source":["mnist_4conv_model = tf.keras.Sequential(name='mnist_4cnn')\n","mnist_4conv_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_4conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution'))\n","mnist_4conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling'))\n","mnist_4conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution2'))\n","mnist_4conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling2'))\n","mnist_4conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution3'))\n","mnist_4conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling3'))\n","mnist_4conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution4'))\n","mnist_4conv_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling4'))\n","mnist_4conv_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_4conv_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_4conv_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_4conv_model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"mnist_4cnn\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","convolution (Conv2D)         (None, 28, 28, 16)        272       \n","_________________________________________________________________\n","pooling (MaxPooling2D)       (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","convolution2 (Conv2D)        (None, 14, 14, 16)        4112      \n","_________________________________________________________________\n","pooling2 (MaxPooling2D)      (None, 7, 7, 16)          0         \n","_________________________________________________________________\n","convolution3 (Conv2D)        (None, 7, 7, 16)          4112      \n","_________________________________________________________________\n","pooling3 (MaxPooling2D)      (None, 3, 3, 16)          0         \n","_________________________________________________________________\n","convolution4 (Conv2D)        (None, 3, 3, 16)          4112      \n","_________________________________________________________________\n","pooling4 (MaxPooling2D)      (None, 1, 1, 16)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 16)                0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                170       \n","=================================================================\n","Total params: 12,778\n","Trainable params: 12,778\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"5f3cb512-dacc-4064-bc2c-955975da412b","executionInfo":{"status":"ok","timestamp":1590776882791,"user_tz":-60,"elapsed":2728572,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"qSaru3i725iT","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_4conv_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_4conv_model_train = mnist_4conv_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","188/188 [==============================] - ETA: 0s - loss: 2.2857 - accuracy: 0.1653\n","Epoch 00001: val_accuracy improved from -inf to 0.19133, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 35s 184ms/step - loss: 2.2857 - accuracy: 0.1653 - val_loss: 2.2624 - val_accuracy: 0.1913\n","Epoch 2/10000\n","188/188 [==============================] - ETA: 0s - loss: 2.1822 - accuracy: 0.3062\n","Epoch 00002: val_accuracy improved from 0.19133 to 0.47433, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 182ms/step - loss: 2.1822 - accuracy: 0.3062 - val_loss: 1.9817 - val_accuracy: 0.4743\n","Epoch 3/10000\n","188/188 [==============================] - ETA: 0s - loss: 1.3815 - accuracy: 0.6084\n","Epoch 00003: val_accuracy improved from 0.47433 to 0.62950, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 180ms/step - loss: 1.3815 - accuracy: 0.6084 - val_loss: 1.1951 - val_accuracy: 0.6295\n","Epoch 4/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.6162 - accuracy: 0.8087\n","Epoch 00004: val_accuracy improved from 0.62950 to 0.84358, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 181ms/step - loss: 0.6162 - accuracy: 0.8087 - val_loss: 0.4896 - val_accuracy: 0.8436\n","Epoch 5/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3983 - accuracy: 0.8776\n","Epoch 00005: val_accuracy improved from 0.84358 to 0.89042, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 182ms/step - loss: 0.3983 - accuracy: 0.8776 - val_loss: 0.3510 - val_accuracy: 0.8904\n","Epoch 6/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2997 - accuracy: 0.9087\n","Epoch 00006: val_accuracy improved from 0.89042 to 0.91900, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 180ms/step - loss: 0.2997 - accuracy: 0.9087 - val_loss: 0.2693 - val_accuracy: 0.9190\n","Epoch 7/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.9231\n","Epoch 00007: val_accuracy improved from 0.91900 to 0.92167, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 181ms/step - loss: 0.2554 - accuracy: 0.9231 - val_loss: 0.2511 - val_accuracy: 0.9217\n","Epoch 8/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.9317\n","Epoch 00008: val_accuracy did not improve from 0.92167\n","188/188 [==============================] - 34s 180ms/step - loss: 0.2240 - accuracy: 0.9317 - val_loss: 0.2551 - val_accuracy: 0.9158\n","Epoch 9/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9384\n","Epoch 00009: val_accuracy improved from 0.92167 to 0.93358, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 180ms/step - loss: 0.2022 - accuracy: 0.9384 - val_loss: 0.2056 - val_accuracy: 0.9336\n","Epoch 10/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1832 - accuracy: 0.9440\n","Epoch 00010: val_accuracy improved from 0.93358 to 0.94292, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 180ms/step - loss: 0.1832 - accuracy: 0.9440 - val_loss: 0.1838 - val_accuracy: 0.9429\n","Epoch 11/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9476\n","Epoch 00011: val_accuracy improved from 0.94292 to 0.94917, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 181ms/step - loss: 0.1691 - accuracy: 0.9476 - val_loss: 0.1623 - val_accuracy: 0.9492\n","Epoch 12/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1594 - accuracy: 0.9510\n","Epoch 00012: val_accuracy improved from 0.94917 to 0.95125, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 180ms/step - loss: 0.1594 - accuracy: 0.9510 - val_loss: 0.1546 - val_accuracy: 0.9513\n","Epoch 13/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1476 - accuracy: 0.9545\n","Epoch 00013: val_accuracy improved from 0.95125 to 0.95217, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 180ms/step - loss: 0.1476 - accuracy: 0.9545 - val_loss: 0.1529 - val_accuracy: 0.9522\n","Epoch 14/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1394 - accuracy: 0.9571\n","Epoch 00014: val_accuracy improved from 0.95217 to 0.95592, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 180ms/step - loss: 0.1394 - accuracy: 0.9571 - val_loss: 0.1420 - val_accuracy: 0.9559\n","Epoch 15/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9589\n","Epoch 00015: val_accuracy did not improve from 0.95592\n","188/188 [==============================] - 34s 179ms/step - loss: 0.1322 - accuracy: 0.9589 - val_loss: 0.1647 - val_accuracy: 0.9503\n","Epoch 16/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9608\n","Epoch 00016: val_accuracy improved from 0.95592 to 0.96225, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 179ms/step - loss: 0.1260 - accuracy: 0.9608 - val_loss: 0.1275 - val_accuracy: 0.9622\n","Epoch 17/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1199 - accuracy: 0.9629\n","Epoch 00017: val_accuracy did not improve from 0.96225\n","188/188 [==============================] - 34s 180ms/step - loss: 0.1199 - accuracy: 0.9629 - val_loss: 0.1366 - val_accuracy: 0.9588\n","Epoch 18/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9646\n","Epoch 00018: val_accuracy did not improve from 0.96225\n","188/188 [==============================] - 34s 179ms/step - loss: 0.1148 - accuracy: 0.9646 - val_loss: 0.1351 - val_accuracy: 0.9577\n","Epoch 19/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9657\n","Epoch 00019: val_accuracy improved from 0.96225 to 0.96417, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 179ms/step - loss: 0.1108 - accuracy: 0.9657 - val_loss: 0.1121 - val_accuracy: 0.9642\n","Epoch 20/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1067 - accuracy: 0.9676\n","Epoch 00020: val_accuracy did not improve from 0.96417\n","188/188 [==============================] - 34s 180ms/step - loss: 0.1067 - accuracy: 0.9676 - val_loss: 0.1222 - val_accuracy: 0.9609\n","Epoch 21/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9697\n","Epoch 00021: val_accuracy improved from 0.96417 to 0.96717, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 180ms/step - loss: 0.1018 - accuracy: 0.9697 - val_loss: 0.1076 - val_accuracy: 0.9672\n","Epoch 22/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9695\n","Epoch 00022: val_accuracy did not improve from 0.96717\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0997 - accuracy: 0.9695 - val_loss: 0.1195 - val_accuracy: 0.9637\n","Epoch 23/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9704\n","Epoch 00023: val_accuracy improved from 0.96717 to 0.96792, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 180ms/step - loss: 0.0967 - accuracy: 0.9704 - val_loss: 0.1041 - val_accuracy: 0.9679\n","Epoch 24/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9712\n","Epoch 00024: val_accuracy did not improve from 0.96792\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0947 - accuracy: 0.9712 - val_loss: 0.1196 - val_accuracy: 0.9634\n","Epoch 25/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9722\n","Epoch 00025: val_accuracy did not improve from 0.96792\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0917 - accuracy: 0.9722 - val_loss: 0.1073 - val_accuracy: 0.9668\n","Epoch 26/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9726\n","Epoch 00026: val_accuracy improved from 0.96792 to 0.96958, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 180ms/step - loss: 0.0888 - accuracy: 0.9726 - val_loss: 0.0971 - val_accuracy: 0.9696\n","Epoch 27/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9735\n","Epoch 00027: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0862 - accuracy: 0.9735 - val_loss: 0.1081 - val_accuracy: 0.9678\n","Epoch 28/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9738\n","Epoch 00028: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0850 - accuracy: 0.9738 - val_loss: 0.1002 - val_accuracy: 0.9672\n","Epoch 29/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9744\n","Epoch 00029: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 33s 178ms/step - loss: 0.0820 - accuracy: 0.9744 - val_loss: 0.1092 - val_accuracy: 0.9665\n","Epoch 30/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9746\n","Epoch 00030: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 33s 178ms/step - loss: 0.0813 - accuracy: 0.9746 - val_loss: 0.1021 - val_accuracy: 0.9674\n","Epoch 31/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9760\n","Epoch 00031: val_accuracy improved from 0.96958 to 0.97208, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 178ms/step - loss: 0.0783 - accuracy: 0.9760 - val_loss: 0.0894 - val_accuracy: 0.9721\n","Epoch 32/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9761\n","Epoch 00032: val_accuracy did not improve from 0.97208\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0772 - accuracy: 0.9761 - val_loss: 0.1122 - val_accuracy: 0.9642\n","Epoch 33/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9762\n","Epoch 00033: val_accuracy did not improve from 0.97208\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0761 - accuracy: 0.9762 - val_loss: 0.1030 - val_accuracy: 0.9671\n","Epoch 34/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9776\n","Epoch 00034: val_accuracy did not improve from 0.97208\n","188/188 [==============================] - 34s 180ms/step - loss: 0.0737 - accuracy: 0.9776 - val_loss: 0.0897 - val_accuracy: 0.9721\n","Epoch 35/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9776\n","Epoch 00035: val_accuracy improved from 0.97208 to 0.97408, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0729 - accuracy: 0.9776 - val_loss: 0.0821 - val_accuracy: 0.9741\n","Epoch 36/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9782\n","Epoch 00036: val_accuracy did not improve from 0.97408\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0701 - accuracy: 0.9782 - val_loss: 0.0895 - val_accuracy: 0.9729\n","Epoch 37/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9790\n","Epoch 00037: val_accuracy improved from 0.97408 to 0.97425, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0690 - accuracy: 0.9790 - val_loss: 0.0802 - val_accuracy: 0.9743\n","Epoch 38/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9787\n","Epoch 00038: val_accuracy did not improve from 0.97425\n","188/188 [==============================] - 34s 182ms/step - loss: 0.0688 - accuracy: 0.9787 - val_loss: 0.0842 - val_accuracy: 0.9741\n","Epoch 39/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9794\n","Epoch 00039: val_accuracy did not improve from 0.97425\n","188/188 [==============================] - 34s 183ms/step - loss: 0.0673 - accuracy: 0.9794 - val_loss: 0.0962 - val_accuracy: 0.9692\n","Epoch 40/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9800\n","Epoch 00040: val_accuracy did not improve from 0.97425\n","188/188 [==============================] - 34s 181ms/step - loss: 0.0659 - accuracy: 0.9800 - val_loss: 0.0818 - val_accuracy: 0.9742\n","Epoch 41/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9805\n","Epoch 00041: val_accuracy did not improve from 0.97425\n","188/188 [==============================] - 34s 181ms/step - loss: 0.0642 - accuracy: 0.9805 - val_loss: 0.0988 - val_accuracy: 0.9693\n","Epoch 42/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9801\n","Epoch 00042: val_accuracy improved from 0.97425 to 0.97600, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0639 - accuracy: 0.9801 - val_loss: 0.0769 - val_accuracy: 0.9760\n","Epoch 43/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9808\n","Epoch 00043: val_accuracy did not improve from 0.97600\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0625 - accuracy: 0.9808 - val_loss: 0.0756 - val_accuracy: 0.9760\n","Epoch 44/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9809\n","Epoch 00044: val_accuracy improved from 0.97600 to 0.97675, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 178ms/step - loss: 0.0614 - accuracy: 0.9809 - val_loss: 0.0740 - val_accuracy: 0.9768\n","Epoch 45/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9814\n","Epoch 00045: val_accuracy did not improve from 0.97675\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0605 - accuracy: 0.9814 - val_loss: 0.0777 - val_accuracy: 0.9754\n","Epoch 46/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9810\n","Epoch 00046: val_accuracy improved from 0.97675 to 0.97758, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 181ms/step - loss: 0.0605 - accuracy: 0.9810 - val_loss: 0.0728 - val_accuracy: 0.9776\n","Epoch 47/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9816\n","Epoch 00047: val_accuracy did not improve from 0.97758\n","188/188 [==============================] - 34s 183ms/step - loss: 0.0590 - accuracy: 0.9816 - val_loss: 0.1508 - val_accuracy: 0.9537\n","Epoch 48/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0585 - accuracy: 0.9822\n","Epoch 00048: val_accuracy did not improve from 0.97758\n","188/188 [==============================] - 34s 183ms/step - loss: 0.0585 - accuracy: 0.9822 - val_loss: 0.0784 - val_accuracy: 0.9753\n","Epoch 49/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9818\n","Epoch 00049: val_accuracy did not improve from 0.97758\n","188/188 [==============================] - 34s 181ms/step - loss: 0.0577 - accuracy: 0.9818 - val_loss: 0.0757 - val_accuracy: 0.9772\n","Epoch 50/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9829\n","Epoch 00050: val_accuracy improved from 0.97758 to 0.97833, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 39s 208ms/step - loss: 0.0571 - accuracy: 0.9829 - val_loss: 0.0713 - val_accuracy: 0.9783\n","Epoch 51/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9826\n","Epoch 00051: val_accuracy did not improve from 0.97833\n","188/188 [==============================] - 34s 181ms/step - loss: 0.0562 - accuracy: 0.9826 - val_loss: 0.0710 - val_accuracy: 0.9772\n","Epoch 52/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9830\n","Epoch 00052: val_accuracy did not improve from 0.97833\n","188/188 [==============================] - 34s 182ms/step - loss: 0.0551 - accuracy: 0.9830 - val_loss: 0.0720 - val_accuracy: 0.9761\n","Epoch 53/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9833\n","Epoch 00053: val_accuracy did not improve from 0.97833\n","188/188 [==============================] - 34s 180ms/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0741 - val_accuracy: 0.9762\n","Epoch 54/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9837\n","Epoch 00054: val_accuracy did not improve from 0.97833\n","188/188 [==============================] - 34s 182ms/step - loss: 0.0537 - accuracy: 0.9837 - val_loss: 0.0797 - val_accuracy: 0.9742\n","Epoch 55/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9827\n","Epoch 00055: val_accuracy did not improve from 0.97833\n","188/188 [==============================] - 34s 180ms/step - loss: 0.0538 - accuracy: 0.9827 - val_loss: 0.0713 - val_accuracy: 0.9782\n","Epoch 56/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9836\n","Epoch 00056: val_accuracy did not improve from 0.97833\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0524 - accuracy: 0.9836 - val_loss: 0.0836 - val_accuracy: 0.9741\n","Epoch 57/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9830\n","Epoch 00057: val_accuracy improved from 0.97833 to 0.97858, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0531 - accuracy: 0.9830 - val_loss: 0.0683 - val_accuracy: 0.9786\n","Epoch 58/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9837\n","Epoch 00058: val_accuracy did not improve from 0.97858\n","188/188 [==============================] - 33s 178ms/step - loss: 0.0525 - accuracy: 0.9837 - val_loss: 0.0795 - val_accuracy: 0.9747\n","Epoch 59/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9844\n","Epoch 00059: val_accuracy improved from 0.97858 to 0.98025, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 178ms/step - loss: 0.0514 - accuracy: 0.9844 - val_loss: 0.0660 - val_accuracy: 0.9803\n","Epoch 60/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9843\n","Epoch 00060: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0502 - accuracy: 0.9843 - val_loss: 0.0730 - val_accuracy: 0.9774\n","Epoch 61/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9846\n","Epoch 00061: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 33s 178ms/step - loss: 0.0493 - accuracy: 0.9846 - val_loss: 0.0766 - val_accuracy: 0.9763\n","Epoch 62/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9848\n","Epoch 00062: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 34s 178ms/step - loss: 0.0489 - accuracy: 0.9848 - val_loss: 0.0680 - val_accuracy: 0.9785\n","Epoch 63/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9850\n","Epoch 00063: val_accuracy did not improve from 0.98025\n","188/188 [==============================] - 34s 178ms/step - loss: 0.0486 - accuracy: 0.9850 - val_loss: 0.0689 - val_accuracy: 0.9788\n","Epoch 64/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9850\n","Epoch 00064: val_accuracy improved from 0.98025 to 0.98050, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0486 - accuracy: 0.9850 - val_loss: 0.0648 - val_accuracy: 0.9805\n","Epoch 65/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9862\n","Epoch 00065: val_accuracy did not improve from 0.98050\n","188/188 [==============================] - 33s 178ms/step - loss: 0.0471 - accuracy: 0.9862 - val_loss: 0.0718 - val_accuracy: 0.9776\n","Epoch 66/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9859\n","Epoch 00066: val_accuracy did not improve from 0.98050\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0471 - accuracy: 0.9859 - val_loss: 0.0661 - val_accuracy: 0.9793\n","Epoch 67/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9858\n","Epoch 00067: val_accuracy did not improve from 0.98050\n","188/188 [==============================] - 34s 178ms/step - loss: 0.0465 - accuracy: 0.9858 - val_loss: 0.0788 - val_accuracy: 0.9753\n","Epoch 68/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9861\n","Epoch 00068: val_accuracy did not improve from 0.98050\n","188/188 [==============================] - 39s 205ms/step - loss: 0.0461 - accuracy: 0.9861 - val_loss: 0.0650 - val_accuracy: 0.9795\n","Epoch 69/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9862\n","Epoch 00069: val_accuracy did not improve from 0.98050\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0452 - accuracy: 0.9862 - val_loss: 0.0667 - val_accuracy: 0.9787\n","Epoch 70/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9859\n","Epoch 00070: val_accuracy improved from 0.98050 to 0.98117, saving model to mnist_4conv_best.h5\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0454 - accuracy: 0.9859 - val_loss: 0.0638 - val_accuracy: 0.9812\n","Epoch 71/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9858\n","Epoch 00071: val_accuracy did not improve from 0.98117\n","188/188 [==============================] - 34s 178ms/step - loss: 0.0451 - accuracy: 0.9858 - val_loss: 0.0695 - val_accuracy: 0.9793\n","Epoch 72/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9865\n","Epoch 00072: val_accuracy did not improve from 0.98117\n","188/188 [==============================] - 34s 178ms/step - loss: 0.0442 - accuracy: 0.9865 - val_loss: 0.0663 - val_accuracy: 0.9789\n","Epoch 73/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9870\n","Epoch 00073: val_accuracy did not improve from 0.98117\n","188/188 [==============================] - 33s 178ms/step - loss: 0.0431 - accuracy: 0.9870 - val_loss: 0.0648 - val_accuracy: 0.9793\n","Epoch 74/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9870\n","Epoch 00074: val_accuracy did not improve from 0.98117\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0436 - accuracy: 0.9870 - val_loss: 0.0629 - val_accuracy: 0.9797\n","Epoch 75/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9875\n","Epoch 00075: val_accuracy did not improve from 0.98117\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0422 - accuracy: 0.9875 - val_loss: 0.0754 - val_accuracy: 0.9771\n","Epoch 76/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9874\n","Epoch 00076: val_accuracy did not improve from 0.98117\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0424 - accuracy: 0.9874 - val_loss: 0.0625 - val_accuracy: 0.9803\n","Epoch 77/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9875\n","Epoch 00077: val_accuracy did not improve from 0.98117\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0418 - accuracy: 0.9875 - val_loss: 0.1388 - val_accuracy: 0.9592\n","Epoch 78/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9868\n","Epoch 00078: val_accuracy did not improve from 0.98117\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0433 - accuracy: 0.9868 - val_loss: 0.0608 - val_accuracy: 0.9811\n","Epoch 79/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9878\n","Epoch 00079: val_accuracy did not improve from 0.98117\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0410 - accuracy: 0.9878 - val_loss: 0.0601 - val_accuracy: 0.9802\n","Epoch 80/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9876\n","Epoch 00080: val_accuracy did not improve from 0.98117\n","188/188 [==============================] - 34s 179ms/step - loss: 0.0406 - accuracy: 0.9876 - val_loss: 0.0663 - val_accuracy: 0.9790\n","Epoch 00080: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"4aaf4c8e-c621-4e8c-cc8b-372b4bdbacf1","executionInfo":{"status":"ok","timestamp":1590776885782,"user_tz":-60,"elapsed":2731542,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"CjaegkV825iW","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["mnist_4conv_model.load_weights('mnist_4conv_best.h5')\n","loss, acc = mnist_4conv_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 3s 10ms/step - loss: 0.0542 - accuracy: 0.9820\n","Accuracy: 0.9819999933242798\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RJEHAceoOM8_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":673},"outputId":"3f63b922-f83a-4b11-8158-cd42d384b8e0","executionInfo":{"status":"error","timestamp":1590953631835,"user_tz":-60,"elapsed":881,"user":{"displayName":"Pedro Reis","photoUrl":"","userId":"06768152741974362425"}}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_4conv_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_4conv_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_4conv_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_4conv_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":51,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-fff0d8516e5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloss_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_3conv_model_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mloss_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_3conv_model_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'mnist_3conv_model_train' is not defined"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"u0yoRqR0HFcp","colab_type":"text"},"source":["#### Regularization\n"]},{"cell_type":"markdown","metadata":{"id":"IwzYtF5FHWbA","colab_type":"text"},"source":["##### **Weight Regularization**\n","The Occam's Razor principle states: given two explanations for something, the explanation most likely to be correct is the \"simplest\" one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simpler models are less likely to overfit than complex ones.\n","\n","A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more \"regular\". This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:\n","\n","*  **L1 regularization**, where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n","*   **L2 regularization**, where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the squared \"L2 norm\" of the weights). L2 regularization is also called weight decay in the context of neural networks. \n","\n","L1 regularization pushes weights towards exactly zero encouraging a sparse model. L2 regularization will penalize the weights parameters without making them sparse since the penalty goes to zero for small weights, one reason why L2 is more common.\n"]},{"cell_type":"markdown","metadata":{"id":"MvlUACQETzuN","colab_type":"text"},"source":["Let's then proceed in making a L2 regularization."]},{"cell_type":"markdown","metadata":{"id":"PDevL6kwsg2p","colab_type":"text"},"source":["**kernel_regularizer:** Regularizer to apply a penalty on the layer's kernel\n"]},{"cell_type":"code","metadata":{"id":"ccnvUGJMT4Sf","colab_type":"code","outputId":"a226389e-c890-4c9b-dbb4-e09f5eac817c","executionInfo":{"status":"ok","timestamp":1590680135076,"user_tz":-60,"elapsed":54395,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["mnist_l2_model = tf.keras.Sequential(name='mnist_l2_cnn')\n","mnist_l2_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_l2_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n","mnist_l2_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling'))\n","mnist_l2_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_l2_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_l2_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_l2_model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"mnist_cnn\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","convolution (Conv2D)         (None, 28, 28, 16)        272       \n","_________________________________________________________________\n","pooling (MaxPooling2D)       (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 3136)              0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                31370     \n","=================================================================\n","Total params: 31,642\n","Trainable params: 31,642\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"2606cd02-02e9-40b0-a53f-179fa05fdca3","executionInfo":{"status":"ok","timestamp":1590684142987,"user_tz":-60,"elapsed":4007895,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"htWtb5kPtGNx","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_l2_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_l2_model_train = mnist_l2_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","187/188 [============================>.] - ETA: 0s - loss: 1.9535 - accuracy: 0.4783\n","Epoch 00001: val_accuracy improved from -inf to 0.77767, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 1.9515 - accuracy: 0.4789 - val_loss: 1.1546 - val_accuracy: 0.7777\n","Epoch 2/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.7455 - accuracy: 0.8356\n","Epoch 00002: val_accuracy improved from 0.77767 to 0.86108, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.7449 - accuracy: 0.8358 - val_loss: 0.5627 - val_accuracy: 0.8611\n","Epoch 3/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.5087 - accuracy: 0.8745\n","Epoch 00003: val_accuracy improved from 0.86108 to 0.87917, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.5085 - accuracy: 0.8745 - val_loss: 0.4756 - val_accuracy: 0.8792\n","Epoch 4/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4534 - accuracy: 0.8859\n","Epoch 00004: val_accuracy improved from 0.87917 to 0.89067, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.4533 - accuracy: 0.8860 - val_loss: 0.4394 - val_accuracy: 0.8907\n","Epoch 5/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4266 - accuracy: 0.8935\n","Epoch 00005: val_accuracy improved from 0.89067 to 0.89508, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.4269 - accuracy: 0.8935 - val_loss: 0.4202 - val_accuracy: 0.8951\n","Epoch 6/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4111 - accuracy: 0.8974\n","Epoch 00006: val_accuracy improved from 0.89508 to 0.89792, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.4108 - accuracy: 0.8975 - val_loss: 0.4088 - val_accuracy: 0.8979\n","Epoch 7/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3994 - accuracy: 0.9006\n","Epoch 00007: val_accuracy improved from 0.89792 to 0.90175, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.3993 - accuracy: 0.9007 - val_loss: 0.3988 - val_accuracy: 0.9018\n","Epoch 8/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3901 - accuracy: 0.9032\n","Epoch 00008: val_accuracy improved from 0.90175 to 0.90400, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3901 - accuracy: 0.9032 - val_loss: 0.3897 - val_accuracy: 0.9040\n","Epoch 9/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3826 - accuracy: 0.9049\n","Epoch 00009: val_accuracy improved from 0.90400 to 0.90583, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3826 - accuracy: 0.9048 - val_loss: 0.3847 - val_accuracy: 0.9058\n","Epoch 10/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3761 - accuracy: 0.9066\n","Epoch 00010: val_accuracy improved from 0.90583 to 0.90717, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3761 - accuracy: 0.9066 - val_loss: 0.3778 - val_accuracy: 0.9072\n","Epoch 11/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3702 - accuracy: 0.9080\n","Epoch 00011: val_accuracy improved from 0.90717 to 0.90742, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3702 - accuracy: 0.9080 - val_loss: 0.3744 - val_accuracy: 0.9074\n","Epoch 12/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3653 - accuracy: 0.9085\n","Epoch 00012: val_accuracy improved from 0.90742 to 0.91108, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3653 - accuracy: 0.9085 - val_loss: 0.3690 - val_accuracy: 0.9111\n","Epoch 13/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3604 - accuracy: 0.9104\n","Epoch 00013: val_accuracy did not improve from 0.91108\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3603 - accuracy: 0.9105 - val_loss: 0.3643 - val_accuracy: 0.9095\n","Epoch 14/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3562 - accuracy: 0.9112\n","Epoch 00014: val_accuracy improved from 0.91108 to 0.91133, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3560 - accuracy: 0.9112 - val_loss: 0.3606 - val_accuracy: 0.9113\n","Epoch 15/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3522 - accuracy: 0.9126\n","Epoch 00015: val_accuracy improved from 0.91133 to 0.91208, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3522 - accuracy: 0.9126 - val_loss: 0.3573 - val_accuracy: 0.9121\n","Epoch 16/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3485 - accuracy: 0.9131\n","Epoch 00016: val_accuracy improved from 0.91208 to 0.91283, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3485 - accuracy: 0.9131 - val_loss: 0.3529 - val_accuracy: 0.9128\n","Epoch 17/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3453 - accuracy: 0.9129\n","Epoch 00017: val_accuracy improved from 0.91283 to 0.91292, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3451 - accuracy: 0.9129 - val_loss: 0.3518 - val_accuracy: 0.9129\n","Epoch 18/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3419 - accuracy: 0.9152\n","Epoch 00018: val_accuracy improved from 0.91292 to 0.91467, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3418 - accuracy: 0.9152 - val_loss: 0.3474 - val_accuracy: 0.9147\n","Epoch 19/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3386 - accuracy: 0.9157\n","Epoch 00019: val_accuracy improved from 0.91467 to 0.91675, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3388 - accuracy: 0.9156 - val_loss: 0.3436 - val_accuracy: 0.9168\n","Epoch 20/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3353 - accuracy: 0.9164\n","Epoch 00020: val_accuracy did not improve from 0.91675\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3356 - accuracy: 0.9164 - val_loss: 0.3427 - val_accuracy: 0.9162\n","Epoch 21/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3327 - accuracy: 0.9174\n","Epoch 00021: val_accuracy did not improve from 0.91675\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3327 - accuracy: 0.9174 - val_loss: 0.3401 - val_accuracy: 0.9165\n","Epoch 22/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3298 - accuracy: 0.9179\n","Epoch 00022: val_accuracy improved from 0.91675 to 0.91767, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3298 - accuracy: 0.9179 - val_loss: 0.3361 - val_accuracy: 0.9177\n","Epoch 23/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3273 - accuracy: 0.9183\n","Epoch 00023: val_accuracy improved from 0.91767 to 0.91825, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.3272 - accuracy: 0.9183 - val_loss: 0.3335 - val_accuracy: 0.9183\n","Epoch 24/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3245 - accuracy: 0.9190\n","Epoch 00024: val_accuracy did not improve from 0.91825\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3242 - accuracy: 0.9190 - val_loss: 0.3324 - val_accuracy: 0.9179\n","Epoch 25/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3217 - accuracy: 0.9197\n","Epoch 00025: val_accuracy improved from 0.91825 to 0.92117, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.3217 - accuracy: 0.9197 - val_loss: 0.3278 - val_accuracy: 0.9212\n","Epoch 26/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.9207\n","Epoch 00026: val_accuracy did not improve from 0.92117\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3191 - accuracy: 0.9208 - val_loss: 0.3265 - val_accuracy: 0.9202\n","Epoch 27/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3167 - accuracy: 0.9212\n","Epoch 00027: val_accuracy improved from 0.92117 to 0.92208, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3164 - accuracy: 0.9214 - val_loss: 0.3227 - val_accuracy: 0.9221\n","Epoch 28/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.9222\n","Epoch 00028: val_accuracy did not improve from 0.92208\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3139 - accuracy: 0.9222 - val_loss: 0.3233 - val_accuracy: 0.9210\n","Epoch 29/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3112 - accuracy: 0.9230\n","Epoch 00029: val_accuracy improved from 0.92208 to 0.92317, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3115 - accuracy: 0.9230 - val_loss: 0.3182 - val_accuracy: 0.9232\n","Epoch 30/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3087 - accuracy: 0.9241\n","Epoch 00030: val_accuracy improved from 0.92317 to 0.92433, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3088 - accuracy: 0.9241 - val_loss: 0.3163 - val_accuracy: 0.9243\n","Epoch 31/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3066 - accuracy: 0.9244\n","Epoch 00031: val_accuracy improved from 0.92433 to 0.92442, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3064 - accuracy: 0.9245 - val_loss: 0.3138 - val_accuracy: 0.9244\n","Epoch 32/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.9251\n","Epoch 00032: val_accuracy improved from 0.92442 to 0.92667, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3039 - accuracy: 0.9250 - val_loss: 0.3121 - val_accuracy: 0.9267\n","Epoch 33/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3015 - accuracy: 0.9264\n","Epoch 00033: val_accuracy did not improve from 0.92667\n","188/188 [==============================] - 17s 90ms/step - loss: 0.3015 - accuracy: 0.9263 - val_loss: 0.3091 - val_accuracy: 0.9265\n","Epoch 34/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.9270\n","Epoch 00034: val_accuracy did not improve from 0.92667\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2990 - accuracy: 0.9270 - val_loss: 0.3073 - val_accuracy: 0.9266\n","Epoch 35/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.9277\n","Epoch 00035: val_accuracy improved from 0.92667 to 0.92758, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2965 - accuracy: 0.9276 - val_loss: 0.3049 - val_accuracy: 0.9276\n","Epoch 36/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.9287\n","Epoch 00036: val_accuracy improved from 0.92758 to 0.92942, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2940 - accuracy: 0.9287 - val_loss: 0.3018 - val_accuracy: 0.9294\n","Epoch 37/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.9291\n","Epoch 00037: val_accuracy improved from 0.92942 to 0.92992, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2917 - accuracy: 0.9291 - val_loss: 0.2996 - val_accuracy: 0.9299\n","Epoch 38/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9300\n","Epoch 00038: val_accuracy did not improve from 0.92992\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2892 - accuracy: 0.9299 - val_loss: 0.2990 - val_accuracy: 0.9289\n","Epoch 39/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.9307\n","Epoch 00039: val_accuracy improved from 0.92992 to 0.93175, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2870 - accuracy: 0.9308 - val_loss: 0.2952 - val_accuracy: 0.9317\n","Epoch 40/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9321\n","Epoch 00040: val_accuracy improved from 0.93175 to 0.93225, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2845 - accuracy: 0.9320 - val_loss: 0.2934 - val_accuracy: 0.9323\n","Epoch 41/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.9325\n","Epoch 00041: val_accuracy improved from 0.93225 to 0.93267, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.2823 - accuracy: 0.9324 - val_loss: 0.2908 - val_accuracy: 0.9327\n","Epoch 42/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2800 - accuracy: 0.9332\n","Epoch 00042: val_accuracy improved from 0.93267 to 0.93383, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2801 - accuracy: 0.9331 - val_loss: 0.2883 - val_accuracy: 0.9338\n","Epoch 43/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.9338\n","Epoch 00043: val_accuracy improved from 0.93383 to 0.93433, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2777 - accuracy: 0.9337 - val_loss: 0.2874 - val_accuracy: 0.9343\n","Epoch 44/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2754 - accuracy: 0.9347\n","Epoch 00044: val_accuracy improved from 0.93433 to 0.93542, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2755 - accuracy: 0.9347 - val_loss: 0.2839 - val_accuracy: 0.9354\n","Epoch 45/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2729 - accuracy: 0.9357\n","Epoch 00045: val_accuracy did not improve from 0.93542\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2732 - accuracy: 0.9356 - val_loss: 0.2819 - val_accuracy: 0.9352\n","Epoch 46/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2710 - accuracy: 0.9358\n","Epoch 00046: val_accuracy improved from 0.93542 to 0.93683, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2710 - accuracy: 0.9358 - val_loss: 0.2796 - val_accuracy: 0.9368\n","Epoch 47/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2690 - accuracy: 0.9373\n","Epoch 00047: val_accuracy improved from 0.93683 to 0.93725, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2689 - accuracy: 0.9373 - val_loss: 0.2776 - val_accuracy: 0.9373\n","Epoch 48/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2666 - accuracy: 0.9379\n","Epoch 00048: val_accuracy improved from 0.93725 to 0.93800, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2666 - accuracy: 0.9379 - val_loss: 0.2756 - val_accuracy: 0.9380\n","Epoch 49/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2646 - accuracy: 0.9388\n","Epoch 00049: val_accuracy improved from 0.93800 to 0.93883, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2646 - accuracy: 0.9387 - val_loss: 0.2731 - val_accuracy: 0.9388\n","Epoch 50/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2624 - accuracy: 0.9391\n","Epoch 00050: val_accuracy did not improve from 0.93883\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2625 - accuracy: 0.9391 - val_loss: 0.2715 - val_accuracy: 0.9388\n","Epoch 51/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2604 - accuracy: 0.9402\n","Epoch 00051: val_accuracy improved from 0.93883 to 0.93942, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2605 - accuracy: 0.9402 - val_loss: 0.2699 - val_accuracy: 0.9394\n","Epoch 52/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.9408\n","Epoch 00052: val_accuracy improved from 0.93942 to 0.94025, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2584 - accuracy: 0.9409 - val_loss: 0.2671 - val_accuracy: 0.9402\n","Epoch 53/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.9414\n","Epoch 00053: val_accuracy improved from 0.94025 to 0.94117, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2565 - accuracy: 0.9414 - val_loss: 0.2657 - val_accuracy: 0.9412\n","Epoch 54/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2546 - accuracy: 0.9422\n","Epoch 00054: val_accuracy improved from 0.94117 to 0.94183, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2547 - accuracy: 0.9423 - val_loss: 0.2633 - val_accuracy: 0.9418\n","Epoch 55/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2527 - accuracy: 0.9429\n","Epoch 00055: val_accuracy improved from 0.94183 to 0.94233, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2527 - accuracy: 0.9430 - val_loss: 0.2624 - val_accuracy: 0.9423\n","Epoch 56/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2508 - accuracy: 0.9439\n","Epoch 00056: val_accuracy improved from 0.94233 to 0.94308, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2507 - accuracy: 0.9439 - val_loss: 0.2599 - val_accuracy: 0.9431\n","Epoch 57/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2490 - accuracy: 0.9440\n","Epoch 00057: val_accuracy did not improve from 0.94308\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2490 - accuracy: 0.9440 - val_loss: 0.2581 - val_accuracy: 0.9423\n","Epoch 58/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2471 - accuracy: 0.9450\n","Epoch 00058: val_accuracy improved from 0.94308 to 0.94417, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 89ms/step - loss: 0.2472 - accuracy: 0.9450 - val_loss: 0.2564 - val_accuracy: 0.9442\n","Epoch 59/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2455 - accuracy: 0.9451\n","Epoch 00059: val_accuracy improved from 0.94417 to 0.94492, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2454 - accuracy: 0.9450 - val_loss: 0.2547 - val_accuracy: 0.9449\n","Epoch 60/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.9457\n","Epoch 00060: val_accuracy did not improve from 0.94492\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2438 - accuracy: 0.9456 - val_loss: 0.2527 - val_accuracy: 0.9448\n","Epoch 61/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2422 - accuracy: 0.9464\n","Epoch 00061: val_accuracy improved from 0.94492 to 0.94542, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2421 - accuracy: 0.9465 - val_loss: 0.2515 - val_accuracy: 0.9454\n","Epoch 62/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2407 - accuracy: 0.9468\n","Epoch 00062: val_accuracy improved from 0.94542 to 0.94692, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2405 - accuracy: 0.9468 - val_loss: 0.2498 - val_accuracy: 0.9469\n","Epoch 63/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.9476\n","Epoch 00063: val_accuracy did not improve from 0.94692\n","188/188 [==============================] - 17s 89ms/step - loss: 0.2389 - accuracy: 0.9476 - val_loss: 0.2492 - val_accuracy: 0.9455\n","Epoch 64/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2373 - accuracy: 0.9480\n","Epoch 00064: val_accuracy improved from 0.94692 to 0.94717, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2373 - accuracy: 0.9480 - val_loss: 0.2467 - val_accuracy: 0.9472\n","Epoch 65/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2361 - accuracy: 0.9483\n","Epoch 00065: val_accuracy improved from 0.94717 to 0.94758, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 89ms/step - loss: 0.2358 - accuracy: 0.9484 - val_loss: 0.2453 - val_accuracy: 0.9476\n","Epoch 66/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.9491\n","Epoch 00066: val_accuracy improved from 0.94758 to 0.94817, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 89ms/step - loss: 0.2344 - accuracy: 0.9491 - val_loss: 0.2436 - val_accuracy: 0.9482\n","Epoch 67/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2329 - accuracy: 0.9499\n","Epoch 00067: val_accuracy did not improve from 0.94817\n","188/188 [==============================] - 17s 89ms/step - loss: 0.2329 - accuracy: 0.9499 - val_loss: 0.2429 - val_accuracy: 0.9475\n","Epoch 68/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2314 - accuracy: 0.9501\n","Epoch 00068: val_accuracy did not improve from 0.94817\n","188/188 [==============================] - 17s 89ms/step - loss: 0.2314 - accuracy: 0.9501 - val_loss: 0.2411 - val_accuracy: 0.9481\n","Epoch 69/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2301 - accuracy: 0.9504\n","Epoch 00069: val_accuracy improved from 0.94817 to 0.94867, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2300 - accuracy: 0.9504 - val_loss: 0.2397 - val_accuracy: 0.9487\n","Epoch 70/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2286 - accuracy: 0.9509\n","Epoch 00070: val_accuracy improved from 0.94867 to 0.94967, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2287 - accuracy: 0.9509 - val_loss: 0.2383 - val_accuracy: 0.9497\n","Epoch 71/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2270 - accuracy: 0.9516\n","Epoch 00071: val_accuracy improved from 0.94967 to 0.95000, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2273 - accuracy: 0.9515 - val_loss: 0.2374 - val_accuracy: 0.9500\n","Epoch 72/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2260 - accuracy: 0.9516\n","Epoch 00072: val_accuracy improved from 0.95000 to 0.95058, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2260 - accuracy: 0.9517 - val_loss: 0.2357 - val_accuracy: 0.9506\n","Epoch 73/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2247 - accuracy: 0.9522\n","Epoch 00073: val_accuracy improved from 0.95058 to 0.95117, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2247 - accuracy: 0.9523 - val_loss: 0.2348 - val_accuracy: 0.9512\n","Epoch 74/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2235 - accuracy: 0.9525\n","Epoch 00074: val_accuracy improved from 0.95117 to 0.95158, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2235 - accuracy: 0.9525 - val_loss: 0.2332 - val_accuracy: 0.9516\n","Epoch 75/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2222 - accuracy: 0.9529\n","Epoch 00075: val_accuracy improved from 0.95158 to 0.95200, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2222 - accuracy: 0.9529 - val_loss: 0.2323 - val_accuracy: 0.9520\n","Epoch 76/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2210 - accuracy: 0.9533\n","Epoch 00076: val_accuracy did not improve from 0.95200\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2210 - accuracy: 0.9533 - val_loss: 0.2314 - val_accuracy: 0.9518\n","Epoch 77/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2202 - accuracy: 0.9536\n","Epoch 00077: val_accuracy improved from 0.95200 to 0.95250, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2199 - accuracy: 0.9538 - val_loss: 0.2297 - val_accuracy: 0.9525\n","Epoch 78/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2186 - accuracy: 0.9546\n","Epoch 00078: val_accuracy improved from 0.95250 to 0.95258, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.2187 - accuracy: 0.9545 - val_loss: 0.2285 - val_accuracy: 0.9526\n","Epoch 79/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2173 - accuracy: 0.9548\n","Epoch 00079: val_accuracy improved from 0.95258 to 0.95300, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2176 - accuracy: 0.9547 - val_loss: 0.2277 - val_accuracy: 0.9530\n","Epoch 80/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2163 - accuracy: 0.9552\n","Epoch 00080: val_accuracy improved from 0.95300 to 0.95317, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2164 - accuracy: 0.9551 - val_loss: 0.2269 - val_accuracy: 0.9532\n","Epoch 81/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2156 - accuracy: 0.9550\n","Epoch 00081: val_accuracy improved from 0.95317 to 0.95333, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2154 - accuracy: 0.9551 - val_loss: 0.2257 - val_accuracy: 0.9533\n","Epoch 82/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2144 - accuracy: 0.9559\n","Epoch 00082: val_accuracy improved from 0.95333 to 0.95367, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2143 - accuracy: 0.9559 - val_loss: 0.2244 - val_accuracy: 0.9537\n","Epoch 83/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2131 - accuracy: 0.9562\n","Epoch 00083: val_accuracy improved from 0.95367 to 0.95458, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2133 - accuracy: 0.9561 - val_loss: 0.2234 - val_accuracy: 0.9546\n","Epoch 84/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2124 - accuracy: 0.9564\n","Epoch 00084: val_accuracy improved from 0.95458 to 0.95467, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2122 - accuracy: 0.9565 - val_loss: 0.2223 - val_accuracy: 0.9547\n","Epoch 85/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2112 - accuracy: 0.9564\n","Epoch 00085: val_accuracy improved from 0.95467 to 0.95483, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2112 - accuracy: 0.9563 - val_loss: 0.2216 - val_accuracy: 0.9548\n","Epoch 86/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2104 - accuracy: 0.9571\n","Epoch 00086: val_accuracy improved from 0.95483 to 0.95517, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2103 - accuracy: 0.9571 - val_loss: 0.2205 - val_accuracy: 0.9552\n","Epoch 87/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2092 - accuracy: 0.9576\n","Epoch 00087: val_accuracy did not improve from 0.95517\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2093 - accuracy: 0.9576 - val_loss: 0.2198 - val_accuracy: 0.9545\n","Epoch 88/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2084 - accuracy: 0.9573\n","Epoch 00088: val_accuracy improved from 0.95517 to 0.95583, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2082 - accuracy: 0.9574 - val_loss: 0.2189 - val_accuracy: 0.9558\n","Epoch 89/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2075 - accuracy: 0.9580\n","Epoch 00089: val_accuracy did not improve from 0.95583\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2074 - accuracy: 0.9580 - val_loss: 0.2177 - val_accuracy: 0.9558\n","Epoch 90/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2065 - accuracy: 0.9583\n","Epoch 00090: val_accuracy did not improve from 0.95583\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2065 - accuracy: 0.9583 - val_loss: 0.2173 - val_accuracy: 0.9555\n","Epoch 91/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2056 - accuracy: 0.9584\n","Epoch 00091: val_accuracy improved from 0.95583 to 0.95608, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2056 - accuracy: 0.9585 - val_loss: 0.2161 - val_accuracy: 0.9561\n","Epoch 92/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2049 - accuracy: 0.9587\n","Epoch 00092: val_accuracy improved from 0.95608 to 0.95675, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2047 - accuracy: 0.9587 - val_loss: 0.2158 - val_accuracy: 0.9567\n","Epoch 93/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2038 - accuracy: 0.9588\n","Epoch 00093: val_accuracy improved from 0.95675 to 0.95683, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2038 - accuracy: 0.9588 - val_loss: 0.2142 - val_accuracy: 0.9568\n","Epoch 94/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2029 - accuracy: 0.9590\n","Epoch 00094: val_accuracy improved from 0.95683 to 0.95700, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2029 - accuracy: 0.9590 - val_loss: 0.2136 - val_accuracy: 0.9570\n","Epoch 95/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2021 - accuracy: 0.9592\n","Epoch 00095: val_accuracy did not improve from 0.95700\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2021 - accuracy: 0.9592 - val_loss: 0.2128 - val_accuracy: 0.9567\n","Epoch 96/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2013 - accuracy: 0.9595\n","Epoch 00096: val_accuracy improved from 0.95700 to 0.95717, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.2013 - accuracy: 0.9595 - val_loss: 0.2119 - val_accuracy: 0.9572\n","Epoch 97/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9596\n","Epoch 00097: val_accuracy improved from 0.95717 to 0.95783, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.2004 - accuracy: 0.9595 - val_loss: 0.2113 - val_accuracy: 0.9578\n","Epoch 98/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9598\n","Epoch 00098: val_accuracy did not improve from 0.95783\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1996 - accuracy: 0.9599 - val_loss: 0.2104 - val_accuracy: 0.9576\n","Epoch 99/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1988 - accuracy: 0.9602\n","Epoch 00099: val_accuracy improved from 0.95783 to 0.95817, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1988 - accuracy: 0.9602 - val_loss: 0.2096 - val_accuracy: 0.9582\n","Epoch 100/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1979 - accuracy: 0.9604\n","Epoch 00100: val_accuracy did not improve from 0.95817\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1980 - accuracy: 0.9604 - val_loss: 0.2091 - val_accuracy: 0.9574\n","Epoch 101/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1972 - accuracy: 0.9607\n","Epoch 00101: val_accuracy did not improve from 0.95817\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1973 - accuracy: 0.9606 - val_loss: 0.2083 - val_accuracy: 0.9579\n","Epoch 102/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1965 - accuracy: 0.9610\n","Epoch 00102: val_accuracy improved from 0.95817 to 0.95850, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1966 - accuracy: 0.9610 - val_loss: 0.2076 - val_accuracy: 0.9585\n","Epoch 103/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1959 - accuracy: 0.9611\n","Epoch 00103: val_accuracy did not improve from 0.95850\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1958 - accuracy: 0.9611 - val_loss: 0.2065 - val_accuracy: 0.9584\n","Epoch 104/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1952 - accuracy: 0.9614\n","Epoch 00104: val_accuracy improved from 0.95850 to 0.95867, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1951 - accuracy: 0.9614 - val_loss: 0.2057 - val_accuracy: 0.9587\n","Epoch 105/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1945 - accuracy: 0.9613\n","Epoch 00105: val_accuracy did not improve from 0.95867\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1944 - accuracy: 0.9613 - val_loss: 0.2054 - val_accuracy: 0.9582\n","Epoch 106/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9618\n","Epoch 00106: val_accuracy did not improve from 0.95867\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1937 - accuracy: 0.9619 - val_loss: 0.2045 - val_accuracy: 0.9583\n","Epoch 107/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1931 - accuracy: 0.9618\n","Epoch 00107: val_accuracy improved from 0.95867 to 0.95883, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1929 - accuracy: 0.9618 - val_loss: 0.2039 - val_accuracy: 0.9588\n","Epoch 108/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1923 - accuracy: 0.9622\n","Epoch 00108: val_accuracy improved from 0.95883 to 0.95917, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1923 - accuracy: 0.9622 - val_loss: 0.2034 - val_accuracy: 0.9592\n","Epoch 109/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.9622\n","Epoch 00109: val_accuracy improved from 0.95917 to 0.95975, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1916 - accuracy: 0.9622 - val_loss: 0.2029 - val_accuracy: 0.9597\n","Epoch 110/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9622\n","Epoch 00110: val_accuracy did not improve from 0.95975\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1910 - accuracy: 0.9622 - val_loss: 0.2023 - val_accuracy: 0.9594\n","Epoch 111/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1904 - accuracy: 0.9625\n","Epoch 00111: val_accuracy did not improve from 0.95975\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1903 - accuracy: 0.9625 - val_loss: 0.2015 - val_accuracy: 0.9594\n","Epoch 112/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9628\n","Epoch 00112: val_accuracy did not improve from 0.95975\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1897 - accuracy: 0.9628 - val_loss: 0.2010 - val_accuracy: 0.9594\n","Epoch 113/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1891 - accuracy: 0.9631\n","Epoch 00113: val_accuracy improved from 0.95975 to 0.96017, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1890 - accuracy: 0.9631 - val_loss: 0.2004 - val_accuracy: 0.9602\n","Epoch 114/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9632\n","Epoch 00114: val_accuracy did not improve from 0.96017\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1884 - accuracy: 0.9631 - val_loss: 0.1998 - val_accuracy: 0.9599\n","Epoch 115/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1877 - accuracy: 0.9633\n","Epoch 00115: val_accuracy did not improve from 0.96017\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1878 - accuracy: 0.9633 - val_loss: 0.1993 - val_accuracy: 0.9595\n","Epoch 116/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1872 - accuracy: 0.9635\n","Epoch 00116: val_accuracy improved from 0.96017 to 0.96058, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1872 - accuracy: 0.9635 - val_loss: 0.1983 - val_accuracy: 0.9606\n","Epoch 117/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1866 - accuracy: 0.9636\n","Epoch 00117: val_accuracy improved from 0.96058 to 0.96117, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1866 - accuracy: 0.9636 - val_loss: 0.1979 - val_accuracy: 0.9612\n","Epoch 118/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1860 - accuracy: 0.9635\n","Epoch 00118: val_accuracy did not improve from 0.96117\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1860 - accuracy: 0.9636 - val_loss: 0.1972 - val_accuracy: 0.9606\n","Epoch 119/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1849 - accuracy: 0.9642\n","Epoch 00119: val_accuracy improved from 0.96117 to 0.96125, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1854 - accuracy: 0.9641 - val_loss: 0.1967 - val_accuracy: 0.9613\n","Epoch 120/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1847 - accuracy: 0.9638\n","Epoch 00120: val_accuracy did not improve from 0.96125\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1849 - accuracy: 0.9638 - val_loss: 0.1961 - val_accuracy: 0.9603\n","Epoch 121/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1844 - accuracy: 0.9642\n","Epoch 00121: val_accuracy did not improve from 0.96125\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1842 - accuracy: 0.9642 - val_loss: 0.1956 - val_accuracy: 0.9611\n","Epoch 122/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9643\n","Epoch 00122: val_accuracy did not improve from 0.96125\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1837 - accuracy: 0.9644 - val_loss: 0.1951 - val_accuracy: 0.9603\n","Epoch 123/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1828 - accuracy: 0.9646\n","Epoch 00123: val_accuracy did not improve from 0.96125\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1831 - accuracy: 0.9645 - val_loss: 0.1944 - val_accuracy: 0.9613\n","Epoch 124/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1826 - accuracy: 0.9647\n","Epoch 00124: val_accuracy did not improve from 0.96125\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1826 - accuracy: 0.9648 - val_loss: 0.1938 - val_accuracy: 0.9607\n","Epoch 125/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9651\n","Epoch 00125: val_accuracy did not improve from 0.96125\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1820 - accuracy: 0.9650 - val_loss: 0.1933 - val_accuracy: 0.9613\n","Epoch 126/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1814 - accuracy: 0.9648\n","Epoch 00126: val_accuracy did not improve from 0.96125\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1815 - accuracy: 0.9648 - val_loss: 0.1928 - val_accuracy: 0.9607\n","Epoch 127/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1809 - accuracy: 0.9651\n","Epoch 00127: val_accuracy improved from 0.96125 to 0.96158, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1809 - accuracy: 0.9650 - val_loss: 0.1925 - val_accuracy: 0.9616\n","Epoch 128/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1803 - accuracy: 0.9653\n","Epoch 00128: val_accuracy did not improve from 0.96158\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1804 - accuracy: 0.9653 - val_loss: 0.1918 - val_accuracy: 0.9613\n","Epoch 129/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1799 - accuracy: 0.9653\n","Epoch 00129: val_accuracy did not improve from 0.96158\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1799 - accuracy: 0.9653 - val_loss: 0.1914 - val_accuracy: 0.9616\n","Epoch 130/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1794 - accuracy: 0.9653\n","Epoch 00130: val_accuracy improved from 0.96158 to 0.96233, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1794 - accuracy: 0.9653 - val_loss: 0.1909 - val_accuracy: 0.9623\n","Epoch 131/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1787 - accuracy: 0.9656\n","Epoch 00131: val_accuracy did not improve from 0.96233\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1789 - accuracy: 0.9656 - val_loss: 0.1907 - val_accuracy: 0.9613\n","Epoch 132/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1784 - accuracy: 0.9657\n","Epoch 00132: val_accuracy did not improve from 0.96233\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1784 - accuracy: 0.9657 - val_loss: 0.1899 - val_accuracy: 0.9615\n","Epoch 133/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1778 - accuracy: 0.9660\n","Epoch 00133: val_accuracy did not improve from 0.96233\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1779 - accuracy: 0.9659 - val_loss: 0.1894 - val_accuracy: 0.9617\n","Epoch 134/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9660\n","Epoch 00134: val_accuracy improved from 0.96233 to 0.96258, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1774 - accuracy: 0.9660 - val_loss: 0.1891 - val_accuracy: 0.9626\n","Epoch 135/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1768 - accuracy: 0.9661\n","Epoch 00135: val_accuracy did not improve from 0.96258\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1769 - accuracy: 0.9661 - val_loss: 0.1884 - val_accuracy: 0.9622\n","Epoch 136/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9659\n","Epoch 00136: val_accuracy did not improve from 0.96258\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1764 - accuracy: 0.9659 - val_loss: 0.1879 - val_accuracy: 0.9622\n","Epoch 137/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1760 - accuracy: 0.9663\n","Epoch 00137: val_accuracy did not improve from 0.96258\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1759 - accuracy: 0.9663 - val_loss: 0.1877 - val_accuracy: 0.9618\n","Epoch 138/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1756 - accuracy: 0.9665\n","Epoch 00138: val_accuracy improved from 0.96258 to 0.96317, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1755 - accuracy: 0.9665 - val_loss: 0.1869 - val_accuracy: 0.9632\n","Epoch 139/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9667\n","Epoch 00139: val_accuracy did not improve from 0.96317\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1750 - accuracy: 0.9668 - val_loss: 0.1864 - val_accuracy: 0.9624\n","Epoch 140/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1745 - accuracy: 0.9668\n","Epoch 00140: val_accuracy did not improve from 0.96317\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1746 - accuracy: 0.9668 - val_loss: 0.1861 - val_accuracy: 0.9630\n","Epoch 141/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1738 - accuracy: 0.9671\n","Epoch 00141: val_accuracy did not improve from 0.96317\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1740 - accuracy: 0.9671 - val_loss: 0.1863 - val_accuracy: 0.9620\n","Epoch 142/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1737 - accuracy: 0.9668\n","Epoch 00142: val_accuracy did not improve from 0.96317\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1736 - accuracy: 0.9669 - val_loss: 0.1852 - val_accuracy: 0.9625\n","Epoch 143/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.9669\n","Epoch 00143: val_accuracy did not improve from 0.96317\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1732 - accuracy: 0.9670 - val_loss: 0.1849 - val_accuracy: 0.9631\n","Epoch 144/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9670\n","Epoch 00144: val_accuracy improved from 0.96317 to 0.96342, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1728 - accuracy: 0.9670 - val_loss: 0.1844 - val_accuracy: 0.9634\n","Epoch 145/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1724 - accuracy: 0.9674\n","Epoch 00145: val_accuracy did not improve from 0.96342\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1723 - accuracy: 0.9675 - val_loss: 0.1840 - val_accuracy: 0.9631\n","Epoch 146/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1719 - accuracy: 0.9674\n","Epoch 00146: val_accuracy did not improve from 0.96342\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1719 - accuracy: 0.9673 - val_loss: 0.1836 - val_accuracy: 0.9631\n","Epoch 147/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9674\n","Epoch 00147: val_accuracy improved from 0.96342 to 0.96392, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1715 - accuracy: 0.9674 - val_loss: 0.1832 - val_accuracy: 0.9639\n","Epoch 148/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1711 - accuracy: 0.9674\n","Epoch 00148: val_accuracy improved from 0.96392 to 0.96417, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1711 - accuracy: 0.9674 - val_loss: 0.1828 - val_accuracy: 0.9642\n","Epoch 149/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1707 - accuracy: 0.9673\n","Epoch 00149: val_accuracy did not improve from 0.96417\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1707 - accuracy: 0.9673 - val_loss: 0.1824 - val_accuracy: 0.9636\n","Epoch 150/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1703 - accuracy: 0.9678\n","Epoch 00150: val_accuracy did not improve from 0.96417\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1702 - accuracy: 0.9678 - val_loss: 0.1822 - val_accuracy: 0.9638\n","Epoch 151/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1699 - accuracy: 0.9679\n","Epoch 00151: val_accuracy did not improve from 0.96417\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1698 - accuracy: 0.9679 - val_loss: 0.1817 - val_accuracy: 0.9632\n","Epoch 152/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1695 - accuracy: 0.9680\n","Epoch 00152: val_accuracy did not improve from 0.96417\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1694 - accuracy: 0.9680 - val_loss: 0.1810 - val_accuracy: 0.9638\n","Epoch 153/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9682\n","Epoch 00153: val_accuracy did not improve from 0.96417\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1690 - accuracy: 0.9682 - val_loss: 0.1807 - val_accuracy: 0.9638\n","Epoch 154/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1685 - accuracy: 0.9682\n","Epoch 00154: val_accuracy did not improve from 0.96417\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1685 - accuracy: 0.9682 - val_loss: 0.1807 - val_accuracy: 0.9637\n","Epoch 155/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1682 - accuracy: 0.9683\n","Epoch 00155: val_accuracy did not improve from 0.96417\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1682 - accuracy: 0.9683 - val_loss: 0.1800 - val_accuracy: 0.9639\n","Epoch 156/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1679 - accuracy: 0.9682\n","Epoch 00156: val_accuracy did not improve from 0.96417\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1678 - accuracy: 0.9683 - val_loss: 0.1800 - val_accuracy: 0.9640\n","Epoch 157/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1673 - accuracy: 0.9684\n","Epoch 00157: val_accuracy improved from 0.96417 to 0.96433, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1675 - accuracy: 0.9683 - val_loss: 0.1792 - val_accuracy: 0.9643\n","Epoch 158/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1670 - accuracy: 0.9683\n","Epoch 00158: val_accuracy did not improve from 0.96433\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1670 - accuracy: 0.9683 - val_loss: 0.1788 - val_accuracy: 0.9638\n","Epoch 159/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1667 - accuracy: 0.9684\n","Epoch 00159: val_accuracy did not improve from 0.96433\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1666 - accuracy: 0.9685 - val_loss: 0.1785 - val_accuracy: 0.9643\n","Epoch 160/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1664 - accuracy: 0.9685\n","Epoch 00160: val_accuracy improved from 0.96433 to 0.96458, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1663 - accuracy: 0.9685 - val_loss: 0.1781 - val_accuracy: 0.9646\n","Epoch 161/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1659 - accuracy: 0.9687\n","Epoch 00161: val_accuracy did not improve from 0.96458\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1659 - accuracy: 0.9687 - val_loss: 0.1778 - val_accuracy: 0.9643\n","Epoch 162/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1657 - accuracy: 0.9687\n","Epoch 00162: val_accuracy improved from 0.96458 to 0.96500, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1656 - accuracy: 0.9688 - val_loss: 0.1774 - val_accuracy: 0.9650\n","Epoch 163/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1652 - accuracy: 0.9690\n","Epoch 00163: val_accuracy improved from 0.96500 to 0.96525, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1652 - accuracy: 0.9690 - val_loss: 0.1770 - val_accuracy: 0.9653\n","Epoch 164/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1648 - accuracy: 0.9693\n","Epoch 00164: val_accuracy did not improve from 0.96525\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1648 - accuracy: 0.9693 - val_loss: 0.1770 - val_accuracy: 0.9652\n","Epoch 165/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1644 - accuracy: 0.9693\n","Epoch 00165: val_accuracy improved from 0.96525 to 0.96542, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1645 - accuracy: 0.9692 - val_loss: 0.1766 - val_accuracy: 0.9654\n","Epoch 166/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1642 - accuracy: 0.9692\n","Epoch 00166: val_accuracy did not improve from 0.96542\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1641 - accuracy: 0.9693 - val_loss: 0.1761 - val_accuracy: 0.9651\n","Epoch 167/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1639 - accuracy: 0.9691\n","Epoch 00167: val_accuracy improved from 0.96542 to 0.96550, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1638 - accuracy: 0.9691 - val_loss: 0.1756 - val_accuracy: 0.9655\n","Epoch 168/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1633 - accuracy: 0.9695\n","Epoch 00168: val_accuracy did not improve from 0.96550\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1634 - accuracy: 0.9695 - val_loss: 0.1753 - val_accuracy: 0.9653\n","Epoch 169/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1629 - accuracy: 0.9698\n","Epoch 00169: val_accuracy did not improve from 0.96550\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1631 - accuracy: 0.9697 - val_loss: 0.1753 - val_accuracy: 0.9654\n","Epoch 170/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1627 - accuracy: 0.9698\n","Epoch 00170: val_accuracy did not improve from 0.96550\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1627 - accuracy: 0.9698 - val_loss: 0.1750 - val_accuracy: 0.9655\n","Epoch 171/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1625 - accuracy: 0.9697\n","Epoch 00171: val_accuracy improved from 0.96550 to 0.96600, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1624 - accuracy: 0.9697 - val_loss: 0.1743 - val_accuracy: 0.9660\n","Epoch 172/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1619 - accuracy: 0.9698\n","Epoch 00172: val_accuracy did not improve from 0.96600\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1620 - accuracy: 0.9697 - val_loss: 0.1746 - val_accuracy: 0.9650\n","Epoch 173/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1618 - accuracy: 0.9699\n","Epoch 00173: val_accuracy improved from 0.96600 to 0.96608, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1617 - accuracy: 0.9699 - val_loss: 0.1739 - val_accuracy: 0.9661\n","Epoch 174/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1614 - accuracy: 0.9700\n","Epoch 00174: val_accuracy did not improve from 0.96608\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1614 - accuracy: 0.9700 - val_loss: 0.1734 - val_accuracy: 0.9661\n","Epoch 175/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1613 - accuracy: 0.9700\n","Epoch 00175: val_accuracy did not improve from 0.96608\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1611 - accuracy: 0.9700 - val_loss: 0.1733 - val_accuracy: 0.9661\n","Epoch 176/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1607 - accuracy: 0.9700\n","Epoch 00176: val_accuracy did not improve from 0.96608\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1607 - accuracy: 0.9700 - val_loss: 0.1727 - val_accuracy: 0.9660\n","Epoch 177/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1604 - accuracy: 0.9701\n","Epoch 00177: val_accuracy improved from 0.96608 to 0.96667, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1604 - accuracy: 0.9701 - val_loss: 0.1726 - val_accuracy: 0.9667\n","Epoch 178/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1599 - accuracy: 0.9703\n","Epoch 00178: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1600 - accuracy: 0.9702 - val_loss: 0.1721 - val_accuracy: 0.9665\n","Epoch 179/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1595 - accuracy: 0.9704\n","Epoch 00179: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1597 - accuracy: 0.9703 - val_loss: 0.1721 - val_accuracy: 0.9654\n","Epoch 180/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1593 - accuracy: 0.9704\n","Epoch 00180: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1595 - accuracy: 0.9704 - val_loss: 0.1715 - val_accuracy: 0.9658\n","Epoch 181/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1592 - accuracy: 0.9705\n","Epoch 00181: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1591 - accuracy: 0.9705 - val_loss: 0.1712 - val_accuracy: 0.9665\n","Epoch 182/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1589 - accuracy: 0.9706\n","Epoch 00182: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1589 - accuracy: 0.9706 - val_loss: 0.1710 - val_accuracy: 0.9663\n","Epoch 183/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1587 - accuracy: 0.9705\n","Epoch 00183: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1586 - accuracy: 0.9706 - val_loss: 0.1706 - val_accuracy: 0.9663\n","Epoch 184/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1582 - accuracy: 0.9704\n","Epoch 00184: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1582 - accuracy: 0.9704 - val_loss: 0.1704 - val_accuracy: 0.9666\n","Epoch 185/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1580 - accuracy: 0.9708\n","Epoch 00185: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1579 - accuracy: 0.9708 - val_loss: 0.1700 - val_accuracy: 0.9664\n","Epoch 186/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1577 - accuracy: 0.9706\n","Epoch 00186: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1576 - accuracy: 0.9706 - val_loss: 0.1698 - val_accuracy: 0.9663\n","Epoch 187/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1571 - accuracy: 0.9709\n","Epoch 00187: val_accuracy improved from 0.96667 to 0.96692, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1573 - accuracy: 0.9708 - val_loss: 0.1694 - val_accuracy: 0.9669\n","Epoch 188/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1571 - accuracy: 0.9707\n","Epoch 00188: val_accuracy improved from 0.96692 to 0.96717, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 91ms/step - loss: 0.1571 - accuracy: 0.9707 - val_loss: 0.1693 - val_accuracy: 0.9672\n","Epoch 189/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9710\n","Epoch 00189: val_accuracy did not improve from 0.96717\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1567 - accuracy: 0.9710 - val_loss: 0.1692 - val_accuracy: 0.9663\n","Epoch 190/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1566 - accuracy: 0.9707\n","Epoch 00190: val_accuracy improved from 0.96717 to 0.96750, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1565 - accuracy: 0.9708 - val_loss: 0.1686 - val_accuracy: 0.9675\n","Epoch 191/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1562 - accuracy: 0.9711\n","Epoch 00191: val_accuracy did not improve from 0.96750\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1561 - accuracy: 0.9711 - val_loss: 0.1684 - val_accuracy: 0.9670\n","Epoch 192/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1560 - accuracy: 0.9713\n","Epoch 00192: val_accuracy did not improve from 0.96750\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1559 - accuracy: 0.9713 - val_loss: 0.1684 - val_accuracy: 0.9674\n","Epoch 193/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1554 - accuracy: 0.9711\n","Epoch 00193: val_accuracy improved from 0.96750 to 0.96767, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1556 - accuracy: 0.9711 - val_loss: 0.1677 - val_accuracy: 0.9677\n","Epoch 194/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1554 - accuracy: 0.9712\n","Epoch 00194: val_accuracy did not improve from 0.96767\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1553 - accuracy: 0.9712 - val_loss: 0.1676 - val_accuracy: 0.9666\n","Epoch 195/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1548 - accuracy: 0.9714\n","Epoch 00195: val_accuracy did not improve from 0.96767\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1550 - accuracy: 0.9714 - val_loss: 0.1675 - val_accuracy: 0.9669\n","Epoch 196/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.9715\n","Epoch 00196: val_accuracy did not improve from 0.96767\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1547 - accuracy: 0.9715 - val_loss: 0.1669 - val_accuracy: 0.9672\n","Epoch 197/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9712\n","Epoch 00197: val_accuracy improved from 0.96767 to 0.96783, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1545 - accuracy: 0.9712 - val_loss: 0.1669 - val_accuracy: 0.9678\n","Epoch 198/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9714\n","Epoch 00198: val_accuracy did not improve from 0.96783\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1542 - accuracy: 0.9714 - val_loss: 0.1665 - val_accuracy: 0.9673\n","Epoch 199/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1540 - accuracy: 0.9717\n","Epoch 00199: val_accuracy improved from 0.96783 to 0.96808, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1540 - accuracy: 0.9717 - val_loss: 0.1664 - val_accuracy: 0.9681\n","Epoch 200/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1537 - accuracy: 0.9717\n","Epoch 00200: val_accuracy did not improve from 0.96808\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1537 - accuracy: 0.9717 - val_loss: 0.1660 - val_accuracy: 0.9674\n","Epoch 201/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1534 - accuracy: 0.9715\n","Epoch 00201: val_accuracy did not improve from 0.96808\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1534 - accuracy: 0.9715 - val_loss: 0.1658 - val_accuracy: 0.9681\n","Epoch 202/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1531 - accuracy: 0.9716\n","Epoch 00202: val_accuracy did not improve from 0.96808\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1531 - accuracy: 0.9716 - val_loss: 0.1657 - val_accuracy: 0.9669\n","Epoch 203/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9720\n","Epoch 00203: val_accuracy did not improve from 0.96808\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1529 - accuracy: 0.9720 - val_loss: 0.1651 - val_accuracy: 0.9678\n","Epoch 204/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1527 - accuracy: 0.9717\n","Epoch 00204: val_accuracy did not improve from 0.96808\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1526 - accuracy: 0.9717 - val_loss: 0.1651 - val_accuracy: 0.9672\n","Epoch 205/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1524 - accuracy: 0.9720\n","Epoch 00205: val_accuracy improved from 0.96808 to 0.96833, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1524 - accuracy: 0.9720 - val_loss: 0.1647 - val_accuracy: 0.9683\n","Epoch 206/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1521 - accuracy: 0.9718\n","Epoch 00206: val_accuracy did not improve from 0.96833\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1520 - accuracy: 0.9718 - val_loss: 0.1644 - val_accuracy: 0.9678\n","Epoch 207/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1518 - accuracy: 0.9721\n","Epoch 00207: val_accuracy did not improve from 0.96833\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1518 - accuracy: 0.9721 - val_loss: 0.1641 - val_accuracy: 0.9682\n","Epoch 208/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1515 - accuracy: 0.9719\n","Epoch 00208: val_accuracy did not improve from 0.96833\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1516 - accuracy: 0.9720 - val_loss: 0.1640 - val_accuracy: 0.9683\n","Epoch 209/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1514 - accuracy: 0.9722\n","Epoch 00209: val_accuracy improved from 0.96833 to 0.96842, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1513 - accuracy: 0.9723 - val_loss: 0.1636 - val_accuracy: 0.9684\n","Epoch 210/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1512 - accuracy: 0.9720\n","Epoch 00210: val_accuracy did not improve from 0.96842\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1511 - accuracy: 0.9720 - val_loss: 0.1635 - val_accuracy: 0.9684\n","Epoch 211/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1508 - accuracy: 0.9721\n","Epoch 00211: val_accuracy improved from 0.96842 to 0.96883, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1508 - accuracy: 0.9721 - val_loss: 0.1631 - val_accuracy: 0.9688\n","Epoch 212/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1507 - accuracy: 0.9726\n","Epoch 00212: val_accuracy did not improve from 0.96883\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1506 - accuracy: 0.9726 - val_loss: 0.1630 - val_accuracy: 0.9683\n","Epoch 213/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1503 - accuracy: 0.9724\n","Epoch 00213: val_accuracy did not improve from 0.96883\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1503 - accuracy: 0.9724 - val_loss: 0.1629 - val_accuracy: 0.9675\n","Epoch 214/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1501 - accuracy: 0.9723\n","Epoch 00214: val_accuracy did not improve from 0.96883\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1501 - accuracy: 0.9723 - val_loss: 0.1628 - val_accuracy: 0.9674\n","Epoch 215/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1499 - accuracy: 0.9723\n","Epoch 00215: val_accuracy did not improve from 0.96883\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1499 - accuracy: 0.9722 - val_loss: 0.1624 - val_accuracy: 0.9681\n","Epoch 216/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1498 - accuracy: 0.9723\n","Epoch 00216: val_accuracy did not improve from 0.96883\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1496 - accuracy: 0.9723 - val_loss: 0.1621 - val_accuracy: 0.9681\n","Epoch 217/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1493 - accuracy: 0.9727\n","Epoch 00217: val_accuracy did not improve from 0.96883\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1494 - accuracy: 0.9726 - val_loss: 0.1619 - val_accuracy: 0.9682\n","Epoch 218/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1490 - accuracy: 0.9725\n","Epoch 00218: val_accuracy did not improve from 0.96883\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1491 - accuracy: 0.9725 - val_loss: 0.1616 - val_accuracy: 0.9685\n","Epoch 219/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1489 - accuracy: 0.9728\n","Epoch 00219: val_accuracy did not improve from 0.96883\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1489 - accuracy: 0.9728 - val_loss: 0.1614 - val_accuracy: 0.9684\n","Epoch 220/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1488 - accuracy: 0.9726\n","Epoch 00220: val_accuracy improved from 0.96883 to 0.96900, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1487 - accuracy: 0.9726 - val_loss: 0.1611 - val_accuracy: 0.9690\n","Epoch 221/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1484 - accuracy: 0.9727\n","Epoch 00221: val_accuracy did not improve from 0.96900\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1484 - accuracy: 0.9727 - val_loss: 0.1608 - val_accuracy: 0.9683\n","Epoch 222/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1480 - accuracy: 0.9730\n","Epoch 00222: val_accuracy did not improve from 0.96900\n","188/188 [==============================] - 17s 90ms/step - loss: 0.1482 - accuracy: 0.9730 - val_loss: 0.1606 - val_accuracy: 0.9685\n","Epoch 223/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1480 - accuracy: 0.9728\n","Epoch 00223: val_accuracy improved from 0.96900 to 0.96917, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1480 - accuracy: 0.9728 - val_loss: 0.1604 - val_accuracy: 0.9692\n","Epoch 224/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.9729\n","Epoch 00224: val_accuracy did not improve from 0.96917\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1477 - accuracy: 0.9729 - val_loss: 0.1605 - val_accuracy: 0.9683\n","Epoch 225/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1475 - accuracy: 0.9729\n","Epoch 00225: val_accuracy did not improve from 0.96917\n","188/188 [==============================] - 17s 88ms/step - loss: 0.1475 - accuracy: 0.9729 - val_loss: 0.1600 - val_accuracy: 0.9690\n","Epoch 226/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1475 - accuracy: 0.9729\n","Epoch 00226: val_accuracy improved from 0.96917 to 0.96958, saving model to mnist_l2_best.h5\n","188/188 [==============================] - 17s 88ms/step - loss: 0.1474 - accuracy: 0.9729 - val_loss: 0.1599 - val_accuracy: 0.9696\n","Epoch 227/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1471 - accuracy: 0.9727\n","Epoch 00227: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1471 - accuracy: 0.9728 - val_loss: 0.1595 - val_accuracy: 0.9690\n","Epoch 228/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9730\n","Epoch 00228: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1468 - accuracy: 0.9731 - val_loss: 0.1593 - val_accuracy: 0.9691\n","Epoch 229/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1468 - accuracy: 0.9731\n","Epoch 00229: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1467 - accuracy: 0.9730 - val_loss: 0.1592 - val_accuracy: 0.9692\n","Epoch 230/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1464 - accuracy: 0.9731\n","Epoch 00230: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1464 - accuracy: 0.9731 - val_loss: 0.1592 - val_accuracy: 0.9682\n","Epoch 231/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1461 - accuracy: 0.9732\n","Epoch 00231: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 17s 88ms/step - loss: 0.1462 - accuracy: 0.9732 - val_loss: 0.1588 - val_accuracy: 0.9690\n","Epoch 232/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1461 - accuracy: 0.9733\n","Epoch 00232: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1460 - accuracy: 0.9733 - val_loss: 0.1589 - val_accuracy: 0.9680\n","Epoch 233/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1458 - accuracy: 0.9731\n","Epoch 00233: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 17s 88ms/step - loss: 0.1458 - accuracy: 0.9731 - val_loss: 0.1583 - val_accuracy: 0.9693\n","Epoch 234/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1455 - accuracy: 0.9732\n","Epoch 00234: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 17s 88ms/step - loss: 0.1455 - accuracy: 0.9732 - val_loss: 0.1584 - val_accuracy: 0.9683\n","Epoch 235/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1453 - accuracy: 0.9734\n","Epoch 00235: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 17s 88ms/step - loss: 0.1453 - accuracy: 0.9733 - val_loss: 0.1578 - val_accuracy: 0.9693\n","Epoch 236/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1451 - accuracy: 0.9734\n","Epoch 00236: val_accuracy did not improve from 0.96958\n","188/188 [==============================] - 17s 89ms/step - loss: 0.1451 - accuracy: 0.9734 - val_loss: 0.1580 - val_accuracy: 0.9686\n","Epoch 00236: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"41396869-2349-44c3-896a-f799cce15fbe","executionInfo":{"status":"ok","timestamp":1590684237113,"user_tz":-60,"elapsed":2658,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"gDYWpS_CtcjR","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["mnist_l2_model.load_weights('mnist_l2_best.h5')\n","loss, acc = mnist_l2_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 2s 6ms/step - loss: 0.1437 - accuracy: 0.9730\n","Accuracy: 0.9729999899864197\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ny06RSS0OxEW","colab_type":"code","colab":{}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_l2_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_l2_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_l2_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_l2_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aF0cxBPPCwiG","colab_type":"text"},"source":["##### Dropout\n","\n","In this scenario, instead of applying regularization to the weights, we will use a different approach to regularization, namely, dropout. The idea behind dropout is to disable a percentage of randomly selected neurons during each step of the training phase, in order to avoid overfitting. In [Keras](https://www.tensorflow.org/api_docs/python/tf/keras), we can apply dropout directly to some layers by defining the corresponding parameters, or by using the [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layer and stating the percentage of neurons to disable."]},{"cell_type":"code","metadata":{"id":"u-OTwW70Cyg9","colab_type":"code","outputId":"3fe5541e-3b1c-4b86-fe2a-c927bb063b09","executionInfo":{"status":"ok","timestamp":1590666724422,"user_tz":-60,"elapsed":596,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["mnist_conv_drop_model = tf.keras.Sequential(name='mnist_cnn_dropout')\n","mnist_conv_drop_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_conv_drop_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution'))\n","mnist_conv_drop_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling'))\n","mnist_conv_drop_model.add(tf.keras.layers.Dropout(0.5, name='dropout'))\n","mnist_conv_drop_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_conv_drop_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_conv_drop_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_conv_drop_model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"mnist_cnn_dropout\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","convolution (Conv2D)         (None, 28, 28, 16)        272       \n","_________________________________________________________________\n","pooling (MaxPooling2D)       (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 3136)              0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                31370     \n","=================================================================\n","Total params: 31,642\n","Trainable params: 31,642\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FA7NT93m6oCP","colab_type":"text"},"source":["Let's train the model:"]},{"cell_type":"code","metadata":{"id":"Xl0td5O3EO9J","colab_type":"code","outputId":"282a4b84-6076-4dc1-8132-7fa9da4684d0","executionInfo":{"status":"ok","timestamp":1590670816588,"user_tz":-60,"elapsed":4092759,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_conv_drop_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_conv_drop_model_train = mnist_conv_drop_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","188/188 [==============================] - ETA: 0s - loss: 1.6992 - accuracy: 0.4979\n","Epoch 00001: val_accuracy improved from -inf to 0.82717, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 107ms/step - loss: 1.6992 - accuracy: 0.4979 - val_loss: 0.8574 - val_accuracy: 0.8272\n","Epoch 2/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.7582 - accuracy: 0.7753\n","Epoch 00002: val_accuracy improved from 0.82717 to 0.86367, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.7576 - accuracy: 0.7754 - val_loss: 0.5100 - val_accuracy: 0.8637\n","Epoch 3/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.5916 - accuracy: 0.8195\n","Epoch 00003: val_accuracy improved from 0.86367 to 0.87617, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 104ms/step - loss: 0.5913 - accuracy: 0.8196 - val_loss: 0.4341 - val_accuracy: 0.8762\n","Epoch 4/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.5279 - accuracy: 0.8381\n","Epoch 00004: val_accuracy improved from 0.87617 to 0.88758, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 107ms/step - loss: 0.5279 - accuracy: 0.8381 - val_loss: 0.3974 - val_accuracy: 0.8876\n","Epoch 5/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4916 - accuracy: 0.8496\n","Epoch 00005: val_accuracy improved from 0.88758 to 0.89217, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.4916 - accuracy: 0.8496 - val_loss: 0.3765 - val_accuracy: 0.8922\n","Epoch 6/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4665 - accuracy: 0.8580\n","Epoch 00006: val_accuracy improved from 0.89217 to 0.89633, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 107ms/step - loss: 0.4662 - accuracy: 0.8581 - val_loss: 0.3620 - val_accuracy: 0.8963\n","Epoch 7/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4488 - accuracy: 0.8635\n","Epoch 00007: val_accuracy improved from 0.89633 to 0.89992, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.4488 - accuracy: 0.8635 - val_loss: 0.3507 - val_accuracy: 0.8999\n","Epoch 8/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4335 - accuracy: 0.8677\n","Epoch 00008: val_accuracy improved from 0.89992 to 0.90300, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.4335 - accuracy: 0.8677 - val_loss: 0.3408 - val_accuracy: 0.9030\n","Epoch 9/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4224 - accuracy: 0.8734\n","Epoch 00009: val_accuracy improved from 0.90300 to 0.90475, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.4223 - accuracy: 0.8733 - val_loss: 0.3328 - val_accuracy: 0.9047\n","Epoch 10/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4081 - accuracy: 0.8778\n","Epoch 00010: val_accuracy improved from 0.90475 to 0.90758, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.4083 - accuracy: 0.8778 - val_loss: 0.3243 - val_accuracy: 0.9076\n","Epoch 11/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3975 - accuracy: 0.8807\n","Epoch 00011: val_accuracy improved from 0.90758 to 0.90958, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.3975 - accuracy: 0.8807 - val_loss: 0.3187 - val_accuracy: 0.9096\n","Epoch 12/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3899 - accuracy: 0.8836\n","Epoch 00012: val_accuracy improved from 0.90958 to 0.91342, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 106ms/step - loss: 0.3899 - accuracy: 0.8836 - val_loss: 0.3103 - val_accuracy: 0.9134\n","Epoch 13/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.8868\n","Epoch 00013: val_accuracy improved from 0.91342 to 0.91450, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.3735 - accuracy: 0.8868 - val_loss: 0.3033 - val_accuracy: 0.9145\n","Epoch 14/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3650 - accuracy: 0.8915\n","Epoch 00014: val_accuracy improved from 0.91450 to 0.91683, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 104ms/step - loss: 0.3654 - accuracy: 0.8915 - val_loss: 0.2983 - val_accuracy: 0.9168\n","Epoch 15/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3572 - accuracy: 0.8953\n","Epoch 00015: val_accuracy improved from 0.91683 to 0.91842, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.3572 - accuracy: 0.8953 - val_loss: 0.2925 - val_accuracy: 0.9184\n","Epoch 16/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3533 - accuracy: 0.8954\n","Epoch 00016: val_accuracy improved from 0.91842 to 0.92092, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 102ms/step - loss: 0.3535 - accuracy: 0.8953 - val_loss: 0.2862 - val_accuracy: 0.9209\n","Epoch 17/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.8988\n","Epoch 00017: val_accuracy improved from 0.92092 to 0.92317, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 101ms/step - loss: 0.3441 - accuracy: 0.8987 - val_loss: 0.2800 - val_accuracy: 0.9232\n","Epoch 18/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.9010\n","Epoch 00018: val_accuracy improved from 0.92317 to 0.92375, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 101ms/step - loss: 0.3335 - accuracy: 0.9010 - val_loss: 0.2747 - val_accuracy: 0.9237\n","Epoch 19/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3294 - accuracy: 0.9026\n","Epoch 00019: val_accuracy improved from 0.92375 to 0.92658, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 102ms/step - loss: 0.3294 - accuracy: 0.9026 - val_loss: 0.2690 - val_accuracy: 0.9266\n","Epoch 20/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3216 - accuracy: 0.9059\n","Epoch 00020: val_accuracy did not improve from 0.92658\n","188/188 [==============================] - 20s 104ms/step - loss: 0.3216 - accuracy: 0.9059 - val_loss: 0.2649 - val_accuracy: 0.9261\n","Epoch 21/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3131 - accuracy: 0.9073\n","Epoch 00021: val_accuracy improved from 0.92658 to 0.93075, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.3133 - accuracy: 0.9073 - val_loss: 0.2568 - val_accuracy: 0.9308\n","Epoch 22/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 0.9108\n","Epoch 00022: val_accuracy improved from 0.93075 to 0.93175, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 101ms/step - loss: 0.3081 - accuracy: 0.9108 - val_loss: 0.2515 - val_accuracy: 0.9317\n","Epoch 23/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3013 - accuracy: 0.9124\n","Epoch 00023: val_accuracy improved from 0.93175 to 0.93417, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.3013 - accuracy: 0.9124 - val_loss: 0.2463 - val_accuracy: 0.9342\n","Epoch 24/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.9142\n","Epoch 00024: val_accuracy did not improve from 0.93417\n","188/188 [==============================] - 19s 102ms/step - loss: 0.2943 - accuracy: 0.9142 - val_loss: 0.2408 - val_accuracy: 0.9342\n","Epoch 25/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.9163\n","Epoch 00025: val_accuracy improved from 0.93417 to 0.93683, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.2857 - accuracy: 0.9163 - val_loss: 0.2347 - val_accuracy: 0.9368\n","Epoch 26/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2790 - accuracy: 0.9191\n","Epoch 00026: val_accuracy improved from 0.93683 to 0.93792, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.2790 - accuracy: 0.9191 - val_loss: 0.2305 - val_accuracy: 0.9379\n","Epoch 27/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.9191\n","Epoch 00027: val_accuracy improved from 0.93792 to 0.94008, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 107ms/step - loss: 0.2751 - accuracy: 0.9191 - val_loss: 0.2246 - val_accuracy: 0.9401\n","Epoch 28/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2689 - accuracy: 0.9219\n","Epoch 00028: val_accuracy did not improve from 0.94008\n","188/188 [==============================] - 20s 105ms/step - loss: 0.2689 - accuracy: 0.9219 - val_loss: 0.2200 - val_accuracy: 0.9398\n","Epoch 29/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.9239\n","Epoch 00029: val_accuracy improved from 0.94008 to 0.94067, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.2629 - accuracy: 0.9239 - val_loss: 0.2150 - val_accuracy: 0.9407\n","Epoch 30/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.9256\n","Epoch 00030: val_accuracy improved from 0.94067 to 0.94292, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.2565 - accuracy: 0.9256 - val_loss: 0.2102 - val_accuracy: 0.9429\n","Epoch 31/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.9273\n","Epoch 00031: val_accuracy improved from 0.94292 to 0.94483, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.2511 - accuracy: 0.9273 - val_loss: 0.2055 - val_accuracy: 0.9448\n","Epoch 32/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.9291\n","Epoch 00032: val_accuracy improved from 0.94483 to 0.94583, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.2458 - accuracy: 0.9291 - val_loss: 0.2013 - val_accuracy: 0.9458\n","Epoch 33/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9314\n","Epoch 00033: val_accuracy improved from 0.94583 to 0.94833, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.2405 - accuracy: 0.9314 - val_loss: 0.1974 - val_accuracy: 0.9483\n","Epoch 34/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2371 - accuracy: 0.9311\n","Epoch 00034: val_accuracy improved from 0.94833 to 0.94900, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.2371 - accuracy: 0.9311 - val_loss: 0.1941 - val_accuracy: 0.9490\n","Epoch 35/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2314 - accuracy: 0.9329\n","Epoch 00035: val_accuracy did not improve from 0.94900\n","188/188 [==============================] - 24s 130ms/step - loss: 0.2314 - accuracy: 0.9329 - val_loss: 0.1898 - val_accuracy: 0.9488\n","Epoch 36/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2275 - accuracy: 0.9344\n","Epoch 00036: val_accuracy improved from 0.94900 to 0.95133, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 104ms/step - loss: 0.2278 - accuracy: 0.9343 - val_loss: 0.1856 - val_accuracy: 0.9513\n","Epoch 37/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2235 - accuracy: 0.9360\n","Epoch 00037: val_accuracy improved from 0.95133 to 0.95258, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.2235 - accuracy: 0.9360 - val_loss: 0.1823 - val_accuracy: 0.9526\n","Epoch 38/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2199 - accuracy: 0.9369\n","Epoch 00038: val_accuracy did not improve from 0.95258\n","188/188 [==============================] - 20s 104ms/step - loss: 0.2199 - accuracy: 0.9369 - val_loss: 0.1799 - val_accuracy: 0.9523\n","Epoch 39/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2170 - accuracy: 0.9374\n","Epoch 00039: val_accuracy improved from 0.95258 to 0.95308, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.2171 - accuracy: 0.9373 - val_loss: 0.1760 - val_accuracy: 0.9531\n","Epoch 40/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9394\n","Epoch 00040: val_accuracy improved from 0.95308 to 0.95500, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.2120 - accuracy: 0.9394 - val_loss: 0.1723 - val_accuracy: 0.9550\n","Epoch 41/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9398\n","Epoch 00041: val_accuracy improved from 0.95500 to 0.95600, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.2104 - accuracy: 0.9398 - val_loss: 0.1693 - val_accuracy: 0.9560\n","Epoch 42/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2051 - accuracy: 0.9405\n","Epoch 00042: val_accuracy did not improve from 0.95600\n","188/188 [==============================] - 20s 105ms/step - loss: 0.2051 - accuracy: 0.9405 - val_loss: 0.1665 - val_accuracy: 0.9560\n","Epoch 43/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2018 - accuracy: 0.9418\n","Epoch 00043: val_accuracy improved from 0.95600 to 0.95700, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 21s 110ms/step - loss: 0.2021 - accuracy: 0.9417 - val_loss: 0.1639 - val_accuracy: 0.9570\n","Epoch 44/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2014 - accuracy: 0.9420\n","Epoch 00044: val_accuracy improved from 0.95700 to 0.95775, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.2014 - accuracy: 0.9420 - val_loss: 0.1610 - val_accuracy: 0.9578\n","Epoch 45/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9438\n","Epoch 00045: val_accuracy improved from 0.95775 to 0.95825, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1958 - accuracy: 0.9438 - val_loss: 0.1585 - val_accuracy: 0.9582\n","Epoch 46/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9451\n","Epoch 00046: val_accuracy improved from 0.95825 to 0.95933, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1924 - accuracy: 0.9451 - val_loss: 0.1561 - val_accuracy: 0.9593\n","Epoch 47/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1913 - accuracy: 0.9449\n","Epoch 00047: val_accuracy improved from 0.95933 to 0.96025, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1913 - accuracy: 0.9449 - val_loss: 0.1536 - val_accuracy: 0.9603\n","Epoch 48/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.9459\n","Epoch 00048: val_accuracy did not improve from 0.96025\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1887 - accuracy: 0.9459 - val_loss: 0.1521 - val_accuracy: 0.9595\n","Epoch 49/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1850 - accuracy: 0.9476\n","Epoch 00049: val_accuracy improved from 0.96025 to 0.96050, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 104ms/step - loss: 0.1850 - accuracy: 0.9476 - val_loss: 0.1492 - val_accuracy: 0.9605\n","Epoch 50/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1828 - accuracy: 0.9476\n","Epoch 00050: val_accuracy improved from 0.96050 to 0.96142, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1828 - accuracy: 0.9476 - val_loss: 0.1477 - val_accuracy: 0.9614\n","Epoch 51/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1827 - accuracy: 0.9474\n","Epoch 00051: val_accuracy did not improve from 0.96142\n","188/188 [==============================] - 20s 107ms/step - loss: 0.1825 - accuracy: 0.9474 - val_loss: 0.1456 - val_accuracy: 0.9612\n","Epoch 52/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1795 - accuracy: 0.9483\n","Epoch 00052: val_accuracy improved from 0.96142 to 0.96217, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1795 - accuracy: 0.9482 - val_loss: 0.1433 - val_accuracy: 0.9622\n","Epoch 53/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1779 - accuracy: 0.9487\n","Epoch 00053: val_accuracy improved from 0.96217 to 0.96283, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 102ms/step - loss: 0.1778 - accuracy: 0.9488 - val_loss: 0.1416 - val_accuracy: 0.9628\n","Epoch 54/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1735 - accuracy: 0.9491\n","Epoch 00054: val_accuracy improved from 0.96283 to 0.96308, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1737 - accuracy: 0.9490 - val_loss: 0.1397 - val_accuracy: 0.9631\n","Epoch 55/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9503\n","Epoch 00055: val_accuracy improved from 0.96308 to 0.96442, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1729 - accuracy: 0.9503 - val_loss: 0.1382 - val_accuracy: 0.9644\n","Epoch 56/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9494\n","Epoch 00056: val_accuracy did not improve from 0.96442\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1721 - accuracy: 0.9494 - val_loss: 0.1370 - val_accuracy: 0.9640\n","Epoch 57/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1697 - accuracy: 0.9505\n","Epoch 00057: val_accuracy improved from 0.96442 to 0.96450, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 102ms/step - loss: 0.1695 - accuracy: 0.9505 - val_loss: 0.1352 - val_accuracy: 0.9645\n","Epoch 58/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.9517\n","Epoch 00058: val_accuracy improved from 0.96450 to 0.96467, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 102ms/step - loss: 0.1670 - accuracy: 0.9517 - val_loss: 0.1335 - val_accuracy: 0.9647\n","Epoch 59/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1660 - accuracy: 0.9519\n","Epoch 00059: val_accuracy improved from 0.96467 to 0.96525, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1659 - accuracy: 0.9519 - val_loss: 0.1319 - val_accuracy: 0.9653\n","Epoch 60/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.9523\n","Epoch 00060: val_accuracy improved from 0.96525 to 0.96567, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1638 - accuracy: 0.9523 - val_loss: 0.1305 - val_accuracy: 0.9657\n","Epoch 61/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.9523\n","Epoch 00061: val_accuracy improved from 0.96567 to 0.96625, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1648 - accuracy: 0.9523 - val_loss: 0.1288 - val_accuracy: 0.9663\n","Epoch 62/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9521\n","Epoch 00062: val_accuracy did not improve from 0.96625\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1608 - accuracy: 0.9521 - val_loss: 0.1276 - val_accuracy: 0.9663\n","Epoch 63/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9525\n","Epoch 00063: val_accuracy improved from 0.96625 to 0.96650, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1613 - accuracy: 0.9525 - val_loss: 0.1266 - val_accuracy: 0.9665\n","Epoch 64/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1595 - accuracy: 0.9526\n","Epoch 00064: val_accuracy improved from 0.96650 to 0.96658, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1595 - accuracy: 0.9526 - val_loss: 0.1255 - val_accuracy: 0.9666\n","Epoch 65/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9544\n","Epoch 00065: val_accuracy improved from 0.96658 to 0.96700, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1569 - accuracy: 0.9544 - val_loss: 0.1240 - val_accuracy: 0.9670\n","Epoch 66/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9553\n","Epoch 00066: val_accuracy improved from 0.96700 to 0.96708, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 25s 135ms/step - loss: 0.1559 - accuracy: 0.9553 - val_loss: 0.1230 - val_accuracy: 0.9671\n","Epoch 67/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9557\n","Epoch 00067: val_accuracy improved from 0.96708 to 0.96792, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1551 - accuracy: 0.9557 - val_loss: 0.1217 - val_accuracy: 0.9679\n","Epoch 68/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9548\n","Epoch 00068: val_accuracy improved from 0.96792 to 0.96858, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1515 - accuracy: 0.9548 - val_loss: 0.1206 - val_accuracy: 0.9686\n","Epoch 69/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1533 - accuracy: 0.9546\n","Epoch 00069: val_accuracy improved from 0.96858 to 0.96875, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 102ms/step - loss: 0.1533 - accuracy: 0.9546 - val_loss: 0.1199 - val_accuracy: 0.9688\n","Epoch 70/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 0.9558\n","Epoch 00070: val_accuracy did not improve from 0.96875\n","188/188 [==============================] - 19s 101ms/step - loss: 0.1519 - accuracy: 0.9558 - val_loss: 0.1196 - val_accuracy: 0.9683\n","Epoch 71/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9564\n","Epoch 00071: val_accuracy improved from 0.96875 to 0.96950, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 102ms/step - loss: 0.1503 - accuracy: 0.9564 - val_loss: 0.1177 - val_accuracy: 0.9695\n","Epoch 72/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.9572\n","Epoch 00072: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1499 - accuracy: 0.9572 - val_loss: 0.1169 - val_accuracy: 0.9690\n","Epoch 73/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1487 - accuracy: 0.9570\n","Epoch 00073: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 21s 110ms/step - loss: 0.1487 - accuracy: 0.9570 - val_loss: 0.1164 - val_accuracy: 0.9684\n","Epoch 74/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.9561\n","Epoch 00074: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1464 - accuracy: 0.9561 - val_loss: 0.1151 - val_accuracy: 0.9693\n","Epoch 75/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9575\n","Epoch 00075: val_accuracy improved from 0.96950 to 0.96975, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1452 - accuracy: 0.9575 - val_loss: 0.1144 - val_accuracy: 0.9697\n","Epoch 76/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1448 - accuracy: 0.9568\n","Epoch 00076: val_accuracy did not improve from 0.96975\n","188/188 [==============================] - 19s 102ms/step - loss: 0.1448 - accuracy: 0.9568 - val_loss: 0.1133 - val_accuracy: 0.9697\n","Epoch 77/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1438 - accuracy: 0.9576\n","Epoch 00077: val_accuracy improved from 0.96975 to 0.96992, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 102ms/step - loss: 0.1437 - accuracy: 0.9577 - val_loss: 0.1125 - val_accuracy: 0.9699\n","Epoch 78/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1443 - accuracy: 0.9574\n","Epoch 00078: val_accuracy improved from 0.96992 to 0.97017, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1443 - accuracy: 0.9574 - val_loss: 0.1121 - val_accuracy: 0.9702\n","Epoch 79/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1415 - accuracy: 0.9583\n","Epoch 00079: val_accuracy did not improve from 0.97017\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1415 - accuracy: 0.9583 - val_loss: 0.1112 - val_accuracy: 0.9698\n","Epoch 80/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9585\n","Epoch 00080: val_accuracy improved from 0.97017 to 0.97025, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1411 - accuracy: 0.9585 - val_loss: 0.1104 - val_accuracy: 0.9703\n","Epoch 81/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.9582\n","Epoch 00081: val_accuracy improved from 0.97025 to 0.97050, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1413 - accuracy: 0.9582 - val_loss: 0.1095 - val_accuracy: 0.9705\n","Epoch 82/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1391 - accuracy: 0.9595\n","Epoch 00082: val_accuracy improved from 0.97050 to 0.97108, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 21s 111ms/step - loss: 0.1391 - accuracy: 0.9595 - val_loss: 0.1087 - val_accuracy: 0.9711\n","Epoch 83/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1371 - accuracy: 0.9600\n","Epoch 00083: val_accuracy did not improve from 0.97108\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1371 - accuracy: 0.9600 - val_loss: 0.1078 - val_accuracy: 0.9711\n","Epoch 84/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9613\n","Epoch 00084: val_accuracy improved from 0.97108 to 0.97150, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1355 - accuracy: 0.9613 - val_loss: 0.1071 - val_accuracy: 0.9715\n","Epoch 85/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1350 - accuracy: 0.9602\n","Epoch 00085: val_accuracy did not improve from 0.97150\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1350 - accuracy: 0.9602 - val_loss: 0.1069 - val_accuracy: 0.9707\n","Epoch 86/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1341 - accuracy: 0.9610\n","Epoch 00086: val_accuracy did not improve from 0.97150\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1340 - accuracy: 0.9610 - val_loss: 0.1060 - val_accuracy: 0.9713\n","Epoch 87/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1328 - accuracy: 0.9604\n","Epoch 00087: val_accuracy improved from 0.97150 to 0.97200, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1328 - accuracy: 0.9604 - val_loss: 0.1054 - val_accuracy: 0.9720\n","Epoch 88/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1333 - accuracy: 0.9612\n","Epoch 00088: val_accuracy did not improve from 0.97200\n","188/188 [==============================] - 19s 102ms/step - loss: 0.1333 - accuracy: 0.9612 - val_loss: 0.1048 - val_accuracy: 0.9715\n","Epoch 89/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 0.9609\n","Epoch 00089: val_accuracy did not improve from 0.97200\n","188/188 [==============================] - 19s 102ms/step - loss: 0.1325 - accuracy: 0.9609 - val_loss: 0.1040 - val_accuracy: 0.9720\n","Epoch 90/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1339 - accuracy: 0.9607\n","Epoch 00090: val_accuracy improved from 0.97200 to 0.97225, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 107ms/step - loss: 0.1339 - accuracy: 0.9607 - val_loss: 0.1033 - val_accuracy: 0.9722\n","Epoch 91/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1316 - accuracy: 0.9617\n","Epoch 00091: val_accuracy did not improve from 0.97225\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1317 - accuracy: 0.9617 - val_loss: 0.1030 - val_accuracy: 0.9722\n","Epoch 92/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9607\n","Epoch 00092: val_accuracy did not improve from 0.97225\n","188/188 [==============================] - 19s 104ms/step - loss: 0.1319 - accuracy: 0.9607 - val_loss: 0.1033 - val_accuracy: 0.9717\n","Epoch 93/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9611\n","Epoch 00093: val_accuracy did not improve from 0.97225\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1307 - accuracy: 0.9611 - val_loss: 0.1020 - val_accuracy: 0.9720\n","Epoch 94/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.9615\n","Epoch 00094: val_accuracy improved from 0.97225 to 0.97250, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1295 - accuracy: 0.9615 - val_loss: 0.1015 - val_accuracy: 0.9725\n","Epoch 95/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1302 - accuracy: 0.9616\n","Epoch 00095: val_accuracy did not improve from 0.97250\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1302 - accuracy: 0.9616 - val_loss: 0.1008 - val_accuracy: 0.9723\n","Epoch 96/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1283 - accuracy: 0.9617\n","Epoch 00096: val_accuracy improved from 0.97250 to 0.97292, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1283 - accuracy: 0.9617 - val_loss: 0.1001 - val_accuracy: 0.9729\n","Epoch 97/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9617\n","Epoch 00097: val_accuracy did not improve from 0.97292\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1272 - accuracy: 0.9617 - val_loss: 0.0996 - val_accuracy: 0.9729\n","Epoch 98/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9637\n","Epoch 00098: val_accuracy did not improve from 0.97292\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1252 - accuracy: 0.9637 - val_loss: 0.0991 - val_accuracy: 0.9724\n","Epoch 99/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9621\n","Epoch 00099: val_accuracy did not improve from 0.97292\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1272 - accuracy: 0.9621 - val_loss: 0.0988 - val_accuracy: 0.9728\n","Epoch 100/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1269 - accuracy: 0.9622\n","Epoch 00100: val_accuracy did not improve from 0.97292\n","188/188 [==============================] - 19s 104ms/step - loss: 0.1269 - accuracy: 0.9622 - val_loss: 0.0980 - val_accuracy: 0.9729\n","Epoch 101/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 0.9629\n","Epoch 00101: val_accuracy did not improve from 0.97292\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1262 - accuracy: 0.9629 - val_loss: 0.0976 - val_accuracy: 0.9728\n","Epoch 102/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9637\n","Epoch 00102: val_accuracy did not improve from 0.97292\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1250 - accuracy: 0.9637 - val_loss: 0.0975 - val_accuracy: 0.9729\n","Epoch 103/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9629\n","Epoch 00103: val_accuracy did not improve from 0.97292\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1247 - accuracy: 0.9629 - val_loss: 0.0967 - val_accuracy: 0.9728\n","Epoch 104/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9639\n","Epoch 00104: val_accuracy improved from 0.97292 to 0.97358, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1225 - accuracy: 0.9639 - val_loss: 0.0961 - val_accuracy: 0.9736\n","Epoch 105/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9638\n","Epoch 00105: val_accuracy did not improve from 0.97358\n","188/188 [==============================] - 19s 104ms/step - loss: 0.1227 - accuracy: 0.9638 - val_loss: 0.0956 - val_accuracy: 0.9735\n","Epoch 106/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9642\n","Epoch 00106: val_accuracy did not improve from 0.97358\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1219 - accuracy: 0.9642 - val_loss: 0.0952 - val_accuracy: 0.9734\n","Epoch 107/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9647\n","Epoch 00107: val_accuracy did not improve from 0.97358\n","188/188 [==============================] - 19s 102ms/step - loss: 0.1214 - accuracy: 0.9647 - val_loss: 0.0947 - val_accuracy: 0.9735\n","Epoch 108/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9636\n","Epoch 00108: val_accuracy improved from 0.97358 to 0.97383, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1214 - accuracy: 0.9636 - val_loss: 0.0943 - val_accuracy: 0.9738\n","Epoch 109/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9638\n","Epoch 00109: val_accuracy did not improve from 0.97383\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1222 - accuracy: 0.9638 - val_loss: 0.0940 - val_accuracy: 0.9733\n","Epoch 110/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1195 - accuracy: 0.9650\n","Epoch 00110: val_accuracy improved from 0.97383 to 0.97392, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 107ms/step - loss: 0.1194 - accuracy: 0.9650 - val_loss: 0.0934 - val_accuracy: 0.9739\n","Epoch 111/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9647\n","Epoch 00111: val_accuracy did not improve from 0.97392\n","188/188 [==============================] - 19s 102ms/step - loss: 0.1187 - accuracy: 0.9647 - val_loss: 0.0931 - val_accuracy: 0.9737\n","Epoch 112/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9634\n","Epoch 00112: val_accuracy improved from 0.97392 to 0.97442, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1204 - accuracy: 0.9634 - val_loss: 0.0929 - val_accuracy: 0.9744\n","Epoch 113/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9653\n","Epoch 00113: val_accuracy did not improve from 0.97442\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1193 - accuracy: 0.9653 - val_loss: 0.0922 - val_accuracy: 0.9737\n","Epoch 114/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9661\n","Epoch 00114: val_accuracy did not improve from 0.97442\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1172 - accuracy: 0.9661 - val_loss: 0.0920 - val_accuracy: 0.9743\n","Epoch 115/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1168 - accuracy: 0.9650\n","Epoch 00115: val_accuracy improved from 0.97442 to 0.97458, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1168 - accuracy: 0.9650 - val_loss: 0.0915 - val_accuracy: 0.9746\n","Epoch 116/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9658\n","Epoch 00116: val_accuracy did not improve from 0.97458\n","188/188 [==============================] - 21s 110ms/step - loss: 0.1163 - accuracy: 0.9658 - val_loss: 0.0913 - val_accuracy: 0.9745\n","Epoch 117/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9654\n","Epoch 00117: val_accuracy improved from 0.97458 to 0.97467, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 108ms/step - loss: 0.1163 - accuracy: 0.9654 - val_loss: 0.0911 - val_accuracy: 0.9747\n","Epoch 118/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9655\n","Epoch 00118: val_accuracy improved from 0.97467 to 0.97483, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1170 - accuracy: 0.9655 - val_loss: 0.0903 - val_accuracy: 0.9748\n","Epoch 119/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9650\n","Epoch 00119: val_accuracy did not improve from 0.97483\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1173 - accuracy: 0.9650 - val_loss: 0.0901 - val_accuracy: 0.9744\n","Epoch 120/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1167 - accuracy: 0.9657\n","Epoch 00120: val_accuracy improved from 0.97483 to 0.97525, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1167 - accuracy: 0.9657 - val_loss: 0.0897 - val_accuracy: 0.9753\n","Epoch 121/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1139 - accuracy: 0.9658\n","Epoch 00121: val_accuracy improved from 0.97525 to 0.97558, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 109ms/step - loss: 0.1139 - accuracy: 0.9658 - val_loss: 0.0894 - val_accuracy: 0.9756\n","Epoch 122/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9661\n","Epoch 00122: val_accuracy did not improve from 0.97558\n","188/188 [==============================] - 21s 110ms/step - loss: 0.1136 - accuracy: 0.9661 - val_loss: 0.0892 - val_accuracy: 0.9748\n","Epoch 123/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9665\n","Epoch 00123: val_accuracy did not improve from 0.97558\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1142 - accuracy: 0.9665 - val_loss: 0.0888 - val_accuracy: 0.9754\n","Epoch 124/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9655\n","Epoch 00124: val_accuracy did not improve from 0.97558\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1134 - accuracy: 0.9655 - val_loss: 0.0884 - val_accuracy: 0.9753\n","Epoch 125/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.9653\n","Epoch 00125: val_accuracy did not improve from 0.97558\n","188/188 [==============================] - 20s 107ms/step - loss: 0.1132 - accuracy: 0.9653 - val_loss: 0.0882 - val_accuracy: 0.9750\n","Epoch 126/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9662\n","Epoch 00126: val_accuracy improved from 0.97558 to 0.97567, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1134 - accuracy: 0.9662 - val_loss: 0.0879 - val_accuracy: 0.9757\n","Epoch 127/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9670\n","Epoch 00127: val_accuracy did not improve from 0.97567\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1121 - accuracy: 0.9670 - val_loss: 0.0876 - val_accuracy: 0.9754\n","Epoch 128/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.9666\n","Epoch 00128: val_accuracy did not improve from 0.97567\n","188/188 [==============================] - 20s 109ms/step - loss: 0.1118 - accuracy: 0.9666 - val_loss: 0.0872 - val_accuracy: 0.9756\n","Epoch 129/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9664\n","Epoch 00129: val_accuracy improved from 0.97567 to 0.97575, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1128 - accuracy: 0.9664 - val_loss: 0.0869 - val_accuracy: 0.9758\n","Epoch 130/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9666\n","Epoch 00130: val_accuracy did not improve from 0.97575\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1126 - accuracy: 0.9666 - val_loss: 0.0869 - val_accuracy: 0.9757\n","Epoch 131/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1107 - accuracy: 0.9670\n","Epoch 00131: val_accuracy did not improve from 0.97575\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1107 - accuracy: 0.9670 - val_loss: 0.0864 - val_accuracy: 0.9753\n","Epoch 132/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1107 - accuracy: 0.9676\n","Epoch 00132: val_accuracy improved from 0.97575 to 0.97583, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1107 - accuracy: 0.9675 - val_loss: 0.0862 - val_accuracy: 0.9758\n","Epoch 133/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9666\n","Epoch 00133: val_accuracy improved from 0.97583 to 0.97633, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1112 - accuracy: 0.9666 - val_loss: 0.0857 - val_accuracy: 0.9763\n","Epoch 134/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1111 - accuracy: 0.9671\n","Epoch 00134: val_accuracy did not improve from 0.97633\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1111 - accuracy: 0.9671 - val_loss: 0.0854 - val_accuracy: 0.9762\n","Epoch 135/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1093 - accuracy: 0.9678\n","Epoch 00135: val_accuracy did not improve from 0.97633\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1092 - accuracy: 0.9678 - val_loss: 0.0851 - val_accuracy: 0.9763\n","Epoch 136/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9682\n","Epoch 00136: val_accuracy did not improve from 0.97633\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1084 - accuracy: 0.9681 - val_loss: 0.0849 - val_accuracy: 0.9760\n","Epoch 137/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9663\n","Epoch 00137: val_accuracy improved from 0.97633 to 0.97650, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 107ms/step - loss: 0.1114 - accuracy: 0.9663 - val_loss: 0.0844 - val_accuracy: 0.9765\n","Epoch 138/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9684\n","Epoch 00138: val_accuracy did not improve from 0.97650\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1087 - accuracy: 0.9684 - val_loss: 0.0842 - val_accuracy: 0.9763\n","Epoch 139/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1083 - accuracy: 0.9674\n","Epoch 00139: val_accuracy did not improve from 0.97650\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1083 - accuracy: 0.9674 - val_loss: 0.0839 - val_accuracy: 0.9762\n","Epoch 140/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9678\n","Epoch 00140: val_accuracy did not improve from 0.97650\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1089 - accuracy: 0.9678 - val_loss: 0.0838 - val_accuracy: 0.9765\n","Epoch 141/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9678\n","Epoch 00141: val_accuracy did not improve from 0.97650\n","188/188 [==============================] - 19s 104ms/step - loss: 0.1077 - accuracy: 0.9678 - val_loss: 0.0835 - val_accuracy: 0.9763\n","Epoch 142/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9684\n","Epoch 00142: val_accuracy improved from 0.97650 to 0.97692, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 107ms/step - loss: 0.1067 - accuracy: 0.9684 - val_loss: 0.0832 - val_accuracy: 0.9769\n","Epoch 143/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9678\n","Epoch 00143: val_accuracy did not improve from 0.97692\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1086 - accuracy: 0.9678 - val_loss: 0.0830 - val_accuracy: 0.9768\n","Epoch 144/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9676\n","Epoch 00144: val_accuracy improved from 0.97692 to 0.97725, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 107ms/step - loss: 0.1071 - accuracy: 0.9676 - val_loss: 0.0828 - val_accuracy: 0.9772\n","Epoch 145/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9687\n","Epoch 00145: val_accuracy did not improve from 0.97725\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1071 - accuracy: 0.9687 - val_loss: 0.0826 - val_accuracy: 0.9770\n","Epoch 146/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9683\n","Epoch 00146: val_accuracy improved from 0.97725 to 0.97775, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1061 - accuracy: 0.9683 - val_loss: 0.0823 - val_accuracy: 0.9778\n","Epoch 147/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9675\n","Epoch 00147: val_accuracy did not improve from 0.97775\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1074 - accuracy: 0.9675 - val_loss: 0.0819 - val_accuracy: 0.9775\n","Epoch 148/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9684\n","Epoch 00148: val_accuracy did not improve from 0.97775\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1070 - accuracy: 0.9684 - val_loss: 0.0818 - val_accuracy: 0.9770\n","Epoch 149/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9693\n","Epoch 00149: val_accuracy did not improve from 0.97775\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1048 - accuracy: 0.9693 - val_loss: 0.0817 - val_accuracy: 0.9772\n","Epoch 150/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1063 - accuracy: 0.9676\n","Epoch 00150: val_accuracy did not improve from 0.97775\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1063 - accuracy: 0.9676 - val_loss: 0.0814 - val_accuracy: 0.9770\n","Epoch 151/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9689\n","Epoch 00151: val_accuracy did not improve from 0.97775\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1047 - accuracy: 0.9689 - val_loss: 0.0811 - val_accuracy: 0.9776\n","Epoch 152/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.9685\n","Epoch 00152: val_accuracy did not improve from 0.97775\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1038 - accuracy: 0.9685 - val_loss: 0.0811 - val_accuracy: 0.9768\n","Epoch 153/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1052 - accuracy: 0.9680\n","Epoch 00153: val_accuracy did not improve from 0.97775\n","188/188 [==============================] - 20s 107ms/step - loss: 0.1052 - accuracy: 0.9680 - val_loss: 0.0807 - val_accuracy: 0.9769\n","Epoch 154/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1053 - accuracy: 0.9685\n","Epoch 00154: val_accuracy did not improve from 0.97775\n","188/188 [==============================] - 19s 101ms/step - loss: 0.1053 - accuracy: 0.9685 - val_loss: 0.0806 - val_accuracy: 0.9773\n","Epoch 155/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9679\n","Epoch 00155: val_accuracy did not improve from 0.97775\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1061 - accuracy: 0.9679 - val_loss: 0.0803 - val_accuracy: 0.9776\n","Epoch 156/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.9689\n","Epoch 00156: val_accuracy improved from 0.97775 to 0.97783, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1038 - accuracy: 0.9689 - val_loss: 0.0802 - val_accuracy: 0.9778\n","Epoch 157/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9689\n","Epoch 00157: val_accuracy did not improve from 0.97783\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1035 - accuracy: 0.9689 - val_loss: 0.0801 - val_accuracy: 0.9771\n","Epoch 158/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9689\n","Epoch 00158: val_accuracy did not improve from 0.97783\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1029 - accuracy: 0.9689 - val_loss: 0.0796 - val_accuracy: 0.9778\n","Epoch 159/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.9691\n","Epoch 00159: val_accuracy did not improve from 0.97783\n","188/188 [==============================] - 24s 130ms/step - loss: 0.1028 - accuracy: 0.9691 - val_loss: 0.0794 - val_accuracy: 0.9772\n","Epoch 160/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1031 - accuracy: 0.9686\n","Epoch 00160: val_accuracy did not improve from 0.97783\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1032 - accuracy: 0.9686 - val_loss: 0.0792 - val_accuracy: 0.9776\n","Epoch 161/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9691\n","Epoch 00161: val_accuracy improved from 0.97783 to 0.97792, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1043 - accuracy: 0.9691 - val_loss: 0.0788 - val_accuracy: 0.9779\n","Epoch 162/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9690\n","Epoch 00162: val_accuracy improved from 0.97792 to 0.97808, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1016 - accuracy: 0.9690 - val_loss: 0.0786 - val_accuracy: 0.9781\n","Epoch 163/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9694\n","Epoch 00163: val_accuracy improved from 0.97808 to 0.97817, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 104ms/step - loss: 0.1018 - accuracy: 0.9694 - val_loss: 0.0786 - val_accuracy: 0.9782\n","Epoch 164/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.9695\n","Epoch 00164: val_accuracy did not improve from 0.97817\n","188/188 [==============================] - 20s 104ms/step - loss: 0.1009 - accuracy: 0.9695 - val_loss: 0.0784 - val_accuracy: 0.9781\n","Epoch 165/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9693\n","Epoch 00165: val_accuracy improved from 0.97817 to 0.97842, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 106ms/step - loss: 0.1026 - accuracy: 0.9693 - val_loss: 0.0782 - val_accuracy: 0.9784\n","Epoch 166/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1006 - accuracy: 0.9702\n","Epoch 00166: val_accuracy improved from 0.97842 to 0.97850, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.1006 - accuracy: 0.9702 - val_loss: 0.0779 - val_accuracy: 0.9785\n","Epoch 167/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9701\n","Epoch 00167: val_accuracy did not improve from 0.97850\n","188/188 [==============================] - 19s 103ms/step - loss: 0.0989 - accuracy: 0.9701 - val_loss: 0.0777 - val_accuracy: 0.9782\n","Epoch 168/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9699\n","Epoch 00168: val_accuracy did not improve from 0.97850\n","188/188 [==============================] - 20s 107ms/step - loss: 0.1001 - accuracy: 0.9699 - val_loss: 0.0776 - val_accuracy: 0.9783\n","Epoch 169/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9713\n","Epoch 00169: val_accuracy improved from 0.97850 to 0.97867, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 105ms/step - loss: 0.0988 - accuracy: 0.9713 - val_loss: 0.0774 - val_accuracy: 0.9787\n","Epoch 170/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9695\n","Epoch 00170: val_accuracy did not improve from 0.97867\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1016 - accuracy: 0.9696 - val_loss: 0.0772 - val_accuracy: 0.9787\n","Epoch 171/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9697\n","Epoch 00171: val_accuracy did not improve from 0.97867\n","188/188 [==============================] - 19s 103ms/step - loss: 0.1012 - accuracy: 0.9697 - val_loss: 0.0771 - val_accuracy: 0.9786\n","Epoch 172/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0992 - accuracy: 0.9700\n","Epoch 00172: val_accuracy improved from 0.97867 to 0.97883, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.0992 - accuracy: 0.9700 - val_loss: 0.0769 - val_accuracy: 0.9788\n","Epoch 173/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1001 - accuracy: 0.9698\n","Epoch 00173: val_accuracy did not improve from 0.97883\n","188/188 [==============================] - 19s 104ms/step - loss: 0.1002 - accuracy: 0.9697 - val_loss: 0.0765 - val_accuracy: 0.9786\n","Epoch 174/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9708\n","Epoch 00174: val_accuracy did not improve from 0.97883\n","188/188 [==============================] - 20s 105ms/step - loss: 0.0991 - accuracy: 0.9708 - val_loss: 0.0768 - val_accuracy: 0.9787\n","Epoch 175/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9698\n","Epoch 00175: val_accuracy did not improve from 0.97883\n","188/188 [==============================] - 20s 107ms/step - loss: 0.0991 - accuracy: 0.9698 - val_loss: 0.0764 - val_accuracy: 0.9787\n","Epoch 176/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9703\n","Epoch 00176: val_accuracy did not improve from 0.97883\n","188/188 [==============================] - 20s 105ms/step - loss: 0.0983 - accuracy: 0.9703 - val_loss: 0.0763 - val_accuracy: 0.9785\n","Epoch 177/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9717\n","Epoch 00177: val_accuracy did not improve from 0.97883\n","188/188 [==============================] - 20s 106ms/step - loss: 0.0964 - accuracy: 0.9717 - val_loss: 0.0761 - val_accuracy: 0.9785\n","Epoch 178/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9708\n","Epoch 00178: val_accuracy did not improve from 0.97883\n","188/188 [==============================] - 20s 105ms/step - loss: 0.0982 - accuracy: 0.9709 - val_loss: 0.0760 - val_accuracy: 0.9784\n","Epoch 179/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.9716\n","Epoch 00179: val_accuracy improved from 0.97883 to 0.97900, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 19s 104ms/step - loss: 0.0974 - accuracy: 0.9716 - val_loss: 0.0757 - val_accuracy: 0.9790\n","Epoch 180/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9710\n","Epoch 00180: val_accuracy did not improve from 0.97900\n","188/188 [==============================] - 20s 106ms/step - loss: 0.0958 - accuracy: 0.9710 - val_loss: 0.0757 - val_accuracy: 0.9788\n","Epoch 181/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9709\n","Epoch 00181: val_accuracy did not improve from 0.97900\n","188/188 [==============================] - 20s 104ms/step - loss: 0.0970 - accuracy: 0.9709 - val_loss: 0.0755 - val_accuracy: 0.9790\n","Epoch 182/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9718\n","Epoch 00182: val_accuracy did not improve from 0.97900\n","188/188 [==============================] - 20s 107ms/step - loss: 0.0967 - accuracy: 0.9718 - val_loss: 0.0752 - val_accuracy: 0.9789\n","Epoch 183/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.9709\n","Epoch 00183: val_accuracy did not improve from 0.97900\n","188/188 [==============================] - 20s 104ms/step - loss: 0.0974 - accuracy: 0.9709 - val_loss: 0.0751 - val_accuracy: 0.9789\n","Epoch 184/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9701\n","Epoch 00184: val_accuracy did not improve from 0.97900\n","188/188 [==============================] - 20s 107ms/step - loss: 0.0971 - accuracy: 0.9701 - val_loss: 0.0749 - val_accuracy: 0.9787\n","Epoch 185/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0965 - accuracy: 0.9711\n","Epoch 00185: val_accuracy did not improve from 0.97900\n","188/188 [==============================] - 20s 104ms/step - loss: 0.0965 - accuracy: 0.9711 - val_loss: 0.0748 - val_accuracy: 0.9790\n","Epoch 186/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9713\n","Epoch 00186: val_accuracy improved from 0.97900 to 0.97925, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.0960 - accuracy: 0.9713 - val_loss: 0.0745 - val_accuracy: 0.9793\n","Epoch 187/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9703\n","Epoch 00187: val_accuracy did not improve from 0.97925\n","188/188 [==============================] - 20s 105ms/step - loss: 0.0985 - accuracy: 0.9703 - val_loss: 0.0745 - val_accuracy: 0.9791\n","Epoch 188/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0976 - accuracy: 0.9706\n","Epoch 00188: val_accuracy did not improve from 0.97925\n","188/188 [==============================] - 20s 105ms/step - loss: 0.0976 - accuracy: 0.9706 - val_loss: 0.0744 - val_accuracy: 0.9792\n","Epoch 189/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9715\n","Epoch 00189: val_accuracy did not improve from 0.97925\n","188/188 [==============================] - 20s 107ms/step - loss: 0.0967 - accuracy: 0.9715 - val_loss: 0.0741 - val_accuracy: 0.9791\n","Epoch 190/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9712\n","Epoch 00190: val_accuracy improved from 0.97925 to 0.97942, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 24s 127ms/step - loss: 0.0952 - accuracy: 0.9712 - val_loss: 0.0739 - val_accuracy: 0.9794\n","Epoch 191/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0953 - accuracy: 0.9705\n","Epoch 00191: val_accuracy did not improve from 0.97942\n","188/188 [==============================] - 20s 104ms/step - loss: 0.0951 - accuracy: 0.9706 - val_loss: 0.0739 - val_accuracy: 0.9793\n","Epoch 192/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9714\n","Epoch 00192: val_accuracy did not improve from 0.97942\n","188/188 [==============================] - 20s 107ms/step - loss: 0.0959 - accuracy: 0.9714 - val_loss: 0.0738 - val_accuracy: 0.9793\n","Epoch 193/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9715\n","Epoch 00193: val_accuracy did not improve from 0.97942\n","188/188 [==============================] - 20s 105ms/step - loss: 0.0938 - accuracy: 0.9715 - val_loss: 0.0735 - val_accuracy: 0.9791\n","Epoch 194/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9707\n","Epoch 00194: val_accuracy did not improve from 0.97942\n","188/188 [==============================] - 20s 105ms/step - loss: 0.0955 - accuracy: 0.9707 - val_loss: 0.0735 - val_accuracy: 0.9794\n","Epoch 195/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9721\n","Epoch 00195: val_accuracy did not improve from 0.97942\n","188/188 [==============================] - 20s 105ms/step - loss: 0.0940 - accuracy: 0.9721 - val_loss: 0.0736 - val_accuracy: 0.9790\n","Epoch 196/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0953 - accuracy: 0.9713\n","Epoch 00196: val_accuracy improved from 0.97942 to 0.97983, saving model to mnist_conv_drop_best.h5\n","188/188 [==============================] - 20s 104ms/step - loss: 0.0953 - accuracy: 0.9713 - val_loss: 0.0731 - val_accuracy: 0.9798\n","Epoch 197/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0932 - accuracy: 0.9730\n","Epoch 00197: val_accuracy did not improve from 0.97983\n","188/188 [==============================] - 20s 104ms/step - loss: 0.0931 - accuracy: 0.9731 - val_loss: 0.0729 - val_accuracy: 0.9797\n","Epoch 198/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9715\n","Epoch 00198: val_accuracy did not improve from 0.97983\n","188/188 [==============================] - 20s 105ms/step - loss: 0.0955 - accuracy: 0.9715 - val_loss: 0.0729 - val_accuracy: 0.9793\n","Epoch 199/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.9709\n","Epoch 00199: val_accuracy did not improve from 0.97983\n","188/188 [==============================] - 20s 109ms/step - loss: 0.0941 - accuracy: 0.9709 - val_loss: 0.0727 - val_accuracy: 0.9793\n","Epoch 200/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9726\n","Epoch 00200: val_accuracy did not improve from 0.97983\n","188/188 [==============================] - 20s 108ms/step - loss: 0.0917 - accuracy: 0.9726 - val_loss: 0.0728 - val_accuracy: 0.9793\n","Epoch 201/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9719\n","Epoch 00201: val_accuracy did not improve from 0.97983\n","188/188 [==============================] - 20s 104ms/step - loss: 0.0921 - accuracy: 0.9719 - val_loss: 0.0724 - val_accuracy: 0.9797\n","Epoch 202/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0925 - accuracy: 0.9724\n","Epoch 00202: val_accuracy did not improve from 0.97983\n","188/188 [==============================] - 20s 106ms/step - loss: 0.0925 - accuracy: 0.9724 - val_loss: 0.0724 - val_accuracy: 0.9794\n","Epoch 203/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9721\n","Epoch 00203: val_accuracy did not improve from 0.97983\n","188/188 [==============================] - 20s 106ms/step - loss: 0.0927 - accuracy: 0.9721 - val_loss: 0.0724 - val_accuracy: 0.9795\n","Epoch 204/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9719\n","Epoch 00204: val_accuracy did not improve from 0.97983\n","188/188 [==============================] - 20s 105ms/step - loss: 0.0922 - accuracy: 0.9719 - val_loss: 0.0720 - val_accuracy: 0.9795\n","Epoch 205/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.0929 - accuracy: 0.9720\n","Epoch 00205: val_accuracy did not improve from 0.97983\n","188/188 [==============================] - 20s 106ms/step - loss: 0.0930 - accuracy: 0.9719 - val_loss: 0.0717 - val_accuracy: 0.9795\n","Epoch 206/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9723\n","Epoch 00206: val_accuracy did not improve from 0.97983\n","188/188 [==============================] - 20s 105ms/step - loss: 0.0919 - accuracy: 0.9723 - val_loss: 0.0718 - val_accuracy: 0.9797\n","Epoch 00206: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pCe4uVs2AkIW","colab_type":"text"},"source":["By looking at the evolution, we can see that the performance of the model on the training data is now lower."]},{"cell_type":"code","metadata":{"id":"doyFaOZdEYCS","colab_type":"code","outputId":"6d2ca495-ec59-4314-a88b-068ce7b84481","executionInfo":{"status":"ok","timestamp":1590670817134,"user_tz":-60,"elapsed":4093301,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"colab":{"base_uri":"https://localhost:8080/","height":366}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_conv_drop_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_conv_drop_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_conv_drop_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_conv_drop_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"rtc9e509BLv0","colab_type":"text"},"source":["And we can assess the performance on the test set:"]},{"cell_type":"code","metadata":{"id":"skwDZOoSEYkY","colab_type":"code","outputId":"5c187f50-67bc-4301-cdcf-2dc68e1fa56d","executionInfo":{"status":"ok","timestamp":1590670819482,"user_tz":-60,"elapsed":4095633,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["mnist_conv_drop_model.load_weights('mnist_conv_drop_best.h5')\n","loss, acc = mnist_conv_drop_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 2s 7ms/step - loss: 0.0629 - accuracy: 0.9808\n","Accuracy: 0.9807999730110168\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vZmnDseW4J8S","colab_type":"text"},"source":["##### Combined L2 + dropout"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"l84wNnzi5xcu","outputId":"33a29319-8560-4e59-c08a-db029dbada8b","executionInfo":{"status":"ok","timestamp":1590685071644,"user_tz":-60,"elapsed":1030,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["mnist_l2_drop_model = tf.keras.Sequential(name='mnist_cnn_l2_dropout')\n","mnist_l2_drop_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n","mnist_l2_drop_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=4, activation='relu', padding='same', name='convolution', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n","mnist_l2_drop_model.add(tf.keras.layers.MaxPool2D(pool_size=2, name='pooling'))\n","mnist_l2_drop_model.add(tf.keras.layers.Dropout(0.5, name='dropout'))\n","mnist_l2_drop_model.add(tf.keras.layers.Flatten(name='flatten'))\n","mnist_l2_drop_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n","mnist_l2_drop_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","mnist_l2_drop_model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"mnist_cnn_l2_dropout\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","convolution (Conv2D)         (None, 28, 28, 16)        272       \n","_________________________________________________________________\n","pooling (MaxPooling2D)       (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 3136)              0         \n","_________________________________________________________________\n","output (Dense)               (None, 10)                31370     \n","=================================================================\n","Total params: 31,642\n","Trainable params: 31,642\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"8e97752d-07e0-4ae8-8cb3-de4d8e885fc2","executionInfo":{"status":"ok","timestamp":1590689485988,"user_tz":-60,"elapsed":4222155,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"b1fGGF-KOZ9W","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_l2_drop_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","mnist_conv_l2_drop_model_train = mnist_l2_drop_model.fit(mnist_train_x, mnist_train_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/10000\n","187/188 [============================>.] - ETA: 0s - loss: 1.7256 - accuracy: 0.5061\n","Epoch 00001: val_accuracy improved from -inf to 0.81567, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 1.7237 - accuracy: 0.5067 - val_loss: 0.9113 - val_accuracy: 0.8157\n","Epoch 2/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.7784 - accuracy: 0.7840\n","Epoch 00002: val_accuracy improved from 0.81567 to 0.86175, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.7783 - accuracy: 0.7839 - val_loss: 0.5592 - val_accuracy: 0.8618\n","Epoch 3/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.6171 - accuracy: 0.8242\n","Epoch 00003: val_accuracy improved from 0.86175 to 0.87533, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.6170 - accuracy: 0.8243 - val_loss: 0.4846 - val_accuracy: 0.8753\n","Epoch 4/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.8433\n","Epoch 00004: val_accuracy improved from 0.87533 to 0.88608, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.5626 - accuracy: 0.8433 - val_loss: 0.4508 - val_accuracy: 0.8861\n","Epoch 5/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.5335 - accuracy: 0.8521\n","Epoch 00005: val_accuracy improved from 0.88608 to 0.89158, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.5336 - accuracy: 0.8521 - val_loss: 0.4325 - val_accuracy: 0.8916\n","Epoch 6/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.5102 - accuracy: 0.8603\n","Epoch 00006: val_accuracy improved from 0.89158 to 0.89358, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.5102 - accuracy: 0.8604 - val_loss: 0.4203 - val_accuracy: 0.8936\n","Epoch 7/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4988 - accuracy: 0.8636\n","Epoch 00007: val_accuracy improved from 0.89358 to 0.89600, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.4987 - accuracy: 0.8636 - val_loss: 0.4110 - val_accuracy: 0.8960\n","Epoch 8/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4844 - accuracy: 0.8682\n","Epoch 00008: val_accuracy improved from 0.89600 to 0.89650, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.4846 - accuracy: 0.8680 - val_loss: 0.4020 - val_accuracy: 0.8965\n","Epoch 9/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4764 - accuracy: 0.8696\n","Epoch 00009: val_accuracy improved from 0.89650 to 0.90158, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.4762 - accuracy: 0.8696 - val_loss: 0.3951 - val_accuracy: 0.9016\n","Epoch 10/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4616 - accuracy: 0.8743\n","Epoch 00010: val_accuracy improved from 0.90158 to 0.90233, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.4619 - accuracy: 0.8743 - val_loss: 0.3902 - val_accuracy: 0.9023\n","Epoch 11/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4592 - accuracy: 0.8752\n","Epoch 00011: val_accuracy improved from 0.90233 to 0.90483, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.4590 - accuracy: 0.8752 - val_loss: 0.3851 - val_accuracy: 0.9048\n","Epoch 12/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4533 - accuracy: 0.8765\n","Epoch 00012: val_accuracy improved from 0.90483 to 0.90517, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.4534 - accuracy: 0.8766 - val_loss: 0.3800 - val_accuracy: 0.9052\n","Epoch 13/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4434 - accuracy: 0.8810\n","Epoch 00013: val_accuracy improved from 0.90517 to 0.90700, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.4437 - accuracy: 0.8809 - val_loss: 0.3759 - val_accuracy: 0.9070\n","Epoch 14/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4395 - accuracy: 0.8819\n","Epoch 00014: val_accuracy improved from 0.90700 to 0.90833, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.4397 - accuracy: 0.8819 - val_loss: 0.3723 - val_accuracy: 0.9083\n","Epoch 15/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4333 - accuracy: 0.8845\n","Epoch 00015: val_accuracy did not improve from 0.90833\n","188/188 [==============================] - 18s 94ms/step - loss: 0.4329 - accuracy: 0.8847 - val_loss: 0.3681 - val_accuracy: 0.9082\n","Epoch 16/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.4276 - accuracy: 0.8851\n","Epoch 00016: val_accuracy improved from 0.90833 to 0.90875, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.4276 - accuracy: 0.8851 - val_loss: 0.3658 - val_accuracy: 0.9087\n","Epoch 17/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4244 - accuracy: 0.8864\n","Epoch 00017: val_accuracy improved from 0.90875 to 0.91158, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.4245 - accuracy: 0.8864 - val_loss: 0.3620 - val_accuracy: 0.9116\n","Epoch 18/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4159 - accuracy: 0.8888\n","Epoch 00018: val_accuracy improved from 0.91158 to 0.91233, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.4161 - accuracy: 0.8888 - val_loss: 0.3572 - val_accuracy: 0.9123\n","Epoch 19/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4127 - accuracy: 0.8908\n","Epoch 00019: val_accuracy improved from 0.91233 to 0.91358, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.4128 - accuracy: 0.8907 - val_loss: 0.3534 - val_accuracy: 0.9136\n","Epoch 20/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4061 - accuracy: 0.8929\n","Epoch 00020: val_accuracy improved from 0.91358 to 0.91575, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.4060 - accuracy: 0.8930 - val_loss: 0.3505 - val_accuracy: 0.9158\n","Epoch 21/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.4028 - accuracy: 0.8935\n","Epoch 00021: val_accuracy improved from 0.91575 to 0.91717, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.4032 - accuracy: 0.8934 - val_loss: 0.3460 - val_accuracy: 0.9172\n","Epoch 22/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3998 - accuracy: 0.8956\n","Epoch 00022: val_accuracy improved from 0.91717 to 0.91850, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3996 - accuracy: 0.8957 - val_loss: 0.3428 - val_accuracy: 0.9185\n","Epoch 23/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3947 - accuracy: 0.8973\n","Epoch 00023: val_accuracy improved from 0.91850 to 0.92000, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3948 - accuracy: 0.8973 - val_loss: 0.3397 - val_accuracy: 0.9200\n","Epoch 24/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3919 - accuracy: 0.8961\n","Epoch 00024: val_accuracy did not improve from 0.92000\n","188/188 [==============================] - 18s 95ms/step - loss: 0.3917 - accuracy: 0.8961 - val_loss: 0.3359 - val_accuracy: 0.9199\n","Epoch 25/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3811 - accuracy: 0.9012\n","Epoch 00025: val_accuracy improved from 0.92000 to 0.92050, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.3810 - accuracy: 0.9012 - val_loss: 0.3325 - val_accuracy: 0.9205\n","Epoch 26/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3800 - accuracy: 0.9007\n","Epoch 00026: val_accuracy improved from 0.92050 to 0.92250, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.3801 - accuracy: 0.9007 - val_loss: 0.3289 - val_accuracy: 0.9225\n","Epoch 27/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3760 - accuracy: 0.9031\n","Epoch 00027: val_accuracy improved from 0.92250 to 0.92367, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.3760 - accuracy: 0.9031 - val_loss: 0.3251 - val_accuracy: 0.9237\n","Epoch 28/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3702 - accuracy: 0.9050\n","Epoch 00028: val_accuracy improved from 0.92367 to 0.92500, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.3705 - accuracy: 0.9050 - val_loss: 0.3220 - val_accuracy: 0.9250\n","Epoch 29/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3700 - accuracy: 0.9059\n","Epoch 00029: val_accuracy improved from 0.92500 to 0.92667, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.3700 - accuracy: 0.9059 - val_loss: 0.3187 - val_accuracy: 0.9267\n","Epoch 30/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3630 - accuracy: 0.9066\n","Epoch 00030: val_accuracy improved from 0.92667 to 0.92800, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3627 - accuracy: 0.9066 - val_loss: 0.3147 - val_accuracy: 0.9280\n","Epoch 31/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3609 - accuracy: 0.9073\n","Epoch 00031: val_accuracy improved from 0.92800 to 0.92925, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 96ms/step - loss: 0.3606 - accuracy: 0.9074 - val_loss: 0.3113 - val_accuracy: 0.9293\n","Epoch 32/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3554 - accuracy: 0.9094\n","Epoch 00032: val_accuracy did not improve from 0.92925\n","188/188 [==============================] - 18s 95ms/step - loss: 0.3555 - accuracy: 0.9093 - val_loss: 0.3087 - val_accuracy: 0.9286\n","Epoch 33/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.3514 - accuracy: 0.9107\n","Epoch 00033: val_accuracy improved from 0.92925 to 0.93008, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3514 - accuracy: 0.9107 - val_loss: 0.3059 - val_accuracy: 0.9301\n","Epoch 34/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3499 - accuracy: 0.9125\n","Epoch 00034: val_accuracy improved from 0.93008 to 0.93283, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3500 - accuracy: 0.9125 - val_loss: 0.3015 - val_accuracy: 0.9328\n","Epoch 35/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3444 - accuracy: 0.9143\n","Epoch 00035: val_accuracy did not improve from 0.93283\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3443 - accuracy: 0.9143 - val_loss: 0.2995 - val_accuracy: 0.9323\n","Epoch 36/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3417 - accuracy: 0.9145\n","Epoch 00036: val_accuracy improved from 0.93283 to 0.93367, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3418 - accuracy: 0.9145 - val_loss: 0.2958 - val_accuracy: 0.9337\n","Epoch 37/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3378 - accuracy: 0.9149\n","Epoch 00037: val_accuracy improved from 0.93367 to 0.93533, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3379 - accuracy: 0.9149 - val_loss: 0.2925 - val_accuracy: 0.9353\n","Epoch 38/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3353 - accuracy: 0.9162\n","Epoch 00038: val_accuracy did not improve from 0.93533\n","188/188 [==============================] - 18s 95ms/step - loss: 0.3353 - accuracy: 0.9162 - val_loss: 0.2903 - val_accuracy: 0.9349\n","Epoch 39/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3319 - accuracy: 0.9182\n","Epoch 00039: val_accuracy improved from 0.93533 to 0.93592, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3318 - accuracy: 0.9182 - val_loss: 0.2873 - val_accuracy: 0.9359\n","Epoch 40/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.9181\n","Epoch 00040: val_accuracy improved from 0.93592 to 0.93633, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3293 - accuracy: 0.9180 - val_loss: 0.2847 - val_accuracy: 0.9363\n","Epoch 41/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.9199\n","Epoch 00041: val_accuracy improved from 0.93633 to 0.93717, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3244 - accuracy: 0.9199 - val_loss: 0.2818 - val_accuracy: 0.9372\n","Epoch 42/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3215 - accuracy: 0.9198\n","Epoch 00042: val_accuracy improved from 0.93717 to 0.93875, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3216 - accuracy: 0.9197 - val_loss: 0.2795 - val_accuracy: 0.9388\n","Epoch 43/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.9213\n","Epoch 00043: val_accuracy improved from 0.93875 to 0.93883, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3202 - accuracy: 0.9213 - val_loss: 0.2766 - val_accuracy: 0.9388\n","Epoch 44/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.9223\n","Epoch 00044: val_accuracy improved from 0.93883 to 0.93975, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3163 - accuracy: 0.9223 - val_loss: 0.2750 - val_accuracy: 0.9398\n","Epoch 45/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.9229\n","Epoch 00045: val_accuracy improved from 0.93975 to 0.94050, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.3136 - accuracy: 0.9229 - val_loss: 0.2723 - val_accuracy: 0.9405\n","Epoch 46/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3124 - accuracy: 0.9234\n","Epoch 00046: val_accuracy improved from 0.94050 to 0.94117, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.3128 - accuracy: 0.9234 - val_loss: 0.2701 - val_accuracy: 0.9412\n","Epoch 47/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3079 - accuracy: 0.9258\n","Epoch 00047: val_accuracy improved from 0.94117 to 0.94142, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3077 - accuracy: 0.9258 - val_loss: 0.2683 - val_accuracy: 0.9414\n","Epoch 48/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.9254\n","Epoch 00048: val_accuracy improved from 0.94142 to 0.94242, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.3083 - accuracy: 0.9255 - val_loss: 0.2659 - val_accuracy: 0.9424\n","Epoch 49/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3062 - accuracy: 0.9259\n","Epoch 00049: val_accuracy improved from 0.94242 to 0.94325, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3062 - accuracy: 0.9259 - val_loss: 0.2639 - val_accuracy: 0.9433\n","Epoch 50/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.9277\n","Epoch 00050: val_accuracy improved from 0.94325 to 0.94375, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 96ms/step - loss: 0.3029 - accuracy: 0.9278 - val_loss: 0.2622 - val_accuracy: 0.9438\n","Epoch 51/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.3041 - accuracy: 0.9272\n","Epoch 00051: val_accuracy improved from 0.94375 to 0.94408, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.3043 - accuracy: 0.9271 - val_loss: 0.2605 - val_accuracy: 0.9441\n","Epoch 52/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2991 - accuracy: 0.9282\n","Epoch 00052: val_accuracy improved from 0.94408 to 0.94492, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2991 - accuracy: 0.9282 - val_loss: 0.2583 - val_accuracy: 0.9449\n","Epoch 53/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.9296\n","Epoch 00053: val_accuracy improved from 0.94492 to 0.94575, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2964 - accuracy: 0.9295 - val_loss: 0.2566 - val_accuracy: 0.9457\n","Epoch 54/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.9283\n","Epoch 00054: val_accuracy did not improve from 0.94575\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2975 - accuracy: 0.9283 - val_loss: 0.2547 - val_accuracy: 0.9457\n","Epoch 55/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.9278\n","Epoch 00055: val_accuracy improved from 0.94575 to 0.94592, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2951 - accuracy: 0.9279 - val_loss: 0.2536 - val_accuracy: 0.9459\n","Epoch 56/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.9299\n","Epoch 00056: val_accuracy improved from 0.94592 to 0.94708, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2925 - accuracy: 0.9300 - val_loss: 0.2515 - val_accuracy: 0.9471\n","Epoch 57/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.9306\n","Epoch 00057: val_accuracy did not improve from 0.94708\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2905 - accuracy: 0.9306 - val_loss: 0.2498 - val_accuracy: 0.9469\n","Epoch 58/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9318\n","Epoch 00058: val_accuracy improved from 0.94708 to 0.94717, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2877 - accuracy: 0.9319 - val_loss: 0.2482 - val_accuracy: 0.9472\n","Epoch 59/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9311\n","Epoch 00059: val_accuracy improved from 0.94717 to 0.94875, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2881 - accuracy: 0.9311 - val_loss: 0.2468 - val_accuracy: 0.9488\n","Epoch 60/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.9323\n","Epoch 00060: val_accuracy did not improve from 0.94875\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2842 - accuracy: 0.9324 - val_loss: 0.2453 - val_accuracy: 0.9482\n","Epoch 61/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2820 - accuracy: 0.9327\n","Epoch 00061: val_accuracy did not improve from 0.94875\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2821 - accuracy: 0.9327 - val_loss: 0.2441 - val_accuracy: 0.9480\n","Epoch 62/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2824 - accuracy: 0.9327\n","Epoch 00062: val_accuracy improved from 0.94875 to 0.94883, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2824 - accuracy: 0.9326 - val_loss: 0.2425 - val_accuracy: 0.9488\n","Epoch 63/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2805 - accuracy: 0.9328\n","Epoch 00063: val_accuracy did not improve from 0.94883\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2808 - accuracy: 0.9327 - val_loss: 0.2416 - val_accuracy: 0.9482\n","Epoch 64/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.9336\n","Epoch 00064: val_accuracy did not improve from 0.94883\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2805 - accuracy: 0.9336 - val_loss: 0.2398 - val_accuracy: 0.9488\n","Epoch 65/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.9343\n","Epoch 00065: val_accuracy improved from 0.94883 to 0.95000, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2782 - accuracy: 0.9344 - val_loss: 0.2386 - val_accuracy: 0.9500\n","Epoch 66/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2771 - accuracy: 0.9354\n","Epoch 00066: val_accuracy improved from 0.95000 to 0.95058, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 96ms/step - loss: 0.2774 - accuracy: 0.9353 - val_loss: 0.2373 - val_accuracy: 0.9506\n","Epoch 67/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2754 - accuracy: 0.9359\n","Epoch 00067: val_accuracy did not improve from 0.95058\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2757 - accuracy: 0.9359 - val_loss: 0.2361 - val_accuracy: 0.9503\n","Epoch 68/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2729 - accuracy: 0.9368\n","Epoch 00068: val_accuracy did not improve from 0.95058\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2729 - accuracy: 0.9368 - val_loss: 0.2348 - val_accuracy: 0.9500\n","Epoch 69/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2729 - accuracy: 0.9358\n","Epoch 00069: val_accuracy improved from 0.95058 to 0.95142, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2729 - accuracy: 0.9358 - val_loss: 0.2337 - val_accuracy: 0.9514\n","Epoch 70/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2719 - accuracy: 0.9366\n","Epoch 00070: val_accuracy did not improve from 0.95142\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2719 - accuracy: 0.9365 - val_loss: 0.2327 - val_accuracy: 0.9513\n","Epoch 71/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2712 - accuracy: 0.9355\n","Epoch 00071: val_accuracy improved from 0.95142 to 0.95192, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2712 - accuracy: 0.9355 - val_loss: 0.2314 - val_accuracy: 0.9519\n","Epoch 72/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2676 - accuracy: 0.9374\n","Epoch 00072: val_accuracy did not improve from 0.95192\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2675 - accuracy: 0.9375 - val_loss: 0.2304 - val_accuracy: 0.9514\n","Epoch 73/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2669 - accuracy: 0.9375\n","Epoch 00073: val_accuracy improved from 0.95192 to 0.95242, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2669 - accuracy: 0.9375 - val_loss: 0.2292 - val_accuracy: 0.9524\n","Epoch 74/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2648 - accuracy: 0.9378\n","Epoch 00074: val_accuracy improved from 0.95242 to 0.95292, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2648 - accuracy: 0.9378 - val_loss: 0.2279 - val_accuracy: 0.9529\n","Epoch 75/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2656 - accuracy: 0.9372\n","Epoch 00075: val_accuracy improved from 0.95292 to 0.95317, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2656 - accuracy: 0.9372 - val_loss: 0.2272 - val_accuracy: 0.9532\n","Epoch 76/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.9382\n","Epoch 00076: val_accuracy improved from 0.95317 to 0.95392, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2639 - accuracy: 0.9382 - val_loss: 0.2261 - val_accuracy: 0.9539\n","Epoch 77/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2650 - accuracy: 0.9380\n","Epoch 00077: val_accuracy did not improve from 0.95392\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2647 - accuracy: 0.9381 - val_loss: 0.2251 - val_accuracy: 0.9531\n","Epoch 78/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.9388\n","Epoch 00078: val_accuracy improved from 0.95392 to 0.95450, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2635 - accuracy: 0.9388 - val_loss: 0.2244 - val_accuracy: 0.9545\n","Epoch 79/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2628 - accuracy: 0.9393\n","Epoch 00079: val_accuracy did not improve from 0.95450\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2628 - accuracy: 0.9393 - val_loss: 0.2232 - val_accuracy: 0.9541\n","Epoch 80/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2621 - accuracy: 0.9410\n","Epoch 00080: val_accuracy improved from 0.95450 to 0.95467, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 96ms/step - loss: 0.2619 - accuracy: 0.9411 - val_loss: 0.2226 - val_accuracy: 0.9547\n","Epoch 81/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.9402\n","Epoch 00081: val_accuracy improved from 0.95467 to 0.95592, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2589 - accuracy: 0.9402 - val_loss: 0.2216 - val_accuracy: 0.9559\n","Epoch 82/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2610 - accuracy: 0.9393\n","Epoch 00082: val_accuracy did not improve from 0.95592\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2609 - accuracy: 0.9393 - val_loss: 0.2207 - val_accuracy: 0.9543\n","Epoch 83/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2561 - accuracy: 0.9406\n","Epoch 00083: val_accuracy did not improve from 0.95592\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2562 - accuracy: 0.9406 - val_loss: 0.2195 - val_accuracy: 0.9551\n","Epoch 84/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2562 - accuracy: 0.9416\n","Epoch 00084: val_accuracy did not improve from 0.95592\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2563 - accuracy: 0.9415 - val_loss: 0.2185 - val_accuracy: 0.9555\n","Epoch 85/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2557 - accuracy: 0.9412\n","Epoch 00085: val_accuracy did not improve from 0.95592\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2557 - accuracy: 0.9411 - val_loss: 0.2178 - val_accuracy: 0.9556\n","Epoch 86/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.9410\n","Epoch 00086: val_accuracy improved from 0.95592 to 0.95633, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2549 - accuracy: 0.9410 - val_loss: 0.2175 - val_accuracy: 0.9563\n","Epoch 87/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2534 - accuracy: 0.9412\n","Epoch 00087: val_accuracy did not improve from 0.95633\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2534 - accuracy: 0.9412 - val_loss: 0.2164 - val_accuracy: 0.9557\n","Epoch 88/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2532 - accuracy: 0.9418\n","Epoch 00088: val_accuracy did not improve from 0.95633\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2529 - accuracy: 0.9419 - val_loss: 0.2155 - val_accuracy: 0.9559\n","Epoch 89/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2512 - accuracy: 0.9420\n","Epoch 00089: val_accuracy did not improve from 0.95633\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2513 - accuracy: 0.9420 - val_loss: 0.2146 - val_accuracy: 0.9559\n","Epoch 90/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.9420\n","Epoch 00090: val_accuracy improved from 0.95633 to 0.95650, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2525 - accuracy: 0.9420 - val_loss: 0.2139 - val_accuracy: 0.9565\n","Epoch 91/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2513 - accuracy: 0.9432\n","Epoch 00091: val_accuracy improved from 0.95650 to 0.95700, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2513 - accuracy: 0.9433 - val_loss: 0.2133 - val_accuracy: 0.9570\n","Epoch 92/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2507 - accuracy: 0.9423\n","Epoch 00092: val_accuracy improved from 0.95700 to 0.95758, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2509 - accuracy: 0.9422 - val_loss: 0.2126 - val_accuracy: 0.9576\n","Epoch 93/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.9444\n","Epoch 00093: val_accuracy did not improve from 0.95758\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2460 - accuracy: 0.9444 - val_loss: 0.2115 - val_accuracy: 0.9565\n","Epoch 94/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2471 - accuracy: 0.9435\n","Epoch 00094: val_accuracy did not improve from 0.95758\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2471 - accuracy: 0.9435 - val_loss: 0.2109 - val_accuracy: 0.9575\n","Epoch 95/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2460 - accuracy: 0.9438\n","Epoch 00095: val_accuracy improved from 0.95758 to 0.95825, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2459 - accuracy: 0.9439 - val_loss: 0.2102 - val_accuracy: 0.9582\n","Epoch 96/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2474 - accuracy: 0.9436\n","Epoch 00096: val_accuracy did not improve from 0.95825\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2474 - accuracy: 0.9436 - val_loss: 0.2094 - val_accuracy: 0.9571\n","Epoch 97/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2454 - accuracy: 0.9440\n","Epoch 00097: val_accuracy did not improve from 0.95825\n","188/188 [==============================] - 18s 96ms/step - loss: 0.2452 - accuracy: 0.9440 - val_loss: 0.2090 - val_accuracy: 0.9565\n","Epoch 98/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.9427\n","Epoch 00098: val_accuracy did not improve from 0.95825\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2464 - accuracy: 0.9427 - val_loss: 0.2083 - val_accuracy: 0.9575\n","Epoch 99/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2422 - accuracy: 0.9449\n","Epoch 00099: val_accuracy did not improve from 0.95825\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2421 - accuracy: 0.9450 - val_loss: 0.2076 - val_accuracy: 0.9581\n","Epoch 100/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2440 - accuracy: 0.9428\n","Epoch 00100: val_accuracy did not improve from 0.95825\n","188/188 [==============================] - 18s 96ms/step - loss: 0.2442 - accuracy: 0.9428 - val_loss: 0.2069 - val_accuracy: 0.9578\n","Epoch 101/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.9440\n","Epoch 00101: val_accuracy improved from 0.95825 to 0.95850, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2432 - accuracy: 0.9440 - val_loss: 0.2063 - val_accuracy: 0.9585\n","Epoch 102/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2407 - accuracy: 0.9442\n","Epoch 00102: val_accuracy improved from 0.95850 to 0.95875, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 96ms/step - loss: 0.2406 - accuracy: 0.9443 - val_loss: 0.2054 - val_accuracy: 0.9588\n","Epoch 103/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2411 - accuracy: 0.9451\n","Epoch 00103: val_accuracy improved from 0.95875 to 0.95908, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2409 - accuracy: 0.9451 - val_loss: 0.2051 - val_accuracy: 0.9591\n","Epoch 104/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9451\n","Epoch 00104: val_accuracy improved from 0.95908 to 0.95933, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2386 - accuracy: 0.9451 - val_loss: 0.2042 - val_accuracy: 0.9593\n","Epoch 105/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2399 - accuracy: 0.9451\n","Epoch 00105: val_accuracy did not improve from 0.95933\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2399 - accuracy: 0.9451 - val_loss: 0.2037 - val_accuracy: 0.9592\n","Epoch 106/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2385 - accuracy: 0.9459\n","Epoch 00106: val_accuracy did not improve from 0.95933\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2387 - accuracy: 0.9458 - val_loss: 0.2029 - val_accuracy: 0.9585\n","Epoch 107/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2394 - accuracy: 0.9457\n","Epoch 00107: val_accuracy did not improve from 0.95933\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2395 - accuracy: 0.9457 - val_loss: 0.2025 - val_accuracy: 0.9591\n","Epoch 108/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2357 - accuracy: 0.9475\n","Epoch 00108: val_accuracy improved from 0.95933 to 0.95950, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2360 - accuracy: 0.9474 - val_loss: 0.2019 - val_accuracy: 0.9595\n","Epoch 109/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2386 - accuracy: 0.9457\n","Epoch 00109: val_accuracy improved from 0.95950 to 0.96000, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2385 - accuracy: 0.9457 - val_loss: 0.2013 - val_accuracy: 0.9600\n","Epoch 110/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2353 - accuracy: 0.9464\n","Epoch 00110: val_accuracy did not improve from 0.96000\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2356 - accuracy: 0.9463 - val_loss: 0.2006 - val_accuracy: 0.9599\n","Epoch 111/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9468\n","Epoch 00111: val_accuracy did not improve from 0.96000\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2353 - accuracy: 0.9469 - val_loss: 0.1999 - val_accuracy: 0.9600\n","Epoch 112/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2339 - accuracy: 0.9467\n","Epoch 00112: val_accuracy did not improve from 0.96000\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2341 - accuracy: 0.9467 - val_loss: 0.1998 - val_accuracy: 0.9600\n","Epoch 113/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9479\n","Epoch 00113: val_accuracy did not improve from 0.96000\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2332 - accuracy: 0.9479 - val_loss: 0.1986 - val_accuracy: 0.9592\n","Epoch 114/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2350 - accuracy: 0.9468\n","Epoch 00114: val_accuracy did not improve from 0.96000\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2350 - accuracy: 0.9467 - val_loss: 0.1983 - val_accuracy: 0.9599\n","Epoch 115/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2318 - accuracy: 0.9478\n","Epoch 00115: val_accuracy improved from 0.96000 to 0.96050, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2318 - accuracy: 0.9477 - val_loss: 0.1979 - val_accuracy: 0.9605\n","Epoch 116/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2312 - accuracy: 0.9486\n","Epoch 00116: val_accuracy did not improve from 0.96050\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2310 - accuracy: 0.9486 - val_loss: 0.1974 - val_accuracy: 0.9603\n","Epoch 117/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2318 - accuracy: 0.9473\n","Epoch 00117: val_accuracy improved from 0.96050 to 0.96075, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2318 - accuracy: 0.9473 - val_loss: 0.1965 - val_accuracy: 0.9607\n","Epoch 118/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2315 - accuracy: 0.9489\n","Epoch 00118: val_accuracy did not improve from 0.96075\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2315 - accuracy: 0.9489 - val_loss: 0.1963 - val_accuracy: 0.9606\n","Epoch 119/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2305 - accuracy: 0.9482\n","Epoch 00119: val_accuracy improved from 0.96075 to 0.96092, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2303 - accuracy: 0.9482 - val_loss: 0.1957 - val_accuracy: 0.9609\n","Epoch 120/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2300 - accuracy: 0.9475\n","Epoch 00120: val_accuracy did not improve from 0.96092\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2300 - accuracy: 0.9475 - val_loss: 0.1951 - val_accuracy: 0.9609\n","Epoch 121/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2299 - accuracy: 0.9478\n","Epoch 00121: val_accuracy improved from 0.96092 to 0.96142, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2299 - accuracy: 0.9477 - val_loss: 0.1948 - val_accuracy: 0.9614\n","Epoch 122/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2292 - accuracy: 0.9479\n","Epoch 00122: val_accuracy did not improve from 0.96142\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2291 - accuracy: 0.9480 - val_loss: 0.1943 - val_accuracy: 0.9607\n","Epoch 123/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2274 - accuracy: 0.9485\n","Epoch 00123: val_accuracy did not improve from 0.96142\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2275 - accuracy: 0.9485 - val_loss: 0.1937 - val_accuracy: 0.9611\n","Epoch 124/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.9484\n","Epoch 00124: val_accuracy did not improve from 0.96142\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2276 - accuracy: 0.9484 - val_loss: 0.1933 - val_accuracy: 0.9612\n","Epoch 125/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2270 - accuracy: 0.9481\n","Epoch 00125: val_accuracy improved from 0.96142 to 0.96150, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2270 - accuracy: 0.9482 - val_loss: 0.1928 - val_accuracy: 0.9615\n","Epoch 126/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2247 - accuracy: 0.9488\n","Epoch 00126: val_accuracy did not improve from 0.96150\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2248 - accuracy: 0.9487 - val_loss: 0.1921 - val_accuracy: 0.9615\n","Epoch 127/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2249 - accuracy: 0.9491\n","Epoch 00127: val_accuracy did not improve from 0.96150\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2249 - accuracy: 0.9491 - val_loss: 0.1919 - val_accuracy: 0.9615\n","Epoch 128/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2256 - accuracy: 0.9493\n","Epoch 00128: val_accuracy improved from 0.96150 to 0.96225, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2254 - accuracy: 0.9494 - val_loss: 0.1913 - val_accuracy: 0.9622\n","Epoch 129/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2240 - accuracy: 0.9496\n","Epoch 00129: val_accuracy did not improve from 0.96225\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2240 - accuracy: 0.9496 - val_loss: 0.1909 - val_accuracy: 0.9618\n","Epoch 130/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2271 - accuracy: 0.9478\n","Epoch 00130: val_accuracy did not improve from 0.96225\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2269 - accuracy: 0.9479 - val_loss: 0.1905 - val_accuracy: 0.9618\n","Epoch 131/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2240 - accuracy: 0.9493\n","Epoch 00131: val_accuracy improved from 0.96225 to 0.96242, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2239 - accuracy: 0.9493 - val_loss: 0.1899 - val_accuracy: 0.9624\n","Epoch 132/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2232 - accuracy: 0.9493\n","Epoch 00132: val_accuracy did not improve from 0.96242\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2233 - accuracy: 0.9492 - val_loss: 0.1894 - val_accuracy: 0.9619\n","Epoch 133/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2221 - accuracy: 0.9506\n","Epoch 00133: val_accuracy improved from 0.96242 to 0.96250, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2220 - accuracy: 0.9506 - val_loss: 0.1891 - val_accuracy: 0.9625\n","Epoch 134/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2217 - accuracy: 0.9499\n","Epoch 00134: val_accuracy did not improve from 0.96250\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2217 - accuracy: 0.9500 - val_loss: 0.1886 - val_accuracy: 0.9622\n","Epoch 135/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2219 - accuracy: 0.9496\n","Epoch 00135: val_accuracy improved from 0.96250 to 0.96300, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2219 - accuracy: 0.9496 - val_loss: 0.1879 - val_accuracy: 0.9630\n","Epoch 136/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2197 - accuracy: 0.9503\n","Epoch 00136: val_accuracy improved from 0.96300 to 0.96333, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2198 - accuracy: 0.9504 - val_loss: 0.1874 - val_accuracy: 0.9633\n","Epoch 137/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2199 - accuracy: 0.9505\n","Epoch 00137: val_accuracy did not improve from 0.96333\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2199 - accuracy: 0.9505 - val_loss: 0.1872 - val_accuracy: 0.9625\n","Epoch 138/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2211 - accuracy: 0.9501\n","Epoch 00138: val_accuracy did not improve from 0.96333\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2210 - accuracy: 0.9501 - val_loss: 0.1867 - val_accuracy: 0.9631\n","Epoch 139/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2184 - accuracy: 0.9518\n","Epoch 00139: val_accuracy did not improve from 0.96333\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2183 - accuracy: 0.9518 - val_loss: 0.1862 - val_accuracy: 0.9630\n","Epoch 140/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2184 - accuracy: 0.9515\n","Epoch 00140: val_accuracy did not improve from 0.96333\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2184 - accuracy: 0.9515 - val_loss: 0.1862 - val_accuracy: 0.9632\n","Epoch 141/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2206 - accuracy: 0.9498\n","Epoch 00141: val_accuracy did not improve from 0.96333\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2207 - accuracy: 0.9498 - val_loss: 0.1858 - val_accuracy: 0.9633\n","Epoch 142/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9507\n","Epoch 00142: val_accuracy improved from 0.96333 to 0.96375, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2196 - accuracy: 0.9507 - val_loss: 0.1850 - val_accuracy: 0.9638\n","Epoch 143/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2179 - accuracy: 0.9516\n","Epoch 00143: val_accuracy did not improve from 0.96375\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2177 - accuracy: 0.9517 - val_loss: 0.1849 - val_accuracy: 0.9638\n","Epoch 144/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2173 - accuracy: 0.9515\n","Epoch 00144: val_accuracy did not improve from 0.96375\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2172 - accuracy: 0.9515 - val_loss: 0.1844 - val_accuracy: 0.9635\n","Epoch 145/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2173 - accuracy: 0.9518\n","Epoch 00145: val_accuracy improved from 0.96375 to 0.96400, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2174 - accuracy: 0.9517 - val_loss: 0.1840 - val_accuracy: 0.9640\n","Epoch 146/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9510\n","Epoch 00146: val_accuracy improved from 0.96400 to 0.96408, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2158 - accuracy: 0.9510 - val_loss: 0.1836 - val_accuracy: 0.9641\n","Epoch 147/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.9503\n","Epoch 00147: val_accuracy improved from 0.96408 to 0.96425, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2177 - accuracy: 0.9502 - val_loss: 0.1831 - val_accuracy: 0.9643\n","Epoch 148/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2162 - accuracy: 0.9513\n","Epoch 00148: val_accuracy did not improve from 0.96425\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2162 - accuracy: 0.9513 - val_loss: 0.1829 - val_accuracy: 0.9635\n","Epoch 149/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2170 - accuracy: 0.9513\n","Epoch 00149: val_accuracy did not improve from 0.96425\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2171 - accuracy: 0.9513 - val_loss: 0.1826 - val_accuracy: 0.9638\n","Epoch 150/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2158 - accuracy: 0.9518\n","Epoch 00150: val_accuracy did not improve from 0.96425\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2157 - accuracy: 0.9518 - val_loss: 0.1822 - val_accuracy: 0.9641\n","Epoch 151/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2151 - accuracy: 0.9509\n","Epoch 00151: val_accuracy improved from 0.96425 to 0.96467, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2151 - accuracy: 0.9510 - val_loss: 0.1815 - val_accuracy: 0.9647\n","Epoch 152/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2145 - accuracy: 0.9526\n","Epoch 00152: val_accuracy improved from 0.96467 to 0.96508, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 96ms/step - loss: 0.2144 - accuracy: 0.9526 - val_loss: 0.1814 - val_accuracy: 0.9651\n","Epoch 153/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2123 - accuracy: 0.9517\n","Epoch 00153: val_accuracy did not improve from 0.96508\n","188/188 [==============================] - 18s 96ms/step - loss: 0.2122 - accuracy: 0.9518 - val_loss: 0.1809 - val_accuracy: 0.9649\n","Epoch 154/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2126 - accuracy: 0.9525\n","Epoch 00154: val_accuracy did not improve from 0.96508\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2126 - accuracy: 0.9525 - val_loss: 0.1807 - val_accuracy: 0.9649\n","Epoch 155/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9521\n","Epoch 00155: val_accuracy did not improve from 0.96508\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2132 - accuracy: 0.9521 - val_loss: 0.1802 - val_accuracy: 0.9648\n","Epoch 156/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2132 - accuracy: 0.9523\n","Epoch 00156: val_accuracy did not improve from 0.96508\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2130 - accuracy: 0.9524 - val_loss: 0.1801 - val_accuracy: 0.9651\n","Epoch 157/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2129 - accuracy: 0.9525\n","Epoch 00157: val_accuracy did not improve from 0.96508\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2128 - accuracy: 0.9525 - val_loss: 0.1796 - val_accuracy: 0.9646\n","Epoch 158/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2112 - accuracy: 0.9531\n","Epoch 00158: val_accuracy improved from 0.96508 to 0.96525, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2113 - accuracy: 0.9530 - val_loss: 0.1794 - val_accuracy: 0.9653\n","Epoch 159/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2108 - accuracy: 0.9525\n","Epoch 00159: val_accuracy did not improve from 0.96525\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2109 - accuracy: 0.9524 - val_loss: 0.1788 - val_accuracy: 0.9651\n","Epoch 160/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2127 - accuracy: 0.9521\n","Epoch 00160: val_accuracy improved from 0.96525 to 0.96558, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2126 - accuracy: 0.9521 - val_loss: 0.1786 - val_accuracy: 0.9656\n","Epoch 161/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2121 - accuracy: 0.9531\n","Epoch 00161: val_accuracy did not improve from 0.96558\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2119 - accuracy: 0.9532 - val_loss: 0.1784 - val_accuracy: 0.9654\n","Epoch 162/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9535\n","Epoch 00162: val_accuracy did not improve from 0.96558\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2096 - accuracy: 0.9535 - val_loss: 0.1781 - val_accuracy: 0.9651\n","Epoch 163/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2094 - accuracy: 0.9527\n","Epoch 00163: val_accuracy improved from 0.96558 to 0.96567, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2095 - accuracy: 0.9526 - val_loss: 0.1775 - val_accuracy: 0.9657\n","Epoch 164/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2088 - accuracy: 0.9535\n","Epoch 00164: val_accuracy improved from 0.96567 to 0.96583, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2088 - accuracy: 0.9535 - val_loss: 0.1773 - val_accuracy: 0.9658\n","Epoch 165/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2084 - accuracy: 0.9536\n","Epoch 00165: val_accuracy did not improve from 0.96583\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2085 - accuracy: 0.9536 - val_loss: 0.1769 - val_accuracy: 0.9653\n","Epoch 166/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2088 - accuracy: 0.9533\n","Epoch 00166: val_accuracy did not improve from 0.96583\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2089 - accuracy: 0.9533 - val_loss: 0.1767 - val_accuracy: 0.9655\n","Epoch 167/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2084 - accuracy: 0.9539\n","Epoch 00167: val_accuracy improved from 0.96583 to 0.96600, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2083 - accuracy: 0.9539 - val_loss: 0.1764 - val_accuracy: 0.9660\n","Epoch 168/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2082 - accuracy: 0.9535\n","Epoch 00168: val_accuracy improved from 0.96600 to 0.96617, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2083 - accuracy: 0.9535 - val_loss: 0.1764 - val_accuracy: 0.9662\n","Epoch 169/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2085 - accuracy: 0.9528\n","Epoch 00169: val_accuracy improved from 0.96617 to 0.96633, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2084 - accuracy: 0.9529 - val_loss: 0.1758 - val_accuracy: 0.9663\n","Epoch 170/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2069 - accuracy: 0.9544\n","Epoch 00170: val_accuracy did not improve from 0.96633\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2069 - accuracy: 0.9544 - val_loss: 0.1756 - val_accuracy: 0.9661\n","Epoch 171/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2090 - accuracy: 0.9531\n","Epoch 00171: val_accuracy did not improve from 0.96633\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2088 - accuracy: 0.9532 - val_loss: 0.1754 - val_accuracy: 0.9663\n","Epoch 172/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9546\n","Epoch 00172: val_accuracy did not improve from 0.96633\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2061 - accuracy: 0.9545 - val_loss: 0.1749 - val_accuracy: 0.9663\n","Epoch 173/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2064 - accuracy: 0.9547\n","Epoch 00173: val_accuracy did not improve from 0.96633\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2064 - accuracy: 0.9547 - val_loss: 0.1746 - val_accuracy: 0.9663\n","Epoch 174/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2080 - accuracy: 0.9534\n","Epoch 00174: val_accuracy did not improve from 0.96633\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2079 - accuracy: 0.9535 - val_loss: 0.1744 - val_accuracy: 0.9662\n","Epoch 175/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2053 - accuracy: 0.9547\n","Epoch 00175: val_accuracy improved from 0.96633 to 0.96667, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2052 - accuracy: 0.9548 - val_loss: 0.1741 - val_accuracy: 0.9667\n","Epoch 176/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9542\n","Epoch 00176: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2060 - accuracy: 0.9542 - val_loss: 0.1736 - val_accuracy: 0.9664\n","Epoch 177/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2046 - accuracy: 0.9547\n","Epoch 00177: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2045 - accuracy: 0.9547 - val_loss: 0.1735 - val_accuracy: 0.9663\n","Epoch 178/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2038 - accuracy: 0.9543\n","Epoch 00178: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2038 - accuracy: 0.9543 - val_loss: 0.1731 - val_accuracy: 0.9665\n","Epoch 179/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2051 - accuracy: 0.9544\n","Epoch 00179: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2052 - accuracy: 0.9544 - val_loss: 0.1728 - val_accuracy: 0.9663\n","Epoch 180/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2041 - accuracy: 0.9544\n","Epoch 00180: val_accuracy did not improve from 0.96667\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2045 - accuracy: 0.9543 - val_loss: 0.1727 - val_accuracy: 0.9661\n","Epoch 181/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2042 - accuracy: 0.9542\n","Epoch 00181: val_accuracy improved from 0.96667 to 0.96692, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2040 - accuracy: 0.9542 - val_loss: 0.1726 - val_accuracy: 0.9669\n","Epoch 182/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2048 - accuracy: 0.9539\n","Epoch 00182: val_accuracy did not improve from 0.96692\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2046 - accuracy: 0.9539 - val_loss: 0.1721 - val_accuracy: 0.9664\n","Epoch 183/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2027 - accuracy: 0.9539\n","Epoch 00183: val_accuracy did not improve from 0.96692\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2025 - accuracy: 0.9539 - val_loss: 0.1718 - val_accuracy: 0.9667\n","Epoch 184/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2021 - accuracy: 0.9544\n","Epoch 00184: val_accuracy did not improve from 0.96692\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2022 - accuracy: 0.9544 - val_loss: 0.1716 - val_accuracy: 0.9668\n","Epoch 185/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2018 - accuracy: 0.9554\n","Epoch 00185: val_accuracy did not improve from 0.96692\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2021 - accuracy: 0.9553 - val_loss: 0.1712 - val_accuracy: 0.9667\n","Epoch 186/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2017 - accuracy: 0.9555\n","Epoch 00186: val_accuracy improved from 0.96692 to 0.96700, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2019 - accuracy: 0.9555 - val_loss: 0.1711 - val_accuracy: 0.9670\n","Epoch 187/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2031 - accuracy: 0.9546\n","Epoch 00187: val_accuracy did not improve from 0.96700\n","188/188 [==============================] - 18s 95ms/step - loss: 0.2029 - accuracy: 0.9547 - val_loss: 0.1708 - val_accuracy: 0.9670\n","Epoch 188/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2036 - accuracy: 0.9543\n","Epoch 00188: val_accuracy improved from 0.96700 to 0.96717, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2036 - accuracy: 0.9543 - val_loss: 0.1709 - val_accuracy: 0.9672\n","Epoch 189/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2020 - accuracy: 0.9541\n","Epoch 00189: val_accuracy did not improve from 0.96717\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2019 - accuracy: 0.9541 - val_loss: 0.1703 - val_accuracy: 0.9671\n","Epoch 190/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2007 - accuracy: 0.9553\n","Epoch 00190: val_accuracy did not improve from 0.96717\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2009 - accuracy: 0.9552 - val_loss: 0.1699 - val_accuracy: 0.9671\n","Epoch 191/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2012 - accuracy: 0.9547\n","Epoch 00191: val_accuracy did not improve from 0.96717\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2010 - accuracy: 0.9548 - val_loss: 0.1697 - val_accuracy: 0.9668\n","Epoch 192/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.2005 - accuracy: 0.9553\n","Epoch 00192: val_accuracy did not improve from 0.96717\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2006 - accuracy: 0.9552 - val_loss: 0.1694 - val_accuracy: 0.9669\n","Epoch 193/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2017 - accuracy: 0.9541\n","Epoch 00193: val_accuracy improved from 0.96717 to 0.96742, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2017 - accuracy: 0.9541 - val_loss: 0.1693 - val_accuracy: 0.9674\n","Epoch 194/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1998 - accuracy: 0.9556\n","Epoch 00194: val_accuracy did not improve from 0.96742\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1999 - accuracy: 0.9555 - val_loss: 0.1689 - val_accuracy: 0.9671\n","Epoch 195/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.2002 - accuracy: 0.9556\n","Epoch 00195: val_accuracy did not improve from 0.96742\n","188/188 [==============================] - 18s 94ms/step - loss: 0.2002 - accuracy: 0.9556 - val_loss: 0.1687 - val_accuracy: 0.9672\n","Epoch 196/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9552\n","Epoch 00196: val_accuracy did not improve from 0.96742\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1998 - accuracy: 0.9552 - val_loss: 0.1685 - val_accuracy: 0.9673\n","Epoch 197/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1975 - accuracy: 0.9569\n","Epoch 00197: val_accuracy did not improve from 0.96742\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1974 - accuracy: 0.9569 - val_loss: 0.1681 - val_accuracy: 0.9671\n","Epoch 198/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1996 - accuracy: 0.9550\n","Epoch 00198: val_accuracy improved from 0.96742 to 0.96758, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1997 - accuracy: 0.9550 - val_loss: 0.1683 - val_accuracy: 0.9676\n","Epoch 199/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1994 - accuracy: 0.9552\n","Epoch 00199: val_accuracy improved from 0.96758 to 0.96767, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1992 - accuracy: 0.9552 - val_loss: 0.1679 - val_accuracy: 0.9677\n","Epoch 200/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1968 - accuracy: 0.9564\n","Epoch 00200: val_accuracy did not improve from 0.96767\n","188/188 [==============================] - 18s 95ms/step - loss: 0.1969 - accuracy: 0.9564 - val_loss: 0.1675 - val_accuracy: 0.9677\n","Epoch 201/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1966 - accuracy: 0.9564\n","Epoch 00201: val_accuracy did not improve from 0.96767\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1966 - accuracy: 0.9565 - val_loss: 0.1671 - val_accuracy: 0.9676\n","Epoch 202/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1966 - accuracy: 0.9559\n","Epoch 00202: val_accuracy did not improve from 0.96767\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1966 - accuracy: 0.9559 - val_loss: 0.1667 - val_accuracy: 0.9677\n","Epoch 203/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.9553\n","Epoch 00203: val_accuracy improved from 0.96767 to 0.96775, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1974 - accuracy: 0.9553 - val_loss: 0.1670 - val_accuracy: 0.9678\n","Epoch 204/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.9555\n","Epoch 00204: val_accuracy did not improve from 0.96775\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1974 - accuracy: 0.9555 - val_loss: 0.1665 - val_accuracy: 0.9677\n","Epoch 205/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1964 - accuracy: 0.9567\n","Epoch 00205: val_accuracy improved from 0.96775 to 0.96825, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 96ms/step - loss: 0.1963 - accuracy: 0.9567 - val_loss: 0.1660 - val_accuracy: 0.9682\n","Epoch 206/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1961 - accuracy: 0.9553\n","Epoch 00206: val_accuracy did not improve from 0.96825\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1962 - accuracy: 0.9553 - val_loss: 0.1661 - val_accuracy: 0.9682\n","Epoch 207/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1960 - accuracy: 0.9559\n","Epoch 00207: val_accuracy did not improve from 0.96825\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1960 - accuracy: 0.9560 - val_loss: 0.1656 - val_accuracy: 0.9682\n","Epoch 208/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9568\n","Epoch 00208: val_accuracy did not improve from 0.96825\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1958 - accuracy: 0.9567 - val_loss: 0.1661 - val_accuracy: 0.9675\n","Epoch 209/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1961 - accuracy: 0.9570\n","Epoch 00209: val_accuracy did not improve from 0.96825\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1961 - accuracy: 0.9569 - val_loss: 0.1653 - val_accuracy: 0.9681\n","Epoch 210/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1952 - accuracy: 0.9569\n","Epoch 00210: val_accuracy did not improve from 0.96825\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1953 - accuracy: 0.9570 - val_loss: 0.1653 - val_accuracy: 0.9678\n","Epoch 211/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9562\n","Epoch 00211: val_accuracy did not improve from 0.96825\n","188/188 [==============================] - 21s 114ms/step - loss: 0.1956 - accuracy: 0.9561 - val_loss: 0.1648 - val_accuracy: 0.9682\n","Epoch 212/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1963 - accuracy: 0.9565\n","Epoch 00212: val_accuracy improved from 0.96825 to 0.96850, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1963 - accuracy: 0.9565 - val_loss: 0.1646 - val_accuracy: 0.9685\n","Epoch 213/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.9565\n","Epoch 00213: val_accuracy did not improve from 0.96850\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1943 - accuracy: 0.9565 - val_loss: 0.1649 - val_accuracy: 0.9677\n","Epoch 214/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1946 - accuracy: 0.9569\n","Epoch 00214: val_accuracy did not improve from 0.96850\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1944 - accuracy: 0.9570 - val_loss: 0.1640 - val_accuracy: 0.9685\n","Epoch 215/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1945 - accuracy: 0.9568\n","Epoch 00215: val_accuracy did not improve from 0.96850\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1945 - accuracy: 0.9568 - val_loss: 0.1640 - val_accuracy: 0.9683\n","Epoch 216/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9564\n","Epoch 00216: val_accuracy did not improve from 0.96850\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1944 - accuracy: 0.9564 - val_loss: 0.1638 - val_accuracy: 0.9682\n","Epoch 217/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1930 - accuracy: 0.9569\n","Epoch 00217: val_accuracy did not improve from 0.96850\n","188/188 [==============================] - 18s 95ms/step - loss: 0.1930 - accuracy: 0.9569 - val_loss: 0.1636 - val_accuracy: 0.9680\n","Epoch 218/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1933 - accuracy: 0.9577\n","Epoch 00218: val_accuracy did not improve from 0.96850\n","188/188 [==============================] - 18s 95ms/step - loss: 0.1931 - accuracy: 0.9578 - val_loss: 0.1634 - val_accuracy: 0.9682\n","Epoch 219/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1932 - accuracy: 0.9574\n","Epoch 00219: val_accuracy improved from 0.96850 to 0.96867, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 95ms/step - loss: 0.1930 - accuracy: 0.9575 - val_loss: 0.1631 - val_accuracy: 0.9687\n","Epoch 220/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9575\n","Epoch 00220: val_accuracy did not improve from 0.96867\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1923 - accuracy: 0.9575 - val_loss: 0.1628 - val_accuracy: 0.9685\n","Epoch 221/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9562\n","Epoch 00221: val_accuracy did not improve from 0.96867\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1939 - accuracy: 0.9563 - val_loss: 0.1628 - val_accuracy: 0.9683\n","Epoch 222/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1921 - accuracy: 0.9574\n","Epoch 00222: val_accuracy did not improve from 0.96867\n","188/188 [==============================] - 18s 95ms/step - loss: 0.1921 - accuracy: 0.9574 - val_loss: 0.1626 - val_accuracy: 0.9684\n","Epoch 223/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1930 - accuracy: 0.9565\n","Epoch 00223: val_accuracy did not improve from 0.96867\n","188/188 [==============================] - 18s 95ms/step - loss: 0.1930 - accuracy: 0.9565 - val_loss: 0.1624 - val_accuracy: 0.9687\n","Epoch 224/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9574\n","Epoch 00224: val_accuracy improved from 0.96867 to 0.96875, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1918 - accuracy: 0.9574 - val_loss: 0.1621 - val_accuracy: 0.9688\n","Epoch 225/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1923 - accuracy: 0.9572\n","Epoch 00225: val_accuracy did not improve from 0.96875\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1924 - accuracy: 0.9572 - val_loss: 0.1620 - val_accuracy: 0.9687\n","Epoch 226/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1917 - accuracy: 0.9563\n","Epoch 00226: val_accuracy improved from 0.96875 to 0.96950, saving model to mnist_l2_drop_best.h5\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1919 - accuracy: 0.9563 - val_loss: 0.1616 - val_accuracy: 0.9695\n","Epoch 227/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9575\n","Epoch 00227: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1908 - accuracy: 0.9576 - val_loss: 0.1615 - val_accuracy: 0.9688\n","Epoch 228/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1901 - accuracy: 0.9574\n","Epoch 00228: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1903 - accuracy: 0.9574 - val_loss: 0.1612 - val_accuracy: 0.9690\n","Epoch 229/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1914 - accuracy: 0.9565\n","Epoch 00229: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1912 - accuracy: 0.9565 - val_loss: 0.1611 - val_accuracy: 0.9688\n","Epoch 230/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1902 - accuracy: 0.9571\n","Epoch 00230: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1902 - accuracy: 0.9571 - val_loss: 0.1609 - val_accuracy: 0.9693\n","Epoch 231/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1902 - accuracy: 0.9572\n","Epoch 00231: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1902 - accuracy: 0.9572 - val_loss: 0.1606 - val_accuracy: 0.9691\n","Epoch 232/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1903 - accuracy: 0.9579\n","Epoch 00232: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1902 - accuracy: 0.9579 - val_loss: 0.1605 - val_accuracy: 0.9688\n","Epoch 233/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1891 - accuracy: 0.9577\n","Epoch 00233: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1890 - accuracy: 0.9577 - val_loss: 0.1605 - val_accuracy: 0.9686\n","Epoch 234/10000\n","188/188 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.9579\n","Epoch 00234: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 18s 95ms/step - loss: 0.1897 - accuracy: 0.9579 - val_loss: 0.1603 - val_accuracy: 0.9691\n","Epoch 235/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1891 - accuracy: 0.9578\n","Epoch 00235: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 18s 94ms/step - loss: 0.1893 - accuracy: 0.9578 - val_loss: 0.1599 - val_accuracy: 0.9692\n","Epoch 236/10000\n","187/188 [============================>.] - ETA: 0s - loss: 0.1910 - accuracy: 0.9576\n","Epoch 00236: val_accuracy did not improve from 0.96950\n","188/188 [==============================] - 18s 95ms/step - loss: 0.1911 - accuracy: 0.9576 - val_loss: 0.1600 - val_accuracy: 0.9691\n","Epoch 00236: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"1afc3177-deb4-4a31-f2ed-6c4b33b791df","executionInfo":{"status":"ok","timestamp":1590693587457,"user_tz":-60,"elapsed":2889,"user":{"displayName":"João Francisco Martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7EDqDJdwaRnx5mPxEBHU3fttmENg6ZUb601t9gQ=s64","userId":"11746321731464752057"}},"id":"8AWGTsdWuL-I","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["mnist_l2_drop_model.load_weights('mnist_l2_drop_best.h5')\n","loss, acc = mnist_l2_drop_model.evaluate(mnist_test_x, mnist_test_y)\n","print('Accuracy: {}'.format(acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 2s 6ms/step - loss: 0.1498 - accuracy: 0.9713\n","Accuracy: 0.9713000059127808\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4MokvRGWO58G","colab_type":"code","colab":{}},"source":["fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n","\n","loss_ax.set_title('Loss')\n","loss_ax.plot(mnist_conv_l2_drop_model_train.history['loss'], '-r', label='Train')\n","loss_ax.plot(mnist_conv_l2_drop_model_train.history['val_loss'], '-g', label='Validation')\n","\n","acc_ax.set_title('Accuracy')\n","acc_ax.plot(mnist_conv_l2_drop_model_train.history['accuracy'], '-r', label='Train')\n","acc_ax.plot(mnist_conv_l2_drop_model_train.history['val_accuracy'], '-g', label='Validation')\n","\n","plt.legend(loc=4)\n","plt.show()"],"execution_count":0,"outputs":[]}]}